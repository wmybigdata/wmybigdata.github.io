<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink内存管理</title>
      <link href="/2021/08/25/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2021/08/25/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink安装和配置</title>
      <link href="/2021/08/25/Flink%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
      <url>/2021/08/25/Flink%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink安装和配置"><a href="#Flink安装和配置" class="headerlink" title="Flink安装和配置"></a>Flink安装和配置</h1><h2 id="1、Flink安装和配置"><a href="#1、Flink安装和配置" class="headerlink" title="1、Flink安装和配置"></a>1、Flink安装和配置</h2><p>flink的安装是非常简单的，其中只需要配置一些文件和参数就好了</p><p>flink的运行时环境是非常重要的</p><p>RPC是远程过程调用，这样的RPC调用过程</p><p>Jobmanager是配置运行的JVM进程，堆内存的 大小，TaskManager的大小，定义的，和堆内存是有区别的，Flink集群是有状态的流式计算，如果要让Flink管理的内存是放在堆外的空间，堆内存创建的对象都是放到里面去，堆内存=堆内内存 + 堆外内存</p><p>heap.size堆内存，堆外内存很难区分，后面就分开了</p><p>问题，TaskManager &gt;  Jobmanager</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">numberOfTaskSlots</span> <span class="token attr-value"> ---> 当前的并行处理，一个taskmanager如何多线程勒，运行独立的线程，一个节点只有要给槽位，多线程执行的话只能一个执行，这样的就是要给并行执行的，这个是静态的能力</span>这个是针对于一个taskmanager来进行决定的<span class="token attr-name">最大的并行能力</span> <span class="token punctuation">=</span> <span class="token attr-value">ts * numberOfTaskSlots</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">workers</span> <span class="token attr-value">这个是对于taskmanager</span>记住在新版本中，主节点是单独的作为要给节点的</code></pre>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0005-数据开发规范</title>
      <link href="/2021/08/25/0005-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/25/0005-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0005-数据开发规范"><a href="#0005-数据开发规范" class="headerlink" title="0005-数据开发规范"></a>0005-数据开发规范</h1><p>话说，无规矩不成方圆，在数据开发领域同样也是适用的。</p><p>是不是经常听到这样的抱怨：</p><ul><li><p>我擦，这张表怎么特么的注释都没有？</p></li><li><p>我日，这个字段命名怎么是拼音全称？</p></li><li><p>怎么这张表的订单id是string类型，那张表的是订单id是bigint类型？</p></li><li><p>害，烦死了，这是谁开发的任务，库名/项目名都没写，我这去其他项目下跑还得一个个加上。</p></li><li><p>这代码谁写的？2张表join获取相关字段，也不写一下表的别名，我哪知道这是这是取自哪张表的 ？</p></li><li><p>运维任务又报错了？ 这是谁写的select * ，union all呀，害死人，上游表加字段了，这搞的。</p></li></ul><h2 id="一-词根"><a href="#一-词根" class="headerlink" title="一 词根"></a>一 词根</h2><p>词根的作用就是大家在对字段，表，指标等命名的时候作为查询使用，需要陆续完善</p><table><thead><tr><th>中文名</th><th>英文</th><th>评审时间</th><th>负责人</th></tr></thead><tbody><tr><td>数量</td><td>cnt</td><td>20210816</td><td>数据研发工程师</td></tr></tbody></table><h2 id="二-词典"><a href="#二-词典" class="headerlink" title="二 词典"></a>二 词典</h2><p>词典比词根更近一步，大家在对字段命名的时候，首先可以参考词典，以及可以参考其字段类型，使各个开发对同一个中文的字段命名及类型全部一致</p><table><thead><tr><th>中文名</th><th>英文名</th><th>字段类型</th><th>评审时间</th></tr></thead><tbody><tr><td>订单id</td><td>order_id</td><td>String</td><td>20210816</td></tr></tbody></table><h2 id="三-命名规范"><a href="#三-命名规范" class="headerlink" title="三 命名规范"></a>三 命名规范</h2><p>表命名</p><p>ods层</p><p>dwd层</p><p>dim层</p><p>dws层</p><p>ads层</p><p>临时表</p><p>手工表</p><p>表命名一般需要加入一及数据域，二级数据域，所以最开始需要划分数据域/主题域</p><p>然后详细的命名方式，根据每家公司有所不同</p><p>任务命名</p><p>一般情况下，任务名与输出表名一致</p><p>字段命名</p><p>参考 词根，词典</p><p>指标命名</p><p>略，之后讲指标平台的时候详细讲</p><h2 id="四-ddl规范"><a href="#四-ddl规范" class="headerlink" title="四 ddl规范"></a>四 ddl规范</h2><p>需不需要加生命周期</p><p>需不需要加创建人，创建时间</p><p>ddl语句需不需要单独维护</p><h2 id="五-sql编写规范"><a href="#五-sql编写规范" class="headerlink" title="五 sql编写规范"></a>五 sql编写规范</h2><p>关于缩进，大小写，空格，注释啥的细节就不说了，讲一下重点</p><p>多表关联时，需要使用别名来引用列 如： t1.user_id</p><p>表名前面需要加上库名/项目名，如 ods.test</p><p>不要出现select * 操作</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825170806956.png" alt="数据开发规范"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0004-数据清洗</title>
      <link href="/2021/08/25/0004-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
      <url>/2021/08/25/0004-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</url>
      
        <content type="html"><![CDATA[<h1 id="0004-数据清洗"><a href="#0004-数据清洗" class="headerlink" title="0004-数据清洗"></a>0004-数据清洗</h1><p>在我们想尽各种办法把数据弄进数据仓库ods层后，接下来的事情就比较有意思了，并且比较重要，对后续的数据模型建设，数据质量的保证，甚至影响管理层的决策(就问你怕不怕？)</p><p>那么，对于ETL过程中的数据清洗，你一般会怎么做呢？但凡你真正的做过数仓，我认为这些都是轻车熟路的，因为这是数据研发的必经之路</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164340099.png" alt="ETL"></p><p>我在对候选人进行考察的时候，也经常会问到这个问题，主要是看一下候选人有没有真实工作经验，然后在开发过程中有没有独立思考，并且知其然更知其所以然，但是候选人回答的都比较片面，比如只是处理空值，可能是公司数据质量的原因，但是就算公司业务库数据质量比较高，我们也应该需要全面的了解一些数据清洗规范。</p><p>那么我就来跟大家分享一下，工作中一般的数据清洗包含哪些并且一般怎么处理的吧。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164432662.png" alt="数据清洗"></p><h2 id="一-空值处理"><a href="#一-空值处理" class="headerlink" title="一 空值处理"></a>一 空值处理</h2><p>什么是空值，就是我们数据抽过来之后，发现有很多的字段为空，为了保证数据完整性或者方面后续的数据处理，这些我们都是需要预先处理的。一般使用默认值或者中位数，使用默认值的成本更低一点吧。</p><p>那么我们需要怎么处理呢，这个就要区分字段类型了。</p><table><thead><tr><th>类型</th><th>默认值</th><th>备注</th></tr></thead><tbody><tr><td>string</td><td>‘未知’，’unkown’，’-999’</td><td>描述，姓名</td></tr><tr><td>int</td><td>0</td><td>次数</td></tr><tr><td>bigint</td><td>0</td><td>次数</td></tr><tr><td>smallint/tinyint</td><td>0</td><td>是否</td></tr><tr><td>double</td><td>0</td><td>金额</td></tr><tr><td>decimal(16,4)</td><td>0</td><td>金额</td></tr><tr><td>datetime</td><td>1970-01-01 00:00:00</td><td>时间</td></tr></tbody></table><p>以上会有一个小问题，string类型给了默认值之后，如果是主键，在进行join操作的时候，本来关联不上的，由于给了默认值，就能关联上了，这种情况可能要单独处理一下。</p><p>但是每种类型的默认值需要和数据分析师，业务方，需求方等其他部门同步一下，避免出现懵逼情况，比如 1970-01-01 00:00:00是个啥</p><h2 id="二-数据格式处理"><a href="#二-数据格式处理" class="headerlink" title="二 数据格式处理"></a>二 数据格式处理</h2><p>数据格式处理，主要包含2个方面，一个是复杂数据类型的解析处理，比如json解析等</p><p>另一方面是内容的统一，对于日期字段数据格式，可以统一为 yyyy-MM-dd</p><h2 id="三-枚举值处理"><a href="#三-枚举值处理" class="headerlink" title="三 枚举值处理"></a>三 枚举值处理</h2><p>我们的数据源可能会来自很多系统，业务开发的习惯可能不太相同，导致枚举值也不太一样，举个例子  a表的 1 代表 男，2代表女，b表的1代表女，2代表男，如果不统一一下的话，将来使用数据的时候就会产生误解，当作为关联条件去关联的时候，更会直接导致数据错误</p><h2 id="四-字段类型处理"><a href="#四-字段类型处理" class="headerlink" title="四 字段类型处理"></a>四 字段类型处理</h2><p>对于同一个字段，他们的字段类型我们需要做到统一，比如 订单id，不能a表是string类型，b表是bigint类型，统一为string类型不好吗？</p><h2 id="五-注释处理"><a href="#五-注释处理" class="headerlink" title="五 注释处理"></a>五 注释处理</h2><p>主要是对业务库没有注释的表或者字段进行补充完善。自己不太确定的需要及时找业务开发确认清楚。</p><h2 id="六-敏感数据处理"><a href="#六-敏感数据处理" class="headerlink" title="六 敏感数据处理"></a>六 敏感数据处理</h2><p>理论上来说数仓的敏感数据都是需要加密的，直到输出到业务库或者其他服务于业务方的存储系统，我们在同步的时候需要解密传输。</p><p>对于姓名，手机号，身份证号，邮箱，银行卡号这些，我们都需要通过自带的加密函数或者udf函数进行加密，保证数据安全。</p><h2 id="七-数据单位统一"><a href="#七-数据单位统一" class="headerlink" title="七 数据单位统一"></a>七 数据单位统一</h2><p>数据单位统一，这个比较好理解了，比如金额，一张表是美元，一张表是人民币，你在对2张表的金额做操作的时候，是不是会有问题，所以这个需要提前定好，统一清洗。</p><h2 id="八-逻辑错误清洗"><a href="#八-逻辑错误清洗" class="headerlink" title="八 逻辑错误清洗"></a>八 逻辑错误清洗</h2><p>举几个简单的例子：年龄不可能超过200岁吧，身份证号不可能是5位吧？</p><p>这个具体怎么清洗，需要根据实际的业务场景。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825165711383.png" alt="数据清洗流程"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0003-数据同步策略</title>
      <link href="/2021/08/25/0003-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/"/>
      <url>/2021/08/25/0003-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0003-数据同步策略"><a href="#0003-数据同步策略" class="headerlink" title="0003-数据同步策略"></a>0003-数据同步策略</h1><p>我们要搭建数据仓库，那么数据是来自哪里呢？</p><p>答案很明显，一般来说，数据会来自3个方向，业务库数据，日志数据及爬虫数据，对于爬虫数据，我们就不做过多的分析，一般就是爬一些竞品信息，维表信息。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825163624184.png" alt="数据同步"></p><p>业务库数据</p><p>什么是业务库数据，指的是业务系统产生的数据，这个业务系统可以是你们公司的官网，app或者小程序。</p><p>对于业务库数据，如果是t+1同步抽取的话，我们一般会选择sqoop或者datax，当然，有些传统企业会使用kettle，不过个人认为kettle还是比较重的，而且好像也不太方便，所以不是很推荐；</p><p>如果是需要实时同步的话，我们一般使用开启binlog，采用canal+kafka的模式实时监听同步。或者使用flink-cdc都是可以实现的</p><p>对于离线抽取，不管是使用什么工具，都会设计到一个抽取策略问题，比如500数据，5万数据，30万数据，1000万数据分别需要怎么存储？</p><p>对于1000万数据来说，我们肯定是需要增量进行抽取的，但是业务库一般来说需要满足如下3个条件，我们才能进行增量抽取：</p><ul><li><p>主键唯一</p></li><li><p>有更新时间并且数据更新时，该时间也会改变</p></li><li><p>不存在物理删除的情况</p></li></ul><p>那么500数据，5w数据和30万数据呢？这个不能拍脑袋就决定了，需要综合考虑，比如该业务上线时长，该业务预期的数据增长等，这个需要与业务方和对应的业务开发进行沟通</p><p>比如50条数据是维表数据，那么全量抽取问题不大，如果是刚上线的业务，且可预见的后期数据量会爆发式增长，那么还是增量抽取比较靠谱，免得后期频繁的修改抽取任务</p><p>日志数据</p><p>日志数据一般来说指的是埋点日志，但是有些ng日志或者其他的日志也会上报进行采集。</p><p>对于日志数据，目前主流的工具可能是flume和logstash，这个好像就没啥特别好说的了，生产环境对于日志数据的要求不会太高，多一点少一点其实问题都不太大。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164015829.png" alt="数据同步"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-数据探查</title>
      <link href="/2021/08/25/0002-%E6%95%B0%E6%8D%AE%E6%8E%A2%E6%9F%A5/"/>
      <url>/2021/08/25/0002-%E6%95%B0%E6%8D%AE%E6%8E%A2%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-数据探查"><a href="#0002-数据探查" class="headerlink" title="0002-数据探查"></a>0002-数据探查</h1><p><font color="blue">给你一个需求或者一条新的业务线可能需要搭建数仓，做数据处理，然后提供一些指标数据给到需求方，你这边会怎么开始呢？</font></p><p><font color="blue">当你接到这个需求的时候，直接开干？抽表，清洗，分层，建模？</font></p><p><font color="blue">呵呵，这就是瞎搞。啥也不知道不了解的情况下，对数据源一无所知的情况下，是谁给你勇气直接开干的？</font></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825161712447.png" alt="数据探查"></p><p>正常情况下，接到新的数据需求或者任务的时候，我们最开始应该需要做一下数据调研，或者说数据探查。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825162346421.png" alt="数据调研"></p><p>那么问题来了，具体我们应该做些啥呢？就拿接到一个需求来举例说明吧：</p><p>那么问题来了，具体我们应该做些啥呢？就拿接到一个需求来举例说明吧：</p><p>业务方让你统计一些数据指标，给了你相关口径，表给没给这个就不清楚了，理论上是要给的，但是如果他自己也不知道，那这个也是没办法的。</p><p>给了的话还好，你直接去业务库查看这些表，但是没给的话，你可能要多一些步骤，可能要去找对应的产品经理或者业务开发帮你一起找。最后，你终于找到需要的那些表了。</p><p>拿到表之后怎么办？慌了吗？</p><p>duck不必，我们可以开心的做数据探查了，对数据有一个整体的把控。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825163205224.png" alt="整体把控"></p><ul><li><p>了解数据表字段能不能满足需求发需求，看看字段有没有少，如果发现少了或者提供的数据源没法满足需求，需要及时找对应的人员沟通，可别自己瞎搞或者跑路了</p></li><li><p>看一下相关表的数据结构，有没有json数据或者或者其他比较复杂的数据，便于数仓处理</p></li><li><p>看一下数据内容，是不是有些字段毫无意义，不需要存储在数仓，或者看一下是不是有很多空置的情况，脏数据情况，其实就是探查一下业务库的数据质量，通过这些我们可以大概判断在数仓会有多少的清洗机制</p></li><li><p>看一下数据量，方便我们在抽取的时候选择增量抽取或者全量抽取，甚至可以问一下业务方业务增长情况，更能准确的决定数据抽取策略。</p></li></ul><p>全部探查完以后，你就可以继续以下的步骤了。</p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-数据仓库体系</title>
      <link href="/2021/08/25/0001-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%BD%93%E7%B3%BB/"/>
      <url>/2021/08/25/0001-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%BD%93%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-数据仓库体系"><a href="#0001-数据仓库体系" class="headerlink" title="0001-数据仓库体系"></a>0001-数据仓库体系</h1><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825160959524.png" alt="数据仓库体系"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive高级源码</title>
      <link href="/2021/08/25/Hive%E9%AB%98%E7%BA%A7%E6%BA%90%E7%A0%81/"/>
      <url>/2021/08/25/Hive%E9%AB%98%E7%BA%A7%E6%BA%90%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive高级源码"><a href="#Hive高级源码" class="headerlink" title="Hive高级源码"></a>Hive高级源码</h1><ul><li><input disabled="" type="checkbox"> 组件分析和核心流程 重点</li><li><input disabled="" type="checkbox"> Hive源码解读 —&gt; 分支解读</li><li><input disabled="" type="checkbox"> Hive Debug —&gt; MR,Spark,Tez —&gt; 某个分支解读</li></ul><h2 id="第-1-章-HQL-是如何转换为-MR-任务的"><a href="#第-1-章-HQL-是如何转换为-MR-任务的" class="headerlink" title="第 1 章 HQL 是如何转换为 MR 任务的"></a>第 1 章 HQL 是如何转换为 MR 任务的</h2><h3 id="1-1-Hive-的核心组成介绍"><a href="#1-1-Hive-的核心组成介绍" class="headerlink" title="1.1 Hive 的核心组成介绍"></a>1.1 Hive 的核心组成介绍</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825153641750.png" alt="hive架构"></p><h4 id="1）用户接口：Client"><a href="#1）用户接口：Client" class="headerlink" title="1）用户接口：Client"></a>1）用户接口：Client</h4><p>​    CLI（command-line interface）、JDBC/ODBC(jdbc 访问 hive)、WEBUI（浏览器访问 hive）</p><h4 id="2）元数据：Metastore"><a href="#2）元数据：Metastore" class="headerlink" title="2）元数据：Metastore"></a>2）元数据：Metastore</h4><p>​    元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表 的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore</p><h4 id="3）Hadoop"><a href="#3）Hadoop" class="headerlink" title="3）Hadoop"></a>3）Hadoop</h4><p>​    使用 HDFS 进行存储，使用 MapReduce 进行计算。</p><h4 id="4）驱动器：Driver"><a href="#4）驱动器：Driver" class="headerlink" title="4）驱动器：Driver"></a>4）驱动器：Driver</h4><h4 id="5）解析器（SQL-Parser）"><a href="#5）解析器（SQL-Parser）" class="headerlink" title="5）解析器（SQL Parser）"></a>5）解析器（SQL Parser）</h4><p>​    将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr； 对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误</p><h4 id="6）编译器（Physical-Plan）"><a href="#6）编译器（Physical-Plan）" class="headerlink" title="6）编译器（Physical Plan）"></a>6）编译器（Physical Plan）</h4><p>​    将 AST 编译生成逻辑执行计划。</p><h4 id="7）优化器（Query-Optimizer）"><a href="#7）优化器（Query-Optimizer）" class="headerlink" title="7）优化器（Query Optimizer）"></a>7）优化器（Query Optimizer）</h4><p>​    对逻辑执行计划进行优化。</p><h4 id="8）执行器（Execution）"><a href="#8）执行器（Execution）" class="headerlink" title="8）执行器（Execution）"></a>8）执行器（Execution）</h4><p>​    把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。</p><h3 id="1-2-HQL-转换为-MR-任务流程说明"><a href="#1-2-HQL-转换为-MR-任务流程说明" class="headerlink" title="1.2 HQL 转换为 MR 任务流程说明"></a>1.2 HQL 转换为 MR 任务流程说明</h3><ul><li><input disabled="" type="checkbox"> 1.进入程序，利用Antlr框架定义HQL的语法规则，对HQL完成词法语法解析，将HQL转换为为AST（抽象语法树）；</li><li><input disabled="" type="checkbox"> 2.遍历AST，抽象出查询的基本组成单元QueryBlock（查询块），可以理解为最小的查询执行单元；</li><li><input disabled="" type="checkbox"> 3.遍历QueryBlock，将其转换为OperatorTree（操作树，也就是逻辑执行计划），可以理解为不可拆分的一个逻辑执行单元；</li><li><input disabled="" type="checkbox"> 4.使用逻辑优化器对OperatorTree（操作树）进行逻辑优化。例如合并不必要的ReduceSinkOperator，减少Shuffle数据量；</li><li><input disabled="" type="checkbox"> 5.遍历OperatorTree，转换为TaskTree。也就是翻译为MR任务的流程，将逻辑执行计划转换为物理执行计划；</li><li><input disabled="" type="checkbox"> 6.使用物理优化器对TaskTree进行物理优化；</li><li><input disabled="" type="checkbox"> 7.生成最终的执行计划，提交任务到Hadoop集群运行</li></ul><p>总结：解析，查询块，操作树，逻辑优化，任务树，物理优化，提交任务执行</p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimpleDateFormat线程安全问题</title>
      <link href="/2021/08/25/SimpleDateFormat%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/"/>
      <url>/2021/08/25/SimpleDateFormat%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>superset搭建</title>
      <link href="/2021/08/24/superset%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/24/superset%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="superset搭建"><a href="#superset搭建" class="headerlink" title="superset搭建"></a>superset搭建</h1><h2 id="python3-6-环境搭建"><a href="#python3-6-环境搭建" class="headerlink" title="python3.6 环境搭建"></a>python3.6 环境搭建</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">yum</span> <span class="token attr-value">upgrade python-setuptools</span><span class="token attr-name">yum</span> <span class="token attr-value">install gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel</span><span class="token attr-name">pip</span> <span class="token attr-value">install cryptography</span><span class="token attr-name">yum</span> <span class="token attr-value">-y install wget</span><span class="token attr-name">wget</span> <span class="token attr-value">https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz</span><span class="token attr-name">tar</span> <span class="token attr-value">-zxvf Python-3.6.0.tgz</span><span class="token attr-name">cd</span> <span class="token attr-value">Python-3.6.0</span><span class="token attr-name">./configure</span> <span class="token attr-value">--prefix=/usr/local/python</span><span class="token attr-name">make</span> <span class="token attr-value">&amp;&amp; make install</span><span class="token comment" spellcheck="true"># 添加Python3的环境变量</span><span class="token attr-name">export</span> <span class="token attr-value">PYTHON_HOME=/usr/local/python</span><span class="token attr-name">export</span> <span class="token attr-value">PATH=$PATH:$PYTHON_HOME/bin</span><span class="token attr-name">source</span> <span class="token attr-value">/etc/profile</span>python3.6<span class="token attr-name">mv</span> <span class="token attr-value">/usr/bin/python /usr/bin/python-2.7.5</span><span class="token attr-name">ln</span> <span class="token attr-value">-s /usr/local/python/bin/python3.6 /usr/bin/python</span><span class="token attr-name">ln</span> <span class="token attr-value">-s /usr/local/python/bin/pip3 /usr/bin/pip</span><span class="token comment" spellcheck="true">#! /usr/bin/python-2.7.5</span><span class="token attr-name">vi</span> <span class="token attr-value">/usr/bin/yum</span><span class="token attr-name"> vi</span> <span class="token attr-value">/usr/libexec/urlgrabber-ext-down</span>  python</code></pre><h2 id="Superset安装"><a href="#Superset安装" class="headerlink" title="Superset安装"></a>Superset安装</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">bash</span> <span class="token attr-value">Miniconda3-latest-Linux-x86_64.sh</span><span class="token attr-name">source</span> <span class="token attr-value">~/.bashrc</span><span class="token attr-name">conda</span> <span class="token attr-value">config --set auto_activate_base false</span><span class="token attr-name">conda</span> <span class="token attr-value">config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><span class="token attr-name">conda</span> <span class="token attr-value">config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><span class="token attr-name">conda</span> <span class="token attr-value">config --set show_channel_urls yes</span><span class="token attr-name">conda</span> <span class="token attr-value">create --name superset python=3.6</span><span class="token attr-name">conda</span> <span class="token attr-value">activate superset</span><span class="token attr-name">conda</span> <span class="token attr-value">deactivate</span><span class="token attr-name">sudo</span> <span class="token attr-value">yum install -y python-setuptools</span><span class="token attr-name">sudo</span> <span class="token attr-value">yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel</span><span class="token attr-name">pip</span> <span class="token attr-value">install --upgrade setuptools pip -i https://pypi.douban.com/simple/</span><span class="token attr-name">pip</span> <span class="token attr-value">install apache-superset -i https://pypi.douban.com/simple/</span><span class="token attr-name">pip</span> <span class="token attr-value">install dataclasses</span><span class="token attr-name">superset</span> <span class="token attr-value">db upgrade</span><span class="token attr-name">export</span> <span class="token attr-value">FLASK_APP=superset</span><span class="token attr-name">flask</span> <span class="token attr-value">fab create-admin</span><span class="token attr-name">superset</span> <span class="token attr-value">init</span><span class="token attr-name">pip</span> <span class="token attr-value">install gunicorn -i https://pypi.douban.com/simple/</span><span class="token attr-name">conda</span> <span class="token attr-value">install mysqlclient</span><span class="token attr-name">gunicorn</span> <span class="token attr-value">--workers 5 --timeout 120 --bind 192.168.21.166:8787  "superset.app:create_app()" --daemon</span><span class="token attr-name">ps</span> <span class="token attr-value">-ef | awk '/superset/ &amp;&amp; !/awk/{print $2}' | xargs kill -9</span><span class="token attr-name">conda</span> <span class="token attr-value">deactivate</span><span class="token attr-name"> conda</span> <span class="token attr-value">activate superset</span></code></pre><h2 id="superset搭建mysql"><a href="#superset搭建mysql" class="headerlink" title="superset搭建mysql"></a>superset搭建mysql</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">mysql</span><span class="token punctuation">:</span><span class="token attr-value">//root:000000@192.168.21.166/superset</span><span class="token attr-name">pip</span> <span class="token attr-value">install dataclasses -i https://pypi.tuna.tsinghua.edu.cn/simple/</span><span class="token attr-name">root</span> <span class="token attr-value">root</span></code></pre><p>clickhouse://default:<a href="mailto:password@192.168.21.113">password@192.168.21.113</a>:8123/superset</p><p>这个重要的是，可以连接clickhouse和superset了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse单节点部署</title>
      <link href="/2021/08/24/Clickhouse%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/24/Clickhouse%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="Clickhouse环境搭建"><a href="#Clickhouse环境搭建" class="headerlink" title="Clickhouse环境搭建"></a>Clickhouse环境搭建</h1><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">sudo</span> <span class="token attr-value">rpm -ivh *.rpm</span><span class="token attr-name"> sudo</span> <span class="token attr-value">vim /etc/clickhouse-server/config.xml</span> <span class="token attr-name"> &lt;listen_host></span><span class="token punctuation">:</span><span class="token attr-value">:&lt;/listen_host></span> <span class="token attr-name"> sudo</span> <span class="token attr-value">systemctl start clickhouse-server</span><span class="token attr-name"> sudo</span> <span class="token attr-value">systemctl stop clickhouse-server</span> </code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkCDC在sqlClient的使用</title>
      <link href="/2021/08/24/FlinkCDC%E5%9C%A8sqlClient%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/08/24/FlinkCDC%E5%9C%A8sqlClient%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="FlinkCDC在sqlClient的使用"><a href="#FlinkCDC在sqlClient的使用" class="headerlink" title="FlinkCDC在sqlClient的使用"></a>FlinkCDC在sqlClient的使用</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>其实在此之前，我也发表一篇关于Flink CDC的使用但是那个时候不是很懂的，很多东西都是懵懵懂懂的，但是今天我又重新写一篇关于自己做的一些的心得吧。</p><h3 id="1、首先环境准备"><a href="#1、首先环境准备" class="headerlink" title="1、首先环境准备"></a>1、首先环境准备</h3><ul><li>搭建Flink1.13.2的集群环境，启动sql-client的时候得启动这个</li><li>部署MySQL5.7单节点就行</li></ul><h3 id="2、配置项"><a href="#2、配置项" class="headerlink" title="2、配置项"></a>2、配置项</h3><h4 id="2-1-MySQL的配置项"><a href="#2-1-MySQL的配置项" class="headerlink" title="2.1 MySQL的配置项"></a>2.1 MySQL的配置项</h4><pre class=" language-xml"><code class="language-xml">server-id=1log-bin=mysql-binbinlog_format=rowbinlog-do-db=flinkInfo</code></pre><p>重启MySQL服务：systemctl restart mysqld</p><h4 id="2-2-Flink-lib目录下面的依赖需要准备的"><a href="#2-2-Flink-lib目录下面的依赖需要准备的" class="headerlink" title="2.2 Flink lib目录下面的依赖需要准备的"></a>2.2 Flink lib目录下面的依赖需要准备的</h4><pre class=" language-xml"><code class="language-xml">flink-connector-kafka_2.11-1.13.2.jarflink-format-changelog-json-2.0.0.jarflink-json-1.13.2.jarflink-sql-connector-mysql-cdc-2.0.0.jarflink-table-blink_2.11-1.13.2.jar</code></pre><p>这些核心的准备好，有些依赖是集群里面就自带的，但是我们分发给一个节点的时候，一定要分发给每一个节点，然后重启flink集群和flink sql client</p><h2 id="MySQL建表"><a href="#MySQL建表" class="headerlink" title="MySQL建表"></a>MySQL建表</h2><pre class=" language-sql"><code class="language-sql"><span class="token operator">-</span> MySQL<span class="token comment" spellcheck="true">/*Table structure for table `order_info` */</span><span class="token keyword">DROP</span> <span class="token keyword">TABLE</span> <span class="token keyword">IF</span> <span class="token keyword">EXISTS</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token punctuation">;</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span> <span class="token punctuation">(</span>  <span class="token punctuation">`</span>id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span> <span class="token keyword">COMMENT</span> <span class="token string">'编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>consignee<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'收货人'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>consignee_tel<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'收件人电话'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>total_amount<span class="token punctuation">`</span> <span class="token keyword">decimal</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'总金额'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_status<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单状态,1表示下单，2表示支付'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>user_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'用户id'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>payment_way<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'付款方式'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>delivery_address<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'送货地址'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_comment<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单备注'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>out_trade_no<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单交易编号（第三方支付用)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>trade_body<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单描述(第三方支付用)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>create_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>operate_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'操作时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>expire_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'失效时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>tracking_no<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'物流单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>parent_order_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'父订单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>img_url<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'图片路径'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>province_id<span class="token punctuation">`</span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'地区'</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token punctuation">`</span>id<span class="token punctuation">`</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token operator">=</span><span class="token number">1</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span><span class="token operator">=</span>utf8 <span class="token keyword">COMMENT</span><span class="token operator">=</span><span class="token string">'订单表'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token comment" spellcheck="true">-- Records of order_info</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">476</span><span class="token punctuation">,</span> <span class="token string">'lAXjcL'</span><span class="token punctuation">,</span> <span class="token string">'13408115089'</span><span class="token punctuation">,</span> <span class="token number">433.00</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token string">'OYyAdSdLxedceqovndCD'</span><span class="token punctuation">,</span> <span class="token string">'ihjAYsSjrgJMQVdFQnSy'</span><span class="token punctuation">,</span> <span class="token string">'8728720206'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 02:21:38'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">477</span><span class="token punctuation">,</span> <span class="token string">'QLiFDb'</span><span class="token punctuation">,</span> <span class="token string">'13415139984'</span><span class="token punctuation">,</span> <span class="token number">772.00</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token string">'OizYrQbKuWvrvdfpkeSZ'</span><span class="token punctuation">,</span> <span class="token string">'wiBhhqhMndCCgXwmWVQq'</span><span class="token punctuation">,</span> <span class="token string">'1679381473'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 09:12:25'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">478</span><span class="token punctuation">,</span> <span class="token string">'iwKjQD'</span><span class="token punctuation">,</span> <span class="token string">'13320383859'</span><span class="token punctuation">,</span> <span class="token number">88.00</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token number">107</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token string">'cbXLKtNHWOcWzJVBWdAs'</span><span class="token punctuation">,</span> <span class="token string">'njjsnknHxsxhuCCeNDDi'</span><span class="token punctuation">,</span> <span class="token string">'0937074290'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 15:56:34'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*Table structure for table `order_detail` */</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token punctuation">(</span>  <span class="token punctuation">`</span>id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span> <span class="token keyword">COMMENT</span> <span class="token string">'编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'sku_id'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_name<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'sku名称（冗余)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>img_url<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'图片名称（冗余)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_price<span class="token punctuation">`</span> <span class="token keyword">decimal</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'购买价格(下单时sku价格）'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_num<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'购买个数'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>create_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token punctuation">`</span>id<span class="token punctuation">`</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token operator">=</span><span class="token number">1</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span><span class="token operator">=</span>utf8 <span class="token keyword">COMMENT</span><span class="token operator">=</span><span class="token string">'订单明细表'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token comment" spellcheck="true">-- Records of order_detail</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1329</span><span class="token punctuation">,</span> <span class="token number">476</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//XLMByOyZDTJQYxphQHNTgYAFzJJCKTmCbzvEJIpz', 8900.00, '3', '2020-06-18 02:21:38');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1330</span><span class="token punctuation">,</span> <span class="token number">477</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token string">'荣耀10 GT游戏加速 AIS手持夜景 6GB+64GB 幻影蓝全网通 移动联通电信'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//ixOCtlYmlxEEgUfPLiLdjMftzrleOEIBKSjrhMne', 2452.00, '4', '2020-06-18 09:12:25');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1331</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'小米Play 流光渐变AI双摄 4GB+64GB 梦幻蓝 全网通4G 双卡双待 小水滴全面屏拍照游戏智能手机'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//RqfEFnAOqnqRnNZLFRvBuwXxwNBtptYJCILDKQYv', 1442.00, '1', '2020-06-18 15:56:34');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1332</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//IwhuCDlsiLenfKjPzbJrIoxswdfofKhJLMzlJAKV', 8900.00, '3', '2020-06-18 15:56:34');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1333</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//bbfwTbAzTWapywODzOtDJMJUEqNTeRTUQuCDkqXP', 8900.00, '1', '2020-06-18 15:56:34');</span></code></pre><h2 id="Flink-SQL建表"><a href="#Flink-SQL建表" class="headerlink" title="Flink SQL建表"></a>Flink SQL建表</h2><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 创建订单信息表</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> order_info<span class="token punctuation">(</span>    id <span class="token keyword">BIGINT</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span>    user_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    create_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    operate_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    province_id <span class="token keyword">INT</span><span class="token punctuation">,</span>    order_status STRING<span class="token punctuation">,</span>    total_amount <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>    <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'mysql-cdc'</span><span class="token punctuation">,</span>    <span class="token string">'hostname'</span> <span class="token operator">=</span> <span class="token string">'flink04'</span><span class="token punctuation">,</span>    <span class="token string">'port'</span> <span class="token operator">=</span> <span class="token string">'3306'</span><span class="token punctuation">,</span>    <span class="token string">'username'</span> <span class="token operator">=</span> <span class="token string">'root'</span><span class="token punctuation">,</span>    <span class="token string">'password'</span> <span class="token operator">=</span> <span class="token string">'000000'</span><span class="token punctuation">,</span>    <span class="token string">'database-name'</span> <span class="token operator">=</span> <span class="token string">'flinkInfo'</span><span class="token punctuation">,</span>    <span class="token string">'table-name'</span> <span class="token operator">=</span> <span class="token string">'order_info'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 创建订单详情表</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> order_detail<span class="token punctuation">(</span>    id <span class="token keyword">BIGINT</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span>    order_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    sku_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    sku_name STRING<span class="token punctuation">,</span>    sku_num <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    order_price <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> create_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>    <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'mysql-cdc'</span><span class="token punctuation">,</span>    <span class="token string">'hostname'</span> <span class="token operator">=</span> <span class="token string">'flink04'</span><span class="token punctuation">,</span>    <span class="token string">'port'</span> <span class="token operator">=</span> <span class="token string">'3306'</span><span class="token punctuation">,</span>    <span class="token string">'username'</span> <span class="token operator">=</span> <span class="token string">'root'</span><span class="token punctuation">,</span>    <span class="token string">'password'</span> <span class="token operator">=</span> <span class="token string">'000000'</span><span class="token punctuation">,</span>    <span class="token string">'database-name'</span> <span class="token operator">=</span> <span class="token string">'flinkInfo'</span><span class="token punctuation">,</span>    <span class="token string">'table-name'</span> <span class="token operator">=</span> <span class="token string">'order_detail'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h2 id="计算指标"><a href="#计算指标" class="headerlink" title="计算指标"></a>计算指标</h2><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span>    od<span class="token punctuation">.</span>id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>id order_id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>user_id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>province_id<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_id<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_name<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_num<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>order_price<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>create_time<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>operate_time<span class="token keyword">FROM</span>   <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span> <span class="token operator">*</span>     <span class="token keyword">FROM</span> order_info    <span class="token keyword">WHERE</span>       order_status <span class="token operator">=</span> <span class="token string">'2'</span>   <span class="token punctuation">)</span> oi   <span class="token keyword">JOIN</span>  <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span> <span class="token operator">*</span>    <span class="token keyword">FROM</span> order_detail  <span class="token punctuation">)</span> od   <span class="token keyword">ON</span> oi<span class="token punctuation">.</span>id <span class="token operator">=</span> od<span class="token punctuation">.</span>order_id<span class="token punctuation">;</span></code></pre><p>最终的计算的结果和MySQL中计算的结果是一样的，说明这个实时导入是没有错的，但是这样的我也是可以在这个session中测试一下，这个几个表是否能进行一个动态的变化，测试结果如下</p><h2 id="测试订单信息和订单详情表是否能够动态的监听"><a href="#测试订单信息和订单详情表是否能够动态的监听" class="headerlink" title="测试订单信息和订单详情表是否能够动态的监听"></a>测试订单信息和订单详情表是否能够动态的监听</h2><h3 id="1、订单信息的原本数据"><a href="#1、订单信息的原本数据" class="headerlink" title="1、订单信息的原本数据"></a>1、订单信息的原本数据</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824173439075.png" alt="原始信息"></p><h3 id="2、修改MySQL中的数据为"><a href="#2、修改MySQL中的数据为" class="headerlink" title="2、修改MySQL中的数据为"></a>2、修改MySQL中的数据为</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824173545868.png" alt="修改数据"></p><p>通过观察，修改的数据在当前的这一次查询当中是不能够发现的，但是在下一次查询的过程当中是可以进行一个捕捉到的</p>]]></content>
      
      
      <categories>
          
          <category> 大数据采集系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据采集系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduce-the-upsert-kafka-Connector</title>
      <link href="/2021/08/24/Introduce-the-upsert-kafka-Connector/"/>
      <url>/2021/08/24/Introduce-the-upsert-kafka-Connector/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduce-the-upsert-kafka-Connector"><a href="#Introduce-the-upsert-kafka-Connector" class="headerlink" title="Introduce-the-upsert-kafka-Connector"></a>Introduce-the-upsert-kafka-Connector</h1><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-149%3A+Introduce+the+upsert-kafka+Connector" target="_blank" rel="noopener">参考文档</a></p><h2 id="一、动机"><a href="#一、动机" class="headerlink" title="一、动机"></a>一、动机</h2><p>通过邮件列表和社区问题，许多用户已经表达了他们对 upsert Kafka 的需求。在查看邮件列表后，我们认为背后有 3 个原因：</p><ul><li>将压缩的 kafka 主题解释为更改日志流，该流将带有键的记录解释为 upsert 事件 [1-3]； </li><li>作为实时管道的一部分，加入多个流以进行丰富并将结果存储到 Kafka 主题中以供以后进一步计算。但是，结果可能包含更新事件。</li><li>作为实时管道的一部分，聚合数据流并将结果存储到 Kafka 主题中以供以后进一步计算。但是，结果可能包含更新事件。</li></ul><h2 id="二、公共接口"><a href="#二、公共接口" class="headerlink" title="二、公共接口"></a>二、公共接口</h2><h3 id="1、Upsert-kafka-连接器定义"><a href="#1、Upsert-kafka-连接器定义" class="headerlink" title="1、Upsert-kafka 连接器定义"></a>1、Upsert-kafka 连接器定义</h3><p>Upsert-kafka 连接器产生一个变更日志流，其中每个数据记录代表一个更新或删除事件。更准确地说，数据记录中的值被解释为同一记录键的最后一个值的“更新”，如果有的话（如果相应的键不存在，则更新将被视为插入）。使用表类比，更改日志流中的数据记录被解释为 UPSERT 又名 INSERT/UPDATE，因为任何具有相同键的现有行都将被覆盖。此外，空值以特殊方式解释：具有空值的记录表示“删除”。</p><h3 id="2、upsert-kafka-上的主键约束"><a href="#2、upsert-kafka-上的主键约束" class="headerlink" title="2、upsert-kafka 上的主键约束"></a>2、upsert-kafka 上的主键约束</h3><p>当 upsert-kafka 连接器用作接收器时，它的工作方式类似于现有的 HBase 接收器。Upsert-kafka sink 不需要 planner 发送 UPDATE_BEFORE 消息（在某些情况下，planner 可能仍然会发送 UPDATE_BEFORE 消息），并且会将 INSERT/UPDATE_AFTER 消息写为带有关键部分的普通 Kafka 记录，并将 DELETE 消息写为带有 null 的 Kafka 记录值（指示键的墓碑）。Flink 将通过主键列的值对数据进行分区来保证主键上的消息排序。</p><p>Upsert-kafka 源是一种变更日志源。变更日志源上的主键语义意味着物化变更日志 (INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE) 在主键约束上是唯一的。Flink 假设所有消息在主键上都是有序的。</p><p>在 Flink 中创建 upsert-kafka 表需要在表上声明主键。主键定义还控制哪些字段应该在 Kafka 的键中结束。因此，我们不需要 upsert-kafka 连接器中的 ‘key.fields’ 选项。默认情况下，主键字段也将存储在 Kafka 的值中。但是用户仍然可以使用选项“value.fields-include”来控制这种行为。</p><h3 id="3、Upsert-kafka-源"><a href="#3、Upsert-kafka-源" class="headerlink" title="3、Upsert-kafka 源"></a>3、Upsert-kafka 源</h3><p>一般来说，upsert-kafka源码的底层topic必须是<a href="https://kafka.apache.org/documentation/#compaction" target="_blank" rel="noopener">compacted</a>。另外，底层topic必须在同一个partition中包含所有key相同的数据，否则结果会出错。</p><p>目前，upsert-kafka 连接器不提供开始读取位置选项。必须从最早的偏移量读取 upsert-kafka 连接器。这是对数据完整性的保护，否则很难解释当用户指定从中间位置开始偏移时的行为以及如何处理从未见过密钥的删除事件。因此，更新插入-kafka连接器不提供类似的选项<code>'``scan.startup.mode'</code>， “和” （仅用于“ ”启动模式）。<code>scan.startup.specific-offsets'``'scan.startup.timestamp-millis'``properties.group.id'``group-offsets</code></p><h3 id="4、Upsert-kafka-接收器"><a href="#4、Upsert-kafka-接收器" class="headerlink" title="4、Upsert-kafka 接收器"></a>4、Upsert-kafka 接收器</h3><p>为了保证消息排序，upsert-kafka sink 将始终在主键字段上以 HASH 分区器模式工作。因此，我们不需要<code>sink.partitioner</code>upsert-kafka 连接器中的 ‘ ‘ 选项。 </p><p>5、Upsert-kafka 连接器选项</p><p>upsert-kafka Connector 中的选项很像 Kafka Connector。</p><table><thead><tr><th>connector</th><th>required</th><th>(none)</th><th>String</th><th>Specify what connector to use, here should be ‘upsert-kafka’.</th></tr></thead><tbody><tr><td>properties.bootstrap.servers</td><td>required</td><td>(none)</td><td>String</td><td>Comma separated list of Kafka brokers.</td></tr><tr><td>key.format</td><td>required</td><td>(none)</td><td>String</td><td>The format used to deserialize and serialize the key of Kafka record. Only insert-only format is supported.</td></tr><tr><td>value.format</td><td>required</td><td>(none)</td><td>String</td><td>The format used to deserialize and serialize the value of Kafka records.</td></tr><tr><td>topic</td><td>optional</td><td>(none)</td><td>String</td><td>Topic name(s) to read data from when the table is used as source or to write data to when the table is used as sink. Note, only one of “<code>topic-pattern</code>“ and “<code>topic</code>“ can be specified for sources. When the table is used as sink, the topic name is the topic to write data to. Note topic list is not supported for sinks.</td></tr><tr><td>topic-pattern</td><td>optional</td><td>(none)</td><td>String</td><td>The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of “<code>topic-pattern</code>“ and “<code>topic</code>“ can be specified for sources.</td></tr><tr><td>value.fields-include</td><td>optional</td><td>‘ALL’</td><td>String</td><td>Controls which fields should end up in the value as well, possible values -  ALL (all fields of the schema, even if they are part of e.g. the key)-  EXCEPT_KEY (all fields of the schema - fields of the key)</td></tr></tbody></table><p><strong>注意：</strong>仅支持将仅插入格式用作<code>'key.format'</code>和 ‘ <code>value.format</code>‘。我们将使用<code>ChangelogMode</code>格式的 来区分格式是否为仅插入。</p><h2 id="三、背景介绍"><a href="#三、背景介绍" class="headerlink" title="三、背景介绍"></a>三、背景介绍</h2><p>在某些场景中，比如GROUP BY聚合之后的结果，需要去更新之前的结果值。这个时候，需要将 Kafka 消息记录的 key 当成主键处理，用来确定一条数据是应该作为插入、删除还是更新记录来处理。在Flink1.11中，可以通过 <strong>flink-cdc-connectors</strong> 项目提供的 <strong>changelog-json format</strong>来实现该功能。关于该功能的使用，见之前的分享<a href="https://mp.weixin.qq.com/s?__biz=MzU2ODQ3NjYyMA==&amp;mid=2247484891&amp;idx=2&amp;sn=5fcf4ec7c4519cf69528d66caccbdd95&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Flink1.11中的CDC Connectors操作实践</a>。</p><p>在Flink1.12版本中， 新增了一个 **upsert connector(upsert-kafka)**，该 connector 扩展自现有的 Kafka connector，工作在 upsert 模式（FLIP-149）下。新的 upsert-kafka connector 既可以作为 source 使用，也可以作为 sink 使用，并且提供了与现有的 kafka connector 相同的基本功能和持久性保证，因为两者之间复用了大部分代码。本文将以Flink1.13.2为例，介绍该功能的基本使用步骤，以下是全文，希望对你有所帮助。</p><h3 id="1-Upsert-Kafka-connector简介"><a href="#1-Upsert-Kafka-connector简介" class="headerlink" title="1 Upsert Kafka connector简介"></a>1 Upsert Kafka connector简介</h3><p><strong>Upsert Kafka Connector</strong>允许用户以upsert的方式从Kafka主题读取数据或将数据写入Kafka主题。</p><p><strong>当作为数据源时</strong>，upsert-kafka Connector会生产一个changelog流，其中每条数据记录都表示一个更新或删除事件。更准确地说，如果不存在对应的key，则视为<strong>INSERT</strong>操作。如果已经存在了相对应的key，则该key对应的value值为最后一次更新的值。</p><p>用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。</p><p><strong>当作为数据汇时</strong>，upsert-kafka Connector会消费一个changelog流。它将<strong>INSERT / UPDATE_AFTER</strong>数据作为正常的Kafka消息值写入(即INSERT和UPDATE操作，都会进行正常写入，如果是更新，则同一个key会存储多条数据，但在读取该表数据时，只保留最后一次更新的值)，并将 DELETE 数据以 value 为空的 Kafka 消息写入（key被打上墓碑标记，表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中</p><h3 id="2-依赖"><a href="#2-依赖" class="headerlink" title="2 依赖"></a>2 依赖</h3><p>为了使用Upsert Kafka连接器，需要添加下面的依赖</p><pre class=" language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-kafka_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.13.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><h3 id="3-使用方式"><a href="#3-使用方式" class="headerlink" title="3 使用方式"></a>3 使用方式</h3><p><strong>使用样例</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 创建一张kafka表，用户存储sink的数据</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> pageviews_per_region <span class="token punctuation">(</span>  user_region STRING<span class="token punctuation">,</span>  pv <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>  uv <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>user_region<span class="token punctuation">)</span> <span class="token operator">NOT</span> ENFORCED<span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>  <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'upsert-kafka'</span><span class="token punctuation">,</span>  <span class="token string">'topic'</span> <span class="token operator">=</span> <span class="token string">'pageviews_per_region'</span><span class="token punctuation">,</span>  <span class="token string">'properties.bootstrap.servers'</span> <span class="token operator">=</span> <span class="token string">'flink04:9092,flink05:9092,flink06:9092'</span><span class="token punctuation">,</span>  <span class="token string">'key.format'</span> <span class="token operator">=</span> <span class="token string">'avro'</span><span class="token punctuation">,</span>  <span class="token string">'value.format'</span> <span class="token operator">=</span> <span class="token string">'avro'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>创建表之后出现问题</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824161258282.png" alt="出现问题"></p><p>尖叫提示：</p><p>要使用 upsert-kafka connector，必须在创建表时使用<strong>PRIMARY KEY</strong>定义主键，并为键（key.format）和值（value.format）指定序列化反序列化格式。</p><p><strong>upsert-kafka connector参数</strong></p><ul><li><strong>connector</strong></li></ul><p><strong>必选</strong>。指定要使用的连接器，Upsert Kafka 连接器使用：<code>'upsert-kafka'</code>。</p><ul><li><strong>topic</strong></li></ul><p><strong>必选</strong>。用于读取和写入的 Kafka topic 名称。</p><ul><li><strong>properties.bootstrap.servers</strong></li></ul><p><strong>必选</strong>。以逗号分隔的 Kafka brokers 列表。</p><ul><li><strong>key.format</strong></li></ul><p><strong>必选</strong>。用于对 Kafka 消息中 key 部分序列化和反序列化的格式。key 字段由<strong>PRIMARY KEY</strong> 语法指定。支持的格式包括 <code>'csv'</code>、<code>'json'</code>、<code>'avro'</code>。</p><ul><li><strong>value.format</strong></li></ul><p><strong>必选</strong>。用于对 Kafka 消息中 value 部分序列化和反序列化的格式。支持的格式包括 <code>'csv'</code>、<code>'json'</code>、<code>'avro'</code>。</p><ul><li><strong>properties.</strong>*</li></ul><p><strong>可选</strong>。该选项可以传递任意的 Kafka 参数。选项的后缀名必须匹配定义在 Kafka 参数文档中的参数名。Flink 会自动移除 选项名中的 “properties.” 前缀，并将转换后的键名以及值传入 KafkaClient。例如，你可以通过 </p><p><code>'properties.allow.auto.create.topics' = 'false'</code> 来禁止自动创建 topic。但是，某些选项，例如<code>'key.deserializer'</code> 和 <code>'value.deserializer'</code> 是不允许通过该方式传递参数，因为 Flink 会重写这些参数的值。</p><ul><li><strong>value.fields-include</strong></li></ul><p><strong>可选</strong>，默认为<strong>ALL</strong>。控制key字段是否出现在 value 中。当取<strong>ALL</strong>时，表示<code>消息的 value 部分将包含 schema 中所有的字段，包括定义为主键的字段。</code>当取<strong>EXCEPT_KEY</strong>时，表示记录的 value 部分包含 schema 的所有字段，定义为主键的字段除外。</p><ul><li><strong>key.fields-prefix</strong></li></ul><p><strong>可选</strong>。为了避免与value字段命名冲突，为key字段添加一个自定义前缀。默认前缀为空。一旦指定了key字段的前缀，必须在DDL中指明前缀的名称，但是在构建key的序列化数据类型时，将移除该前缀。见下面的示例。在需要注意的是：使用该配置属性，<strong>value.fields-include</strong>的值必须为<strong>EXCEPT_KEY</strong>。</p><p>尖叫提示：</p><p>如果指定了key字段前缀，但在DDL中并没有添加该前缀字符串，那么在向该表写入数时，会抛出下面异常：</p><p>[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.api.ValidationException: All fields in ‘key.fields’ must be prefixed with ‘qwe’ when option ‘key.fields-prefix’ is set but field ‘do_date’ is not prefixed.</p><pre class=" language-sql"><code class="language-sql"> <span class="token comment" spellcheck="true">-- 创建一张upsert表，当指定了qwe前缀，涉及的key必须指定qwe前缀</span> <span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> result_total_pvuv_min_prefix <span class="token punctuation">(</span>     qwedo_date     STRING<span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 统计日期，必须包含qwe前缀</span>     qwedo_min      STRING<span class="token punctuation">,</span>      <span class="token comment" spellcheck="true">-- 统计分钟，必须包含qwe前缀</span>     pv          <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 点击量</span>     uv          <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 一天内同个访客多次访问仅计算一个UV</span>     currenttime <span class="token keyword">TIMESTAMP</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true">-- 当前时间</span>     <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>qwedo_date<span class="token punctuation">,</span> qwedo_min<span class="token punctuation">)</span> <span class="token operator">NOT</span> ENFORCED <span class="token comment" spellcheck="true">-- 必须包含qwe前缀</span> <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>   <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'upsert-kafka'</span><span class="token punctuation">,</span>   <span class="token string">'topic'</span> <span class="token operator">=</span> <span class="token string">'result_total_pvuv_min_prefix'</span><span class="token punctuation">,</span>   <span class="token string">'properties.bootstrap.servers'</span> <span class="token operator">=</span> <span class="token string">'kms-2:9092,kms-3:9092,kms-4:9092'</span><span class="token punctuation">,</span>   <span class="token string">'key.json.ignore-parse-errors'</span> <span class="token operator">=</span> <span class="token string">'true'</span><span class="token punctuation">,</span>   <span class="token string">'value.json.fail-on-missing-field'</span> <span class="token operator">=</span> <span class="token string">'false'</span><span class="token punctuation">,</span>   <span class="token string">'key.format'</span> <span class="token operator">=</span> <span class="token string">'json'</span><span class="token punctuation">,</span>   <span class="token string">'value.format'</span> <span class="token operator">=</span> <span class="token string">'json'</span><span class="token punctuation">,</span>   <span class="token string">'key.fields-prefix'</span><span class="token operator">=</span><span class="token string">'qwe'</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">-- 指定前缀qwe</span>   <span class="token string">'value.fields-include'</span> <span class="token operator">=</span> <span class="token string">'EXCEPT_KEY'</span> <span class="token comment" spellcheck="true">-- key不出现kafka消息的value中</span> <span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 向该表中写入数据</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> result_total_pvuv_min_prefix <span class="token keyword">SELECT</span>   do_date<span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--  时间分区</span>   cast<span class="token punctuation">(</span>DATE_FORMAT <span class="token punctuation">(</span>access_time<span class="token punctuation">,</span><span class="token string">'HH:mm'</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> STRING<span class="token punctuation">)</span> <span class="token keyword">AS</span> do_min<span class="token punctuation">,</span><span class="token comment" spellcheck="true">-- 分钟级别的时间</span>   pv<span class="token punctuation">,</span>   uv<span class="token punctuation">,</span>   <span class="token keyword">CURRENT_TIMESTAMP</span> <span class="token keyword">AS</span> currenttime <span class="token comment" spellcheck="true">-- 当前时间</span> <span class="token keyword">from</span>   view_total_pvuv_min<span class="token punctuation">;</span></code></pre><ul><li><strong>sink.parallelism</strong></li></ul><p><strong>可选</strong>。定义 upsert-kafka sink 算子的并行度。默认情况下，由框架确定并行度，与上游链接算子的并行度保持一致。</p><h3 id="4-其他注意事项"><a href="#4-其他注意事项" class="headerlink" title="4 其他注意事项"></a>4 其他注意事项</h3><p><strong>Key和Value的序列化格式</strong></p><p>关于Key、value的序列化可以参考Kafka connector。值得注意的是，必须指定Key和Value的序列化格式，其中Key是通过<strong>PRIMARY KEY</strong>指定的。</p><p><strong>Primary Key约束</strong></p><p><strong>Upsert Kafka</strong> 工作在 upsert 模式（FLIP-149）下。当我们创建表时，需要在 DDL 中定义主键。具有相同key的数据，会存在相同的分区中。在 changlog source 上定义主键意味着在物化后的 changelog 上主键具有唯一性。定义的主键将决定哪些字段出现在 Kafka 消息的 key 中。</p><p><strong>一致性保障</strong></p><p>默认情况下，如果启用 checkpoint，Upsert Kafka sink 会保证至少一次将数据插入 Kafka topic。</p><p>这意味着，Flink 可以将具有相同 key 的重复记录写入 Kafka topic。但由于该连接器以 upsert 的模式工作，该连接器作为 source 读入时，可以确保具有相同主键值下仅最后一条消息会生效。因此，upsert-kafka 连接器可以像 HBase sink 一样实现幂等写入。</p><p><strong>分区水位线</strong></p><p>Flink 支持根据 Upsert Kafka 的 每个分区的数据特性发送相应的 watermark。当使用这个特性的时候，watermark 是在 Kafka consumer 内部生成的。合并每个分区生成的 watermark 的方式和 streaming shuffle 的方式是一致的(<strong>单个分区的输入取最大值，多个分区的输入取最小值</strong>)。数据源产生的 watermark 是取决于该 consumer 负责的所有分区中当前最小的 watermark。如果该 consumer 负责的部分分区是空闲的，那么整体的 watermark 并不会前进。在这种情况下，可以通过设置合适的 table.exec.source.idle-timeout 来缓解这个问题。</p><p><strong>数据类型</strong></p><p>Upsert Kafka 用字节bytes存储消息的 key 和 value，因此没有 schema 或数据类型。消息按格式进行序列化和反序列化，例如：csv、json、avro。不同的序列化格式所提供的数据类型有所不同，因此需要根据使用的序列化格式进行确定表字段的数据类型是否与该序列化类型提供的数据类型兼容。</p><h2 id="四、实施细则"><a href="#四、实施细则" class="headerlink" title="四、实施细则"></a>四、实施细则</h2><p>由于 upsert-kafka 连接器只产生不包含 UPDATE_BEFORE 消息的 upsert 流。但是，一些操作需要 UPDATE_BEFORE 消息才能正确处理，例如聚合。因此，我们需要一个物理节点来实现 upsert 流并生成带有完整更改消息的更改日志流。在物理运算符中，我们将使用状态来知道密钥是否是第一次被看到。该运算符将生成 INSERT 行，或额外为前一个图像生成 UPDATE_BEFORE 行，或生成所有列都填充有值的 DELETE 行。</p><p>规划者怎么知道要添加这样的物化算子呢？这取决于 FLIP-95 中引入的源的 ChangeMode。目前，我们仅支持仅插入或所有类型（例如 CDC 格式）的 ChangelogMode。在此 FLIP 中，我们将支持 [UPDATE_AFTER, DELETE] ChangelogMode，这表明源将在运行时仅发出 UPDATE_AFTER 和 DELETE 消息。当source的ChangelogMode为[UPDATE_AFTER, DELETE]时，planner会添加上面的物化算子。 </p><p>考虑到 Kafka 连接器和 upsert-kafka 连接器之间的相似性，我们应该重用引擎盖下的大部分代码，只引入不同的连接器工厂。</p><h2 id="五、兼容性、弃用和迁移计划"><a href="#五、兼容性、弃用和迁移计划" class="headerlink" title="五、兼容性、弃用和迁移计划"></a>五、兼容性、弃用和迁移计划</h2><p>此更改引入了一项新功能，但并不意味着任何兼容性问题。</p><h2 id="六、被拒绝的替代品"><a href="#六、被拒绝的替代品" class="headerlink" title="六、被拒绝的替代品"></a>六、被拒绝的替代品</h2><h3 id="1、在-Kafka-连接器中引入新属性-vs-引入-upsert-kafka-连接器"><a href="#1、在-Kafka-连接器中引入新属性-vs-引入-upsert-kafka-连接器" class="headerlink" title="1、在 Kafka 连接器中引入新属性 vs 引入 upsert-kafka 连接器"></a>1、在 Kafka 连接器中引入新属性 vs 引入 upsert-kafka 连接器</h3><ol><li>很难解释当用户指定从中间位置开始偏移时的行为（例如如何处理不存在的删除事件）。如果用户这样做是很危险的。所以我们目前没有在新的连接器中提供偏移选项。</li><li>这是对同一个 kafka 主题的不同视角/抽象（附加与更新插入）。如果我们可以将它们分开而不是将它们混合在一个连接器中会更容易理解。新连接器需要散列接收器分区器、主键声明、常规格式。如果我们将它们混合在一个连接器中，则可能会混淆如何正确使用这些选项。</li><li>upsert-kafka 连接器的语义与 Kafka Stream 中的 KTable 相同。所以对于Kafka Stream和KSQL用户来说非常方便。我们在邮件列表中看到了几个问题 [1][2]，询问如何对 KTable 进行建模以及如何在 Flink SQL 中加入 KTable。</li></ol><h3 id="2、使用-upsert-kafka-作为新的连接器名称-vs-使用-kafka-compacted-作为名称-vs-使用-ktable-作为名称"><a href="#2、使用-upsert-kafka-作为新的连接器名称-vs-使用-kafka-compacted-作为名称-vs-使用-ktable-作为名称" class="headerlink" title="2、使用 upsert-kafka 作为新的连接器名称 vs 使用 kafka- compacted 作为名称 vs 使用 ktable 作为名称"></a>2、使用 upsert-kafka 作为新的连接器名称 vs 使用 kafka- compacted 作为名称 vs 使用 ktable 作为名称</h3><p>考虑到 KTable 的隐含含义比预期的要多，并且 kafka-compacted 中的压缩含义与 topic 而不是 table 本身相关，因此更适合使用 upsert-kafka 作为新连接器的名称，这更直接。</p><h2 id="七、未来工作"><a href="#七、未来工作" class="headerlink" title="七、未来工作"></a>七、未来工作</h2><h3 id="1、支持有界-upsert-kafka-源"><a href="#1、支持有界-upsert-kafka-源" class="headerlink" title="1、支持有界 upsert-kafka 源"></a>1、支持有界 upsert-kafka 源</h3><p>用户也可能希望以批处理模式使用 upsert-kafka 连接器。但是我们需要更多地讨论这个特性。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Netflix全程无锁</title>
      <link href="/2021/08/24/Netflix%E5%85%A8%E7%A8%8B%E6%97%A0%E9%94%81/"/>
      <url>/2021/08/24/Netflix%E5%85%A8%E7%A8%8B%E6%97%A0%E9%94%81/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.12.0源码阅读</title>
      <link href="/2021/08/24/Flink1-12-0%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
      <url>/2021/08/24/Flink1-12-0%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink1-12-0源码阅读"><a href="#Flink1-12-0源码阅读" class="headerlink" title="Flink1.12.0源码阅读"></a>Flink1.12.0源码阅读</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="一、任务提交流程"><a href="#一、任务提交流程" class="headerlink" title="一、任务提交流程"></a>一、任务提交流程</h3><ol><li>命令行提交命令</li><li>20多个步骤、几十个类、数千行代码，关键源码加注释</li><li>16个小节</li><li>PPT动图</li></ol><h3 id="二、组件通信"><a href="#二、组件通信" class="headerlink" title="二、组件通信"></a>二、组件通信</h3><ol><li>actor模型，Akka基本原理和实现</li><li>5大关键角色</li><li>代理转发、处理细节</li><li>PPT动图</li></ol><h3 id="三、任务调度"><a href="#三、任务调度" class="headerlink" title="三、任务调度"></a>三、任务调度</h3><ol><li>图：流图、作业图、执行图、物理执行图</li><li>调用位置、如何转换</li><li>task调度：调度器、调度策略、调度模型</li><li>task执行、以map算子为例</li></ol><h3 id="四、内存管理"><a href="#四、内存管理" class="headerlink" title="四、内存管理"></a>四、内存管理</h3><ol><li>Flink 1.10之后的管理模型：jobmanager,taskmanager</li><li>模型有效避免JVM内存的不足</li><li>分配过程、内存管理的概念、数据结构，特有的管理器</li><li>网络传输的内存管理，数据传输</li><li>反压过程</li></ol><h2 id="一、任务提交流程-1"><a href="#一、任务提交流程-1" class="headerlink" title="一、任务提交流程"></a>一、任务提交流程</h2><h3 id="1、程序入口"><a href="#1、程序入口" class="headerlink" title="1、程序入口"></a>1、程序入口</h3><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">flink</span> <span class="token attr-value">run >>> 里面有很多的依赖环境，有脚本的获取</span>CliFrontendYarnJobClusterEntrypoint<span class="token attr-name">TaskExecutorRunner</span> <span class="token attr-value">>>> TaskManagerRunner(Standalone)</span>org.apache.flink.client.cli.CliFrontend<span class="token attr-name">config.sh</span> <span class="token attr-value">环境信息获取</span></code></pre><h3 id="2、提交流程参数解析"><a href="#2、提交流程参数解析" class="headerlink" title="2、提交流程参数解析"></a>2、提交流程参数解析</h3><p>Ctrl + N —&gt; CliFrontend</p><p>Ctrl + F12 可以快速的筛选，然后直接输入main就可以快速的找到了</p><pre class=" language-properties"><code class="language-properties">1、获取flink的conf目录的路径<span class="token attr-name">main</span> <span class="token attr-value">-> getConfigurationDirectoryFromEnv</span>2、根据conf路径，加载配置<span class="token attr-name">loadConfiguration，我们不是配置了Flink</span> <span class="token attr-value">yaml文件吗</span>3、封装命令行接口：按顺序Generic、Yarn、Default，如何区分是什么集群loadCustomCommandLines<span class="token attr-name">4、runSecured</span> <span class="token attr-value">// 对封装好的参数进行解析</span><span class="token attr-name">    ---></span> <span class="token attr-value">run ---> CliFrontendParser</span><span class="token attr-name">    对我们提交的参数进行解析</span> <span class="token attr-value">-t -s -d 具体的赋值，参数值</span></code></pre><h3 id="3、提交流程，选择哪种命令行客户端"><a href="#3、提交流程，选择哪种命令行客户端" class="headerlink" title="3、提交流程，选择哪种命令行客户端"></a>3、提交流程，选择哪种命令行客户端</h3>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQLbinlog日志学习</title>
      <link href="/2021/08/24/MySQLbinlog%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/24/MySQLbinlog%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink-CDC-2.0-正式发布-详解核心改进</title>
      <link href="/2021/08/24/Flink-CDC-2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83-%E8%AF%A6%E8%A7%A3%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B/"/>
      <url>/2021/08/24/Flink-CDC-2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83-%E8%AF%A6%E8%A7%A3%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-CDC-2-0-正式发布-详解核心改进"><a href="#Flink-CDC-2-0-正式发布-详解核心改进" class="headerlink" title="Flink-CDC-2.0-正式发布-详解核心改进"></a>Flink-CDC-2.0-正式发布-详解核心改进</h1><h2 id="一、CDC-概述"><a href="#一、CDC-概述" class="headerlink" title="一、CDC 概述"></a>一、CDC 概述</h2><p>CDC 的全称是 Change Data Capture ，在广义的概念上，只要是能捕获数据变更的技术，我们都可以称之为 CDC 。目前通常描述的 CDC 技术主要面向数据库的变更，是一种用于捕获数据库中数据变更的技术。CDC 技术的应用场景非常广泛：</p><ul><li><strong>数据同步：</strong>用于备份，容灾；</li><li><strong>数据分发：</strong>一个数据源分发给多个下游系统；</li><li><strong>数据采集：</strong>面向数据仓库 / 数据湖的 ETL 数据集成，是非常重要的数据源。</li></ul><p>CDC 的技术方案非常多，目前业界主流的实现机制可以分为两种：</p><ul><li><strong>基于查询的 CDC：</strong><ul><li>离线调度查询作业，批处理。把一张表同步到其他系统，每次通过查询去获取表中最新的数据；</li><li>无法保障数据一致性，查的过程中有可能数据已经发生了多次变更；</li><li>不保障实时性，基于离线调度存在天然的延迟。</li></ul></li><li><strong>基于日志的 CDC：</strong><ul><li>实时消费日志，流处理，例如 MySQL 的 binlog 日志完整记录了数据库中的变更，可以把 binlog 文件当作流的数据源；</li><li>保障数据一致性，因为 binlog 文件包含了所有历史变更明细；</li><li>保障实时性，因为类似 binlog 的日志文件是可以流式消费的，提供的是实时数据。</li></ul></li></ul><p>对比常见的开源 CDC 方案，我们可以发现：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824142945266.png" alt="常见开源CDC方案比较"></p><ul><li>对比增量同步能力，<ul><li>基于日志的方式，可以很好的做到增量同步；</li><li>而基于查询的方式是很难做到增量同步的。</li></ul></li><li>对比全量同步能力，基于查询或者日志的 CDC 方案基本都支持，除了 Canal。</li><li>而对比全量 + 增量同步的能力，只有 Flink CDC、Debezium、Oracle Goldengate 支持较好。</li><li>从架构角度去看，该表将架构分为单机和分布式，这里的分布式架构不单纯体现在数据读取能力的水平扩展上，更重要的是在大数据场景下分布式系统接入能力。例如 Flink CDC 的数据入湖或者入仓的时候，下游通常是分布式的系统，如 Hive、HDFS、Iceberg、Hudi 等，那么从对接入分布式系统能力上看，Flink CDC 的架构能够很好地接入此类系统。</li><li>在数据转换 / 数据清洗能力上，当数据进入到 CDC 工具的时候是否能较方便的对数据做一些过滤或者清洗，甚至聚合？<ul><li>在 Flink CDC 上操作相当简单，可以通过 Flink SQL 去操作这些数据；</li><li>但是像 DataX、Debezium 等则需要通过脚本或者模板去做，所以用户的使用门槛会比较高。</li></ul></li><li>另外，在生态方面，这里指的是下游的一些数据库或者数据源的支持。Flink CDC 下游有丰富的 Connector，例如写入到 TiDB、MySQL、Pg、HBase、Kafka、ClickHouse 等常见的一些系统，也支持各种自定义 connector。</li></ul><h2 id="二、Flink-CDC-项目"><a href="#二、Flink-CDC-项目" class="headerlink" title="二、Flink CDC 项目"></a>二、Flink CDC 项目</h2><p>讲到这里，先带大家回顾下开发 Flink CDC 项目的动机。</p><h3 id="1-Dynamic-Table-amp-ChangeLog-Stream"><a href="#1-Dynamic-Table-amp-ChangeLog-Stream" class="headerlink" title="1. Dynamic Table &amp; ChangeLog Stream"></a>1. Dynamic Table &amp; ChangeLog Stream</h3><p>大家都知道 Flink 有两个基础概念：Dynamic Table 和 Changelog Stream。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143237941.png" alt="Flink变更日志流"></p><ul><li>Dynamic Table 就是 Flink SQL 定义的动态表，动态表和流的概念是对等的。参照上图，流可以转换成动态表，动态表也可以转换成流。</li><li>在 Flink SQL中，数据在从一个算子流向另外一个算子时都是以 Changelog Stream 的形式，任意时刻的 Changelog Stream 可以翻译为一个表，也可以翻译为一个流。</li></ul><p>联想下 MySQL 中的表和 binlog 日志，就会发现：MySQL 数据库的一张表所有的变更都记录在 binlog 日志中，如果一直对表进行更新，binlog 日志流也一直会追加，数据库中的表就相当于 binlog 日志流在某个时刻点物化的结果；日志流就是将表的变更数据持续捕获的结果。这说明 Flink SQL 的 Dynamic Table 是可以非常自然地表示一张不断变化的 MySQL 数据库表。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143525924.png" alt="数据格式"></p><p>在此基础上，我们调研了一些 CDC 技术，最终选择了 Debezium 作为 Flink CDC 的底层采集工具。Debezium 支持全量同步，也支持增量同步，也支持全量 + 增量的同步，非常灵活，同时基于日志的 CDC 技术使得提供 Exactly-Once 成为可能。</p><p>将 Flink SQL 的内部数据结构 RowData 和 Debezium 的数据结构进行对比，可以发现两者是非常相似的。</p><ul><li>每条 RowData 都有一个元数据 RowKind，包括 4 种类型， 分别是插入 (INSERT)、更新前镜像 (UPDATE_BEFORE)、更新后镜像 (UPDATE_AFTER)、删除 (DELETE)，这四种类型和数据库里面的 binlog 概念保持一致。</li><li>而 Debezium 的数据结构，也有一个类似的元数据 op 字段， op 字段的取值也有四种，分别是 c、u、d、r，各自对应 create、update、delete、read。对于代表更新操作的 u，其数据部分同时包含了前镜像 (before) 和后镜像 (after)。</li></ul><p>通过分析两种数据结构，Flink 和 Debezium 两者的底层数据是可以非常方便地对接起来的，大家可以发现 Flink 做 CDC 从技术上是非常合适的。</p><h3 id="2-传统-CDC-ETL-分析"><a href="#2-传统-CDC-ETL-分析" class="headerlink" title="2. 传统 CDC ETL 分析"></a>2. 传统 CDC ETL 分析</h3><p>我们来看下传统 CDC 的 ETL 分析链路，如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143809866.png" alt="传统的CDC ETL分析"></p><p>目前我们公司就是在使用的是传统的ETL技术来进行实现的</p><p>传统的基于 CDC 的 ETL 分析中，数据采集工具是必须的，国外用户常用 Debezium，国内用户常用阿里开源的 Canal，采集工具负责采集数据库的增量数据，一些采集工具也支持同步全量数据。采集到的数据一般输出到消息中间件如 Kafka，然后 Flink 计算引擎再去消费这一部分数据写入到目的端，目的端可以是各种 DB，数据湖，实时数仓和离线数仓。</p><p>注意，Flink 提供了 changelog-json format，可以将 changelog 数据写入离线数仓如 Hive / HDFS；对于实时数仓，Flink 支持将 changelog 通过 upsert-kafka connector 直接写入 Kafka。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144049616.png" alt="image-20210824144049616"></p><p>我们一直在思考是否可以使用 Flink CDC 去替换上图中虚线框内的采集组件和消息队列，从而简化分析链路，降低维护成本。同时更少的组件也意味着数据时效性能够进一步提高。答案是可以的，于是就有了我们基于 Flink CDC 的 ETL 分析流程。</p><h3 id="3-基于-Flink-CDC-的-ETL-分析"><a href="#3-基于-Flink-CDC-的-ETL-分析" class="headerlink" title="3. 基于 Flink CDC 的 ETL 分析"></a>3. 基于 Flink CDC 的 ETL 分析</h3><p>在使用了 Flink CDC 之后，除了组件更少，维护更方便外，另一个优势是通过 Flink SQL 极大地降低了用户使用门槛，可以看下面的例子：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144211235.png" alt="Flink CDC ETL分析"></p><p>该例子是通过 Flink CDC 去同步数据库数据并写入到 TiDB，用户直接使用 Flink SQL 创建了产品和订单的 MySQL-CDC 表，然后对数据流进行 JOIN 加工，加工后直接写入到下游数据库。通过一个 Flink SQL 作业就完成了 CDC 的数据分析，加工和同步。</p><p>大家会发现这是一个纯 SQL 作业，这意味着只要会 SQL 的 BI，业务线同学都可以完成此类工作。与此同时，用户也可以利用 Flink SQL 提供的丰富语法进行数据清洗、分析、聚合。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144319174.png" alt="Flink CDC的聚合分析"></p><p>而这些能力，对于现有的 CDC 方案来说，进行数据的清洗，分析和聚合是非常困难的。</p><p>此外，利用 <strong>Flink SQL 双流 JOIN、维表 JOIN、UDTF</strong> 语法可以非常容易地完成数据打宽，以及各种业务逻辑加工。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144524752.png" alt="Flink如何实现数据打宽"></p><h3 id="4-Flink-CDC-项目发展"><a href="#4-Flink-CDC-项目发展" class="headerlink" title="4. Flink CDC 项目发展"></a>4. Flink CDC 项目发展</h3><ul><li><p>2020 年 7 月由云邪提交了第一个 commit，这是基于个人兴趣孵化的项目；</p></li><li><p>2020 年 7 中旬支持了 MySQL-CDC；</p></li><li><p>2020 年 7 月末支持了 Postgres-CDC；</p></li><li><p>一年的时间，该项目在 GitHub 上的 star 数已经超过 800。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144641494.png" alt="image-20210824144641494"></p></li></ul><h2 id="三、Flink-CDC-2-0-详解"><a href="#三、Flink-CDC-2-0-详解" class="headerlink" title="三、Flink CDC 2.0 详解"></a>三、Flink CDC 2.0 详解</h2><h3 id="1-Flink-CDC-痛点"><a href="#1-Flink-CDC-痛点" class="headerlink" title="1. Flink CDC 痛点"></a>1. Flink CDC 痛点</h3><p>MySQL CDC 是 Flink CDC 中使用最多也是最重要的 Connector，本文下述章节描述 Flink CDC Connector 均为 MySQL CDC Connector。</p><p>随着 Flink CDC 项目的发展，得到了很多用户在社区的反馈，主要归纳为三个：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144838126.png" alt="Flink CDC痛点"></p><ul><li>全量 + 增量读取的过程需要保证所有数据的一致性，因此需要通过加锁保证，但是加锁在数据库层面上是一个十分高危的操作。底层 Debezium 在保证数据一致性时，需要对读取的库或表加锁，全局锁可能导致数据库锁住，表级锁会锁住表的读，DBA 一般不给锁权限。</li><li>不支持水平扩展，因为 Flink CDC 底层是基于 Debezium，起架构是单节点，所以Flink CDC 只支持单并发。在全量阶段读取阶段，如果表非常大 (亿级别)，读取时间在小时甚至天级别，用户不能通过增加资源去提升作业速度。</li><li>全量读取阶段不支持 checkpoint：CDC 读取分为两个阶段，全量读取和增量读取，目前全量读取阶段是不支持 checkpoint 的，因此会存在一个问题：当我们同步全量数据时，假设需要 5 个小时，当我们同步了 4 小时的时候作业失败，这时候就需要重新开始，再读取 5 个小时。</li></ul><h3 id="2-Debezium-锁分析"><a href="#2-Debezium-锁分析" class="headerlink" title="2. Debezium 锁分析"></a>2. Debezium 锁分析</h3><p>Flink CDC 底层封装了 Debezium， Debezium 同步一张表分为两个阶段：</p><ul><li><strong>全量阶段：</strong>查询当前表中所有记录；</li><li><strong>增量阶段：</strong>从 binlog 消费变更数据。</li></ul><p>大部分用户使用的场景都是全量 + 增量同步，加锁是发生在全量阶段，目的是为了确定全量阶段的初始位点，保证增量 + 全量实现一条不多，一条不少，从而保证数据一致性。从下图中我们可以分析全局锁和表锁的一些加锁流程，左边红色线条是锁的生命周期，右边是 MySQL 开启可重复读事务的生命周期。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145148162.png" alt="image-20210824145148162"></p><p>以全局锁为例，首先是获取一个锁，然后再去开启可重复读的事务。这里锁住操作是读取 binlog 的起始位置和当前表的 schema。这样做的目的是保证 binlog 的起始位置和读取到的当前 schema 是可以对应上的，因为表的 schema 是会改变的，比如如删除列或者增加列。在读取这两个信息后，SnapshotReader 会在可重复读事务里读取全量数据，在全量数据读取完成后，会启动 BinlogReader 从读取的 binlog 起始位置开始增量读取，从而保证全量数据 + 增量数据的无缝衔接。</p><p>表锁是全局锁的退化版，因为全局锁的权限会比较高，因此在某些场景，用户只有表锁。表锁锁的时间会更长，因为表锁有个特征：锁提前释放了可重复读的事务默认会提交，所以锁需要等到全量数据读完后才能释放。</p><p>经过上面分析，接下来看看这些锁到底会造成怎样严重的后果</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145357151.png" alt="image-20210824145357151"></p><p>Flink CDC 1.x 可以不加锁，能够满足大部分场景，但牺牲了一定的数据准确性。Flink CDC 1.x 默认加全局锁，虽然能保证数据一致性，但存在上述 hang 住数据的风险。</p><h3 id="3-Flink-CDC-2-0-设计-以-MySQL-为例"><a href="#3-Flink-CDC-2-0-设计-以-MySQL-为例" class="headerlink" title="3. Flink CDC 2.0 设计 ( 以 MySQL 为例)"></a>3. Flink CDC 2.0 设计 ( 以 MySQL 为例)</h3><p>通过上面的分析，可以知道 2.0 的设计方案，核心要解决上述的三个问题，即支持无锁、水平扩展、checkpoint。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145543291.png" alt="image-20210824145543291"></p><p>DBlog 这篇论文里描述的无锁算法如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150134772.png" alt="image-20210824150134772"></p><p>左边是 Chunk 的切分算法描述，Chunk 的切分算法其实和很多数据库的分库分表原理类似，通过表的主键对表中的数据进行分片。假设每个 Chunk 的步长为 10，按照这个规则进行切分，只需要把这些 Chunk 的区间做成左开右闭或者左闭右开的区间，保证衔接后的区间能够等于表的主键区间即可。</p><p>右边是每个 Chunk 的无锁读算法描述，该算法的核心思想是在划分了 Chunk 后，对于每个 Chunk 的全量读取和增量读取，在不用锁的条件下完成一致性的合并。Chunk 的切分如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150249154.png" alt="image-20210824150249154"></p><p>因为每个 chunk 只负责自己主键范围内的数据，不难推导，只要能够保证每个 Chunk 读取的一致性，就能保证整张表读取的一致性，这便是无锁算法的基本原理。</p><p>Netflix 的 DBLog 论文中 Chunk 读取算法是通过在 DB 维护一张信号表，再通过信号表在 binlog 文件中打点，记录每个 chunk 读取前的 Low Position (低位点) 和读取结束之后 High Position (高位点) ，在低位点和高位点之间去查询该 Chunk 的全量数据。在读取出这一部分 Chunk 的数据之后，再将这 2 个位点之间的 binlog 增量数据合并到 chunk 所属的全量数据，从而得到高位点时刻，该 chunk 对应的全量数据。</p><p>Flink CDC 结合自身的情况，在 Chunk 读取算法上做了去信号表的改进，不需要额外维护信号表，通过直接读取 binlog 位点替代在 binlog 中做标记的功能，整体的 chunk 读算法描述如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150514617.png" alt="image-20210824150514617"></p><p>比如正在读取 Chunk-1，Chunk 的区间是 [K1, K10]，首先直接将该区间内的数据 select 出来并把它存在 buffer 中，在 select 之前记录 binlog 的一个位点 (低位点)，select 完成后记录 binlog 的一个位点 (高位点)。然后开始增量部分，消费从低位点到高位点的 binlog。</p><ul><li>图中的 - ( k2,100 ) + ( k2,108 ) 记录表示这条数据的值从 100 更新到 108；</li><li>第二条记录是删除 k3；</li><li>第三条记录是更新 k2 为 119；</li><li>第四条记录是 k5 的数据由原来的 77 变更为 100。</li></ul><p>观察图片中右下角最终的输出，会发现在消费该 chunk 的 binlog 时，出现的 key 是k2、k3、k5，我们前往 buffer 将这些 key 做标记。</p><ul><li>对于 k1、k4、k6、k7 来说，在高位点读取完毕之后，这些记录没有变化过，所以这些数据是可以直接输出的；</li><li>对于改变过的数据，则需要将增量的数据合并到全量的数据中，只保留合并后的最终数据。例如，k2 最终的结果是 119 ，那么只需要输出 +(k2,119)，而不需要中间发生过改变的数据。</li></ul><p>通过这种方式，Chunk 最终的输出就是在高位点是 chunk 中最新的数据。</p><p>上图描述的是单个 Chunk 的一致性读，但是如果有多个表分了很多不同的 Chunk，且这些 Chunk 分发到了不同的 task 中，那么如何分发 Chunk 并保证全局一致性读呢？</p><p>这个就是基于 FLIP-27 来优雅地实现的，通过下图可以看到有 SourceEnumerator 的组件，这个组件主要用于 Chunk 的划分，划分好的 Chunk 会提供给下游的 SourceReader 去读取，通过把 chunk 分发给不同的 SourceReader 便实现了并发读取 Snapshot Chunk 的过程，同时基于 FLIP-27 我们能较为方便地做到 chunk 粒度的 checkpoint。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150831713.png" alt="image-20210824150831713"></p><p>当 Snapshot Chunk 读取完成之后，需要有一个汇报的流程，如下图中橘色的汇报信息，将 Snapshot Chunk 完成信息汇报给 SourceEnumerator。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151035349.png" alt="image-20210824151035349"></p><p>汇报的主要目的是为了后续分发 binlog chunk (如下图)。因为 Flink CDC 支持全量 + 增量同步，所以当所有 Snapshot Chunk 读取完成之后，还需要消费增量的 binlog，这是通过下发一个 binlog chunk 给任意一个 Source Reader 进行单并发读取实现的。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151132414.png" alt="image-20210824151132414"></p><p>对于大部分用户来讲，其实无需过于关注如何无锁算法和分片的细节，了解整体的流程就好。</p><p>整体流程可以概括为，首先通过主键对表进行 Snapshot Chunk 划分，再将 Snapshot Chunk 分发给多个 SourceReader，每个 Snapshot Chunk 读取时通过算法实现无锁条件下的一致性读，SourceReader 读取时支持 chunk 粒度的 checkpoint，在所有 Snapshot Chunk 读取完成后，下发一个 binlog chunk 进行增量部分的 binlog 读取，这便是 Flink CDC 2.0 的整体流程，如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151250393.png" alt="image-20210824151250393"></p><p>Flink CDC 是一个完全开源的项目，项目所有设计和源码目前都已贡献到开源社区，<a href="https://github.com/ververica/flink-cdc-connectors/releases/tag/release-2.0.0">Flink CDC 2.0</a> 也已经正式发布，此次的核心改进和提升包括：</p><ul><li>提供 MySQL CDC 2.0，核心feature 包括<ul><li>并发读取，全量数据的读取性能可以水平扩展；</li><li>全程无锁，不对线上业务产生锁的风险；</li><li>断点续传，支持全量阶段的 checkpoint。</li></ul></li><li>搭建文档网站，提供多版本文档支持，文档支持关键词搜索</li></ul><p>笔者用 TPC-DS 数据集中的 customer 表进行了测试，Flink 版本是 1.13.1，customer 表的数据量是 6500 万条，Source 并发为 8，全量读取阶段:</p><ul><li>MySQL CDC 2.0 用时 <strong>13</strong> 分钟；</li><li>MySQL CDC 1.4 用时 <strong>89</strong> 分钟；</li><li>读取性能提升 <strong>6.8</strong> 倍。</li></ul><p>为了提供更好的文档支持，Flink CDC 社区搭建了文档网站，网站支持对文档的版本管理：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152059965.png" alt="image-20210824152059965"></p><p>文档网站支持关键字搜索功能，非常实用：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152117345.png" alt="image-20210824152117345"></p><h2 id="四、未来规划"><a href="#四、未来规划" class="headerlink" title="四、未来规划"></a>四、未来规划</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152546448.png" alt="image-20210824152546448"></p><p>关于 CDC 项目的未来规划，我们希望围绕稳定性，进阶 feature 和生态集成三个方面展开。</p><ul><li><strong>稳定性</strong><ul><li>通过社区的方式吸引更多的开发者，公司的开源力量提升 Flink CDC 的成熟度；</li><li>支持 Lazy Assigning。Lazy Assigning 的思路是将 chunk 先划分一批，而不是一次性进行全部划分。当前 Source Reader 对数据读取进行分片是一次性全部划分好所有 chunk，例如有 1 万个 chunk，可以先划分 1 千个 chunk，而不是一次性全部划分，在 SourceReader 读取完 1 千 chunk 后再继续划分，节约划分 chunk 的时间。</li></ul></li><li><strong>进阶 Feature</strong><ul><li>支持 Schema Evolution。这个场景是：当同步数据库的过程中，突然在表中添加了一个字段，并且希望后续同步下游系统的时候能够自动加入这个字段；</li><li>支持 Watermark Pushdown 通过 CDC 的 binlog 获取到一些心跳信息，这些心跳的信息可以作为一个 Watermark，通过这个心跳信息可以知道到这个流当前消费的一些进度；</li><li>支持 META 数据，分库分表的场景下，有可能需要元数据知道这条数据来源哪个库哪个表，在下游系统入湖入仓可以有更多的灵活操作；</li><li>整库同步：用户要同步整个数据库只需一行 SQL 语法即可完成，而不用每张表定义一个 DDL 和 query。</li></ul></li><li><strong>生态集成</strong><ul><li>集成更多上游数据库，如 Oracle，MS SqlServer。Cloudera 目前正在积极贡献 oracle-cdc connector；</li><li>在入湖层面，Hudi 和 Iceberg 写入上有一定的优化空间，例如在高 QPS 入湖的时候，数据分布有比较大的性能影响，这一点可以通过与生态打通和集成继续优化。</li></ul></li></ul><p>最后，欢迎大家加入 Flink CDC 用户群一起交流。</p><h2 id="五、附录"><a href="#五、附录" class="headerlink" title="五、附录"></a>五、附录</h2><p><a href="https://github.com/ververica/flink-cdc-connectors?spm=a2csy.flink.0.0.49493bdcI4Avjy"><font color="red">[1]&nbsp; &nbsp; Flink CDC 项目地址</font></a></p><p><a href="https://ververica.github.io/flink-cdc-connectors/master/" target="_blank" rel="noopener"><font color="red">[2]&nbsp; &nbsp; Flink CDC 文档网站</font></a></p><p><a href="https://www.percona.com/blog/2014/03/11/introducing-backup-locks-percona-server-2/?spm=a2csy..0.0.55796c2e2qSuAf" target="_blank" rel="noopener"><font color="red">[3]&nbsp; &nbsp; Percona MySQL 全局锁时间分析</font></a></p><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface?spm=a2csy..0.0.55796c2e2qSuAf" target="_blank" rel="noopener"><font color="red">[4]&nbsp; &nbsp; Flink FLIP-27设计文档</font></a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据采集系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据采集系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入解读FlinkSQL1.13</title>
      <link href="/2021/08/24/%E6%B7%B1%E5%85%A5%E8%A7%A3%E8%AF%BBFlinkSQL1-13/"/>
      <url>/2021/08/24/%E6%B7%B1%E5%85%A5%E8%A7%A3%E8%AF%BBFlinkSQL1-13/</url>
      
        <content type="html"><![CDATA[<h1 id="深入解读FlinkSQL1-13"><a href="#深入解读FlinkSQL1-13" class="headerlink" title="深入解读FlinkSQL1.13"></a>深入解读FlinkSQL1.13</h1><p>参考网址：<a href="https://flink-learning.org.cn/article/detail/a8b0895d4271bf6b770927eea214612d" target="_blank" rel="noopener">https://flink-learning.org.cn/article/detail/a8b0895d4271bf6b770927eea214612d</a></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113303217.png" alt="Flink SQL核心功能"></p><ol><li>Flink SQL 1.13 概览</li><li>核心 feature 解读</li><li>重要改进解读</li><li>Flink SQL 1.14 未来规划</li><li>总结</li></ol><h2 id="一、Flink-SQL-1-13-概览"><a href="#一、Flink-SQL-1-13-概览" class="headerlink" title="一、Flink SQL 1.13 概览"></a>一、Flink SQL 1.13 概览</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113358728.png" alt="Flink SQL概览"></p><p>Flink 1.13 是一个社区大版本，解决的 issue 在 1000 个以上，通过上图我们可以看到，解决的问题大部分是关于 Table/SQL 模块，一共 400 多个 issue 占了总体的 37% 左右。这些 issue 主要围绕了 5 个 FLIP 展开，在本文中我们也会根据这 5 个方面进行介绍，它们分别是：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113444978.png" alt="Table SQL"></p><p>下面我们对这些 FLIP 进行详细解读。</p><h2 id="二、-核心-feature-解读"><a href="#二、-核心-feature-解读" class="headerlink" title="二、 核心 feature 解读"></a>二、 核心 feature 解读</h2><h3 id="1-FLIP-145：支持-Window-TVF"><a href="#1-FLIP-145：支持-Window-TVF" class="headerlink" title="1. FLIP-145：支持 Window TVF"></a>1. FLIP-145：支持 Window TVF</h3><p>社区的小伙伴应该了解，在腾讯、阿里巴巴、字节跳动等公司的内部分支已经开发了这个功能的基础版本。这次 Flink 社区也在 Flink 1.13 推出了 TVF 的相关支持和优化。下面将从 Window TVF 语法、近实时累计计算场景、 Window 性能优化、多维数据分析，来分析这个新功能。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113601029.png" alt="Window TVF"></p><h4 id="1-1-Window-TVF-语法"><a href="#1-1-Window-TVF-语法" class="headerlink" title="1.1 Window TVF 语法"></a>1.1 Window TVF 语法</h4><p>在 1.13 版本前，window 的实现是通过一个特殊的 SqlGroupedWindowFunction：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span>   TUMBLE_START<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  TUMBLE_END<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  TUMBLE_ROWTIME<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token function">SUM</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span><span class="token keyword">FROM</span> MyTable<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> TUMBLE<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span></code></pre><p>在 1.13 版本中，我们对它进行了 Table-Valued Function 的语法标准化：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end<span class="token punctuation">,</span>WINDOW_time<span class="token punctuation">,</span><span class="token function">SUM</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span> <span class="token keyword">FROM</span> <span class="token keyword">Table</span><span class="token punctuation">(</span>TUMBLE<span class="token punctuation">(</span><span class="token keyword">Table</span> myTable<span class="token punctuation">,</span>DESCRIPTOR<span class="token punctuation">(</span>biztime<span class="token punctuation">)</span><span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end</code></pre><p>通过对比两种语法，我们可以发现：TVF 语法更加灵活，不需要必须跟在 GROUP BY 关键字后面，同时 Window TVF 基于关系代数，使得其更加标准。在只需要划分窗口场景时，可以只用 TVF，无需用 GROUP BY 做聚合，这使得 TVF 扩展性和表达能力更强，支持自定义 TVF（例如实现 TOP-N 的 TVF）。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113822022.png" alt="TOP-N TVF"></p><p>上图中的示例就是利用 TVF 做的滚动窗口的划分，只需要把数据划分到窗口，无需聚合；如果后续需要聚合，再进行 GROP BY 即可。同时，对于熟悉批 SQL 的用户来说，这种操作是非常自然的，我们不再需要像 1.13 版本之前那样必须要用特殊的 SqlGroupedWindowFunction 将窗口划分和聚合绑定在一起。</p><p>目前 Window TVF 支持 tumble window，hop window，新增了 cumulate window；session window 预计在 1.14 版本也会支持。</p><h4 id="1-2-Cumulate-Window"><a href="#1-2-Cumulate-Window" class="headerlink" title="1.2 Cumulate Window"></a>1.2 Cumulate Window</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113914700.png" alt="cumulate window"></p><p>Cumulate window 就是累计窗口，简单来说，以上图里面时间轴上的一个区间为窗口步长。</p><ul><li>第一个 window 统计的是一个区间的数据；</li><li>第二个 window 统计的是第一区间和第二个区间的数据；</li><li>第三个 window 统计的是第一区间，第二个区间和第三个区间的数据。</li></ul><p>累积计算在业务场景中非常常见，如累积 UV 场景。在 UV 大盘曲线中：我们每隔 10 分钟统计一次当天累积用户 UV。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114137054.png" alt="近实时累计计算"></p><p>在 1.13 版本之前，当需要做这种计算时，我们一般的 SQL 写法如下：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> cumulative_UV<span class="token keyword">SELECT</span> date_str<span class="token punctuation">,</span><span class="token function">MAX</span><span class="token punctuation">(</span>time_str<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token keyword">DISTINCT</span> user_id<span class="token punctuation">)</span> <span class="token keyword">as</span> UV<span class="token keyword">FROM</span> <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span>      DATE_FORMAT<span class="token punctuation">(</span>ts<span class="token punctuation">,</span><span class="token string">'yyyy-MM-dd'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> date_str<span class="token punctuation">,</span>      SUBSTR<span class="token punctuation">(</span>DATE_FORMAT<span class="token punctuation">(</span>ts<span class="token punctuation">,</span><span class="token string">'HH:mm'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token operator">||</span> <span class="token string">'0'</span> <span class="token keyword">as</span> time_str<span class="token punctuation">,</span>      user_id  <span class="token keyword">FROM</span> user_behavior<span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> date_str</code></pre><p>先将每条记录所属的时间窗口字段拼接好，然后再对所有记录按照拼接好的时间窗口字段，通过 GROUP BY 做聚合，从而达到近似累积计算的效果。</p><ul><li>1.13 版本前的写法有很多缺点，首先这个聚合操作是每条记录都会计算一次。其次，在追逆数据的时候，消费堆积的数据时，UV 大盘的曲线就会跳变。</li><li>在 1.13 版本支持了 TVF 写法，基于 cumulate window，我们可以修改为下面的写法，将每条数据按照 Event Time 精确地分到每个 Window 里面, 每个窗口的计算通过 watermark 触发，即使在追数据场景中也不会跳变。</li></ul><pre class=" language-sql"><code class="language-sql"><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> cumulative_UV<span class="token keyword">SELECT</span> WINDOW_end<span class="token punctuation">,</span><span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token keyword">DISTINCT</span> user_id<span class="token punctuation">)</span> <span class="token keyword">as</span> UV<span class="token keyword">FROM</span> <span class="token keyword">Table</span><span class="token punctuation">(</span>    CUMULATE<span class="token punctuation">(</span><span class="token keyword">Table</span> user_behavior<span class="token punctuation">,</span>DESCRIPTOR<span class="token punctuation">(</span>ts<span class="token punctuation">)</span><span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTES<span class="token punctuation">,</span>INTERVAL <span class="token string">'1'</span> DAY<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end</code></pre><p>UV 大盘曲线效果如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114419320.png" alt="UV大盘曲线效果下图所示"></p><h4 id="1-3-Window-性能优化"><a href="#1-3-Window-性能优化" class="headerlink" title="1.3 Window 性能优化"></a>1.3 Window 性能优化</h4><p>Flink 1.13 社区开发者们对 Window TVF 进行了一系列的性能优化，包括：</p><ul><li><strong>内存优化：</strong>通过内存预分配，缓存 window 的数据，通过 window watermark 触发计算，通过申请一些内存 buffer 避免高频的访问 state；</li><li><strong>切片优化：</strong>将 window 切片，尽可能复用已计算结果，如 hop window，cumulate window。计算过的分片数据无需再次计算，只需对切片的计算结果进行复用；</li><li><strong>算子优化：</strong>window 算子支持 local-global 优化；同时支持 count(distinct) 自动解热点优化；</li><li><strong>迟到数据：</strong>支持将迟到数据计算到后续分片，保证数据准确性。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114505810.png" alt="Window性能优化"></p><p>基于这些优化，我们通过开源 Benchmark (Nexmark) 进行性能测试。结果显示 window 的普适性能有 2x 提升，且在 count(distinct) 场景会有更好的性能提升。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114537906.png" alt="Window性能优化"></p><h4 id="1-4-多维数据分析"><a href="#1-4-多维数据分析" class="headerlink" title="1.4 多维数据分析"></a>1.4 多维数据分析</h4><p>语法的标准化带来了更多的灵活性和扩展性，用户可以直接在 window 窗口函数上进行多维分析。如下图所示，可以直接进行 GROUPING SETS、ROLLUP、CUBE 的分析计算。如果是在 1.13 之前的版本，我们可能需要对这些分组进行单独的 SQL 聚合，再对聚合结果做 union 操作才能达到类似的效果。而现在，类似这种多维分析的场景，可以直接在 window TVF 上支持。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114730789.png" alt="多维数据分析"></p><h5 id="1-4-1-支持-Window-Top-N"><a href="#1-4-1-支持-Window-Top-N" class="headerlink" title="1.4.1 支持 Window Top-N"></a>1.4.1 <strong>支持 Window Top-N</strong></h5><p>除了多维分析，Window TVF 也支持 Top-N 语法，使得在 Window 上取 Top-N 的写法更加简单。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114826337.png" alt="多维数据分析SQL"></p><h3 id="2-FLIP-162：时区和时间函数"><a href="#2-FLIP-162：时区和时间函数" class="headerlink" title="2. FLIP-162：时区和时间函数"></a>2. FLIP-162：时区和时间函数</h3><h4 id="2-1-时区问题分析"><a href="#2-1-时区问题分析" class="headerlink" title="2.1 时区问题分析"></a>2.1 时区问题分析</h4><p>大家在使用 Flink SQL 时反馈了很多时区相关的问题，造成时区问题的原因可以归纳为 3 个：</p><ul><li>PROCTIME() 函数应该考虑时区，但未考虑时区；</li><li>CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/NOW() 函数未考虑时区；</li><li>Flink 的时间属性，只支持定义在 TIMESTAMP 这种数据类型上面，这个类型是无时区的，TIMESTAMP 类型不考虑时区，但用户希望是本地时区的时间。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114929478.png" alt="时区问题分析"></p><p>针对 TIMESTAMP 类型没有考虑时区的问题，我们提议通过TIMESTAMP_LTZ类型支持 (TIMESTAMP_LTZ 是 timestamp with local time zone 的缩写)。可以通过下面的表格来进行和 TIMESTAMP 的对比：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115024280.png" alt="时区问题分析"></p><p>TIMESTAMP_LTZ 区别于之前我们使用的 TIMESTAMP，它表示绝对时间的含义。通过对比我们可以发现：</p><ul><li>如果我们配置使用 TIMESTAMP，它可以是字符串类型的。用户不管是从英国还是中国时区来观察，这个值都是一样的；</li><li>但是对于 TIMSTAMP_TLZ 来说，它的来源就是一个 Long 值，表示从时间原点流逝过的时间。同一时刻，从时间原点流逝的时间在所有时区都是相同的，所以这个 Long 值是绝对时间的概念。当我们在不同的时区去观察这个值，我们会用本地的时区去解释成 “年-月-日-时-分-秒” 的可读格式，这就是 TIMSTAMP_TLZ 类型，TIMESTAMP_LTZ 类型也更加符合用户在不同时区下的使用习惯。</li></ul><p>下面的例子展示了 TIMESTAMP 和 TIMESTAMP_LTZ 两个类型的区别。</p><h4 id="2-2-时间函数纠正"><a href="#2-2-时间函数纠正" class="headerlink" title="2.2 时间函数纠正"></a>2.2 时间函数纠正</h4><p><strong>订正 PROCTIME() 函数</strong></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115208530.png" alt="时间函数纠正"></p><p>当我们有了 TIMESTAMP_LTZ 这个类型的时候，我们对 PROCTIME() 类型做了纠正：在 1.13 版本之前，它总是返回 UTC 的 TIMESTAMP；而现在，我们把返回类型变为了 TIMESTAMP_LTZ。PROCTIME 除了表示函数之外，也可以表示时间属性的标记。</p><p><strong>订正 CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/NOW() 函数</strong></p><p>这些函数在不同时区下出来的值是会发生变化的。例如在英国 UTC 时区时候是凌晨 2 点；但是如果你设置了时区是 UTC+8，时间就是在早上的 10 点。不同时区的实际时间会发生变化，效果如下图：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115254981.png" alt="时间函数的纠正"><strong>解决 processing time Window 时区问题</strong></p><p>大家都知道 proctime 可以表示一个时间属性，对 proctime 的 window 操作：</p><ul><li>在 1.13 版本之前，如果我们需要做按天的 window 操作，你需要手动解决时区问题，去做一些 8 小时的偏移然后再减回去；</li><li>在 FLIP-162 中我们解决了这个问题，现在用户使用的时候十分简单，只需要声明 proctime 属性，因为 PROCTIME() 函数的返回值是TIMESTAMP_LTZ，所以结果是会考虑本地的时区。下图的例子显示了在不同的时区下，proctime 属性的 window 的聚合是按照本地时区进行的。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121417679.png" alt="image-20210824121417679"></p><p><strong>订正 Streaming 和 Batch 模式下函数取值方式</strong></p><p>时间函数其实在流和批上面的表现形式会有所区别，这次修正主要是让其更加符合用户实际的使用习惯。例如以下函数：</p><ul><li>在流模式中是 per-record 计算，即每条数据都计算一次；</li><li>在 Batch 模式是 query-start 计算，即在作业开始前计算一次。例如我们常用的一些 Batch 计算引擎，如 Hive 也是在每一个批开始前计算一次。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121453936.png" alt="时间函数的纠正"></p><h4 id="2-3-时间类型使用"><a href="#2-3-时间类型使用" class="headerlink" title="2.3 时间类型使用"></a>2.3 时间类型使用</h4><p>在 1.13 版本也支持了在 TIMESTAMP 列上定义 Event time，也就是说Event time 现在既支持定义在 TIMESTAMP 列上，也支持定义在 TIMESTAMP_ LTZ 列上。那么作为用户，具体什么场景用什么类型呢？</p><ul><li>当作业的上游源数据包含了字符串的时间（如：2021-4-15 14:00:00）这样的场景，直接声明为 TIMESTAMP 然后把 Event time 定义在上面即可，窗口在计算的时候会基于时间字符串进行切分，最终会计算出符合你实际想要的预想结果；</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121528721.png" alt="时间类型的使用"></p><p>当上游数据源的打点时间属于 long 值，表示的是一个绝对时间的含义。在 1.13 版本你可以把 Event time 定义在 TIMESTAMP_LTZ 上面。此时定义在 TIMESTAMP_LTZ 类型上的各种 WINDOW 聚合，都能够自动的解决 8 小时的时区偏移问题，无需按照之前的 SQL 写法额外做时区的修改和订正。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121559194.png" alt="时间类型使用"></p><p>小提示：Flink SQL 中关于时间函数，时区支持的这些提升，是版本不兼容的。用户在进行版本更新的时候需要留意作业逻辑中是否包含此类函数，避免升级后业务受到影响。</p><h4 id="2-4-夏令时支持"><a href="#2-4-夏令时支持" class="headerlink" title="2.4 夏令时支持"></a>2.4 夏令时支持</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121631915.png" alt="夏令时支持"></p><p>在 Flink 1.13 以前，对于国外夏令时时区的用户，做窗口相关的计算操作是十分困难的一件事，因为存在夏令时和冬令时切换的跳变。</p><p>Flink 1.13 通过支持在 TIMESTAMP_LTZ 列上定义时间属性，同时 Flink SQL 在 WINDOW 处理时巧妙地结合 TIMESTAMP 和 TIMESTAMP_LTZ 类型，优雅地支持了夏令时。这对国外夏令时时区用户，以及有海外业务场景的公司比较有用。</p><h2 id="三、重要改进解读"><a href="#三、重要改进解读" class="headerlink" title="三、重要改进解读"></a>三、重要改进解读</h2><h3 id="1-FLIP-152：提升-Hive-语法兼容性"><a href="#1-FLIP-152：提升-Hive-语法兼容性" class="headerlink" title="1. FLIP-152：提升 Hive 语法兼容性"></a>1. FLIP-152：提升 Hive 语法兼容性</h3><p>FLIP-152 主要是做了 Hive 语法的兼容性增强，支持了 Hive 的一些常用 DML 和 DQL 语法，包括：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121758967.png" alt="提升hive语法兼容性问题"></p><p>通过 Hive dialect 支持 Hive 常用语法。Hive 有很多的内置函数，Hive dialect 需要配合 HiveCatalog 和 Hive Module 一起使用，Hive Module 提供了 Hive 所有内置函数，加载后可以直接访问。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121826256.png" alt="Hivedialect支持hive 常用语法"></p><p>与此同时，我们还可以通过 Hive dialect 创建/删除 Catalog 函数以及一些自定义的函数，这样使得 Flink SQL 与 Hive 的兼容性得到了极大的提升，让熟悉 Hive 的用户使用起来会更加方便。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121903273.png" alt="删除函数"></p><h3 id="2-FLIP-163：改进-SQL-Client"><a href="#2-FLIP-163：改进-SQL-Client" class="headerlink" title="2. FLIP-163：改进 SQL Client"></a>2. FLIP-163：改进 SQL Client</h3><p>在 1.13 版本之前，大家觉得 Flink SQL Client 就是周边的一个小工具。但是，FLIP-163 在 1.13 版本进行了重要改进：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121932578.png" alt="改进SQL Client"></p><ol><li>通过 -i 的参数，提前把 DDL 一次性加载初始化，方便初始化表的多个 DDL 语句，不需要多次执行命令创建表，替代了之前用 yaml 文件方式创建表；</li><li>支持 -f 参数，其中 SQL 文件支持 DML（insert into）语句；</li><li>支持更多实用的配置：</li></ol><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121955707.png" alt="SQL client配置"></p><ol><li><ul><li>通过 <strong>SET SQL-client.verbose = true</strong> , 开启 verbose，通过开启 verbose 打印整个信息，相对以前只输出一句话更加容易追踪错误信息；</li><li>通过 <strong>SET execution.runtime-mode=streaming / batch</strong> 支持设置批/流作业模式；</li><li>通过 <strong>SET pipline.name=my_Flink_job</strong> 设置作业名称；</li><li>通过 <strong>SET execution.savepoint.path=/tmp/Flink-savepoints/savepoint-bb0dab</strong> 设置作业 savepoint 路径；</li><li>对于有依赖的多个作业，通过 <strong>SET Table.dml-sync=true</strong> 去选择是否异步执行，例如离线作业，作业 a 跑完才能跑作业 b ，通过设置为 true 实现执行有依赖关系的 pipeline 调度。</li></ul></li></ol><p>4、同时支持 STATEMENT SET语法：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824122057735.png" alt="sql client"></p><p>有可能我们的一个查询不止写到一个 sink 里面，而是需要输出到多个 sink，比如一个 sink 写到 jdbc，一个 sink 写到 HBase。</p><ul><li>在 1.13 版本之前需要启动 2 个 query 去完成这个作业；</li><li>在 1.13 版本，我们可以把这些放到一个 statement 里面，以一个作业的方式去执行，能够实现节点的复用，节约资源。</li></ul><h3 id="3-FLIP-136：增强-DataStream-和-Table-的转换"><a href="#3-FLIP-136：增强-DataStream-和-Table-的转换" class="headerlink" title="3. FLIP-136：增强 DataStream 和 Table 的转换"></a>3. FLIP-136：增强 DataStream 和 Table 的转换</h3><p>虽然 Flink SQL 大大降低了我们使用实时计算的一些使用门槛，但 Table/SQL 这种高级封装也屏蔽了一些底层实现，如 timer，state 等。不少高级用户希望能够直接操作 DataStream 获得更多的灵活性，这就需要在 Table 和 DataStream 之间进行转换。FLIP-136 增强了 Table 和 DataStream 间的转换，使得用户在两者之间的转换更加容易。</p><ul><li>支持 DataStream 和 Table 转换时传递 EVENT TIME 和 WATERMARK；</li></ul><pre class=" language-java"><code class="language-java">Table Table <span class="token operator">=</span> TableEnv<span class="token punctuation">.</span><span class="token function">fromDataStream</span><span class="token punctuation">(</span>    dataStream<span class="token punctuation">,</span>  Schema<span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">columnByMetadata</span><span class="token punctuation">(</span><span class="token string">"rowtime"</span><span class="token punctuation">,</span><span class="token string">"TIMESTMP(3)"</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">watermark</span><span class="token punctuation">(</span><span class="token string">"rowtime"</span><span class="token punctuation">,</span><span class="token string">"SOURCE_WATERMARK()"</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">)</span></code></pre><ul><li>支持 Changelog 数据流在 Table 和 DataStream 间相互转换。</li></ul><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">//DATASTREAM 转 Table</span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">fromChangelogStream</span><span class="token punctuation">(</span>DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span><span class="token punctuation">)</span><span class="token operator">:</span> TableStreamTableEnvironment<span class="token punctuation">.</span><span class="token function">fromChangelogStream</span><span class="token punctuation">(</span>DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span><span class="token punctuation">,</span>Schema<span class="token punctuation">)</span><span class="token operator">:</span> Table<span class="token comment" spellcheck="true">//Table 转 DATASTREAM</span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">toChangelogStream</span><span class="token punctuation">(</span>Table<span class="token punctuation">)</span><span class="token operator">:</span> DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">toChangelogStream</span><span class="token punctuation">(</span>Table<span class="token punctuation">,</span>Schema<span class="token punctuation">)</span><span class="token operator">:</span> DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span>  </code></pre><h2 id="四、Flink-SQL-1-14-未来规划"><a href="#四、Flink-SQL-1-14-未来规划" class="headerlink" title="四、Flink SQL 1.14 未来规划"></a>四、Flink SQL 1.14 未来规划</h2><p>1.14 版本主要有以下几点规划：</p><ul><li><strong>删除 Legacy Planner</strong>：从 Flink 1.9 开始，在阿里贡献了 Blink-Planner 之后，很多一些新的 Feature 已经基于此 Blink Planner 进行开发，以前旧的 Legacy Planner 会彻底删除；</li><li><strong>完善 Window TVF</strong>：支持 session window，支持 window TVF 的 allow -lateness 等；</li><li><strong>提升 Schema Handling</strong>：全链路的 Schema 处理能力以及关键校验的提升；</li><li><strong>增强 Flink CDC 支持</strong>：增强对上游 CDC 系统的集成能力，Flink SQL 内更多的算子支持 CDC 数据流。</li></ul><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>本文详细解读了 Flink SQL 1.13 的核心功能和重要改进。</p><ul><li>支持 Window TVF；</li><li>系统地解决时区和时间函数问题；</li><li>提升 Hive 和 Flink 的兼容性；</li><li>改进 SQL Client；</li><li>增强 DataStream 和 Table 的转换。</li></ul><p>同时还分享了社区关于 Flink SQL 1.14 的未来规划，相信看完文章的同学可以对 Flink SQL 在这个版本中的变化有更多的了解，在实践过程中大家可以多多关注这些新的改动和变化，感受它们所带来的业务层面上的便捷。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ApacheFlink实战教程CEP实战</title>
      <link href="/2021/08/24/ApacheFlink%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8BCEP%E5%AE%9E%E6%88%98/"/>
      <url>/2021/08/24/ApacheFlink%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8BCEP%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h1 id="ApacheFlink实战教程CEP实战"><a href="#ApacheFlink实战教程CEP实战" class="headerlink" title="ApacheFlink实战教程CEP实战"></a>ApacheFlink实战教程CEP实战</h1><ol><li>Flink CEP 概念以及使用场景。</li><li>如何使用 Flink CEP。</li><li>如何扩展 Flink CEP。</li></ol><h2 id="Flink-CEP-概念以及使用场景"><a href="#Flink-CEP-概念以及使用场景" class="headerlink" title="Flink CEP 概念以及使用场景"></a>Flink CEP 概念以及使用场景</h2><h3 id="1-什么是-CEP"><a href="#1-什么是-CEP" class="headerlink" title="1.什么是 CEP"></a>1.什么是 CEP</h3><p>CEP 的意思是复杂事件处理，例如：起床–&gt;洗漱–&gt;吃饭–&gt;上班等一系列串联起来的事件流形成的模式称为 CEP。如果发现某一次起床后没有刷牙洗脸亦或是吃饭就直接上班，就可以把这种非正常的事件流匹配出来进行分析，看看今天是不是起晚了。</p><p>下图中列出了几个例子：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824104216837.png" alt="Flink CEP Example"></p><ul><li><strong>第一个是异常行为检测的例子：</strong>假设车辆维修的场景中，当一辆车出现故障时，这辆车会被送往维修点维修，然后被重新投放到市场运行。如果这辆车被投放到市场之后还未被使用就又被报障了，那么就有可能之前的维修是无效的。</li><li><strong>第二个是策略营销的例子：</strong>假设打车的场景中，用户在 APP 上规划了一个行程订单，如果这个行程在下单之后超过一定的时间还没有被司机接单的话，那么就需要将这个订单输出到下游做相关的策略调整。</li><li><strong>第三个是运维监控的例子：</strong>通常运维会监控服务器的 CPU、网络 IO 等指标超过阈值时产生相应的告警。但是在实际使用中，后台服务的重启、网络抖动等情况都会造成瞬间的流量毛刺，对非关键链路可以忽略这些毛刺而只对频繁发生的异常进行告警以减少误报。</li></ul><h3 id="2-Flink-CEP-应用场景"><a href="#2-Flink-CEP-应用场景" class="headerlink" title="2.Flink CEP 应用场景"></a>2.Flink CEP 应用场景</h3><ul><li><strong>风险控制：</strong>对用户异常行为模式进行实时检测，当一个用户发生了不该发生的行为，判定这个用户是不是有违规操作的嫌疑。</li><li><strong>策略营销：</strong>用预先定义好的规则对用户的行为轨迹进行实时跟踪，对行为轨迹匹配预定义规则的用户实时发送相应策略的推广。</li><li><strong>运维监控：</strong>灵活配置多指标、多依赖来实现更复杂的监控模式。</li></ul><h3 id="3-Flink-CEP-原理"><a href="#3-Flink-CEP-原理" class="headerlink" title="3.Flink CEP 原理"></a>3.Flink CEP 原理</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824104701307.png" alt="Flink CEP原理图"></p><p>Flink CEP 内部是用 NFA（非确定有限自动机）来实现的，由点和边组成的一个状态图，以一个初始状态作为起点，经过一系列的中间状态，达到终态。点分为起始状态、中间状态、最终状态三种，边分为 take、ignore、proceed 三种。</p><ul><li><strong>take</strong>：必须存在一个条件判断，当到来的消息满足 take 边条件判断时，把这个消息放入结果集，将状态转移到下一状态。</li><li><strong>ignore</strong>：当消息到来时，可以忽略这个消息，将状态自旋在当前不变，是一个自己到自己的状态转移。</li><li><strong>proceed</strong>：又叫做状态的空转移，当前状态可以不依赖于消息到来而直接转移到下一状态。举个例子，当用户购买商品时，如果购买前有一个咨询客服的行为，需要把咨询客服行为和购买行为两个消息一起放到结果集中向下游输出；如果购买前没有咨询客服的行为，只需把购买行为放到结果集中向下游输出就可以了。 也就是说，如果有咨询客服的行为，就存在咨询客服状态的上的消息保存，如果没有咨询客服的行为，就不存在咨询客服状态的上的消息保存，咨询客服状态是由一条 proceed 边和下游的购买状态相连。</li></ul><p>下面以一个打车的例子来展示状态是如何流转的，规则见下图所示。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105008651.png" alt="打车案例"></p><p>以乘客制定行程作为开始，匹配乘客的下单事件，如果这个订单超时还没有被司机接单的话，就把行程事件和下单事件作为结果集往下游输出。</p><p>假如消息到来顺序为：行程–&gt;其他–&gt;下单–&gt;其他。</p><p>状态流转如下：</p><p>1.开始时状态处于行程状态，即等待用户制定行程。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105557056.png" alt="image-20210824105557056"></p><p>2.当收到行程事件时，匹配行程状态的条件，把行程事件放到结果集中，通过 take 边将状态往下转移到下单状态。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105707540.png" alt="image-20210824105707540"></p><p>3.由于下单状态上有一条 ignore 边，所以可以忽略收到的其他事件，直到收到下单事件时将其匹配，放入结果集中，并且将当前状态往下转移到超时未接单状态。这时候结果集当中有两个事件：制定行程事件和下单事件。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105843389.png" alt="image-20210824105843389"></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105851550.png" alt="image-20210824105851550"></p><p>4.超时未接单状态时，如果来了一些其他事件，同样可以被 ignore 边忽略，直到超时事件的触发，将状态往下转移到最终状态，这时候整个模式匹配成功，最终将结果集中的制定行程事件和下单事件输出到下游。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105952153.png" alt="image-20210824105952153"></p><p>上面是一个匹配成功的例子，如果是不成功的例子会怎么样？</p><p>假如当状态处于超时未接单状态时，收到了一个接单事件，那么就不符合超时未被接单的触发条件，此时整个模式匹配失败，之前放入结果集中的行程事件和下单事件会被清理。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110020762.png" alt="image-20210824110020762"></p><h2 id="Flink-CEP-程序开发"><a href="#Flink-CEP-程序开发" class="headerlink" title="Flink CEP 程序开发"></a>Flink CEP 程序开发</h2><p>本节将详细介绍 Flink CEP 的程序结构以及 API。</p><h3 id="1-Flink-CEP-程序结构"><a href="#1-Flink-CEP-程序结构" class="headerlink" title="1.Flink CEP 程序结构"></a>1.Flink CEP 程序结构</h3><p>主要分为两部分：定义事件模式和匹配结果处理。</p><p>官方示例如下：</p><pre class=" language-java"><code class="language-java">DataStream<span class="token operator">&lt;</span>Event<span class="token operator">></span> input <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>Pattern<span class="token operator">&lt;</span>Event<span class="token punctuation">,</span> <span class="token operator">?</span><span class="token operator">></span> pattern <span class="token operator">=</span> Pattern<span class="token punctuation">.</span>&lt;Event<span class="token operator">></span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getId</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">42</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token string">"middle"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">subtype</span><span class="token punctuation">(</span>SubEvent<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>SubEvent<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>SubEvent subEvent<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> subEvent<span class="token punctuation">.</span><span class="token function">getVolume</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">10.0</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">followedBy</span><span class="token punctuation">(</span><span class="token string">"end"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>         <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"end"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>         <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">;</span>PatternStream<span class="token operator">&lt;</span>Event<span class="token operator">></span> patternStream <span class="token operator">=</span> CEP<span class="token punctuation">.</span><span class="token function">pattern</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> pattern<span class="token punctuation">)</span><span class="token punctuation">;</span>DataStream<span class="token operator">&lt;</span>Alert<span class="token operator">></span> result <span class="token operator">=</span> patternStream<span class="token punctuation">.</span><span class="token function">select</span><span class="token punctuation">(</span>    <span class="token keyword">new</span> <span class="token class-name">PatternProcessFunction</span><span class="token operator">&lt;</span>Event<span class="token punctuation">,</span> Alert<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">select</span><span class="token punctuation">(</span>                Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> List<span class="token operator">&lt;</span>Event<span class="token operator">>></span> pattern<span class="token punctuation">,</span>                Context ctx<span class="token punctuation">,</span>                Collector<span class="token operator">&lt;</span>Alert<span class="token operator">></span> out<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token function">createAlertFrom</span><span class="token punctuation">(</span>pattern<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong>程序结构分为三部分：</strong>首先需要定义一个模式(<strong>Pattern</strong>)，即第 2 行代码所示，接着把定义好的模式绑定在 DataStream 上（<strong>第 25 行</strong>），最后就可以在具有 CEP 功能的 DataStream 上将匹配的结果进行处理（<strong>第 27 行</strong>）。下面对关键部分做详细讲解：</p><ul><li><strong>定义模式：</strong>上面示例中，分为了三步，首先匹配一个 ID 为 42 的事件，接着匹配一个体积大于等于 10 的事件，最后等待收到一个 name 等于 end 的事件。</li><li><strong>匹配结果输出：</strong>此部分，需要重点注意 select 函数（第 30 行，注：本文基于 Flink 1.7 版本）里边的 Map 类型的 pattern 参数，Key 是一个 pattern 的 name，它的取值是模式定义中的 Begin 节点 start，或者是接下来 next 里面的 middle，或者是第三个步骤的 end。后面的 map 中的 value 是每一步发生的匹配事件。因在每一步中是可以使用循环属性的，可以匹配发生多次，所以 map 中的 value 是匹配发生多次的所有事件的一个集合。</li></ul><h3 id="2-Flink-CEP-构成"><a href="#2-Flink-CEP-构成" class="headerlink" title="2.Flink CEP 构成"></a>2.Flink CEP 构成</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110422728.png" alt="image-20210824110422728"></p><p>上图中，蓝色方框代表的是一个个单独的模式；浅黄色的椭圆代表的是这个模式上可以添加的属性，包括模式可以发生的循环次数，或者这个模式是贪婪的还是可选的；橘色的椭圆代表的是模式间的关系，定义了多个模式之间是怎么样串联起来的。通过定义模式，添加相应的属性，将多个模式串联起来三步，就可以构成了一个完整的 Flink CEP 程序。</p><h4 id="2-1-定义模式"><a href="#2-1-定义模式" class="headerlink" title="2.1 定义模式"></a>2.1 定义模式</h4><p>下面是示例代码：</p><pre class=" language-java"><code class="language-java">pattern<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getId</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">42</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>定义模式主要有如下 5 个部分组成：</p><ul><li><strong>pattern</strong>：前一个模式</li><li><strong>next/followedBy/…</strong>：开始一个新的模式</li><li><strong>start</strong>：模式名称</li><li><strong>where</strong>：模式的内容</li><li><strong>filter</strong>：核心处理逻辑</li></ul><h4 id="2-2-模式的属性"><a href="#2-2-模式的属性" class="headerlink" title="2.2 模式的属性"></a>2.2 模式的属性</h4><p>接下来介绍一下怎样设置模式的属性。模式的属性主要分为循环属性和可选属性。</p><ul><li>循环属性可以定义模式匹配发生固定次数（<strong>times</strong>），匹配发生一次以上（<strong>oneOrMore</strong>），匹配发生多次以上(<strong>timesOrMore</strong>)。</li><li>可选属性可以设置模式是贪婪的（<strong>greedy</strong>），即匹配最长的串，或设置为可选的（<strong>optional</strong>），有则匹配，无则忽略。</li></ul><h4 id="2-3-模式的有效期"><a href="#2-3-模式的有效期" class="headerlink" title="2.3 模式的有效期"></a>2.3 模式的有效期</h4><p>由于模式的匹配事件存放在状态中进行管理，所以需要设置一个全局的有效期（within）。若不指定有效期，匹配事件会一直保存在状态中不会被清除。至于有效期能开多大，要依据具体使用场景和数据量来衡量，关键要看匹配的事件有多少，随着匹配的事件增多，新到达的消息遍历之前的匹配事件会增加 CPU、内存的消耗，并且随着状态变大，数据倾斜也会越来越严重。</p><h4 id="2-4-模式间的联系"><a href="#2-4-模式间的联系" class="headerlink" title="2.4 模式间的联系"></a>2.4 模式间的联系</h4><p>主要分为三种：严格连续性（next/notNext），宽松连续性（followedBy/notFollowedBy），和非确定宽松连续性（followedByAny）。</p><p>三种模式匹配的差别见下表所示：</p><table><thead><tr><th align="left"><strong>模式&amp;数据流</strong></th><th align="left"><strong>严格连续性</strong></th><th align="left"><strong>宽松连续性</strong></th><th align="left"><strong>非确定宽松连续性</strong></th></tr></thead><tbody><tr><td align="left">Pattern(A B) Streaming(‘a’,’c’,’b1′,’b2′)</td><td align="left">不匹配</td><td align="left">匹配 输出：a,b1</td><td align="left">匹配 输出：a,b1 a,b2</td></tr></tbody></table><p>总结如下：</p><ul><li><strong>严格连续性</strong>：需要消息的顺序到达与模式完全一致。</li><li><strong>宽松连续性</strong>：允许忽略不匹配的事件。</li><li><strong>非确定宽松连性</strong>：不仅可以忽略不匹配的事件，也可以忽略已经匹配的事件。</li></ul><h4 id="2-5-多模式组合"><a href="#2-5-多模式组合" class="headerlink" title="2.5 多模式组合"></a>2.5 多模式组合</h4><p>除了前面提到的模式定义和模式间的联系，还可以把相连的多个模式组合在一起看成一个模式组，类似于视图，可以在这个模式视图上进行相关操作。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110937444.png" alt="image-20210824110937444"></p><p>上图这个例子里面，首先匹配了一个登录事件，然后接下来匹配浏览，下单，购买这三个事件反复发生三次的用户。</p><p>如果没有模式组的话，代码里面浏览，下单，购买要写三次。有了模式组，只需把浏览，下单，购买这三个事件当做一个模式组，把相应的属性加上 times(3) 就可以了。</p><h4 id="2-6-处理结果"><a href="#2-6-处理结果" class="headerlink" title="2.6 处理结果"></a>2.6 处理结果</h4><p>处理匹配的结果主要有四个接口：PatternFlatSelectFunction，PatternSelectFunction，PatternFlatTimeoutFunction 和 PatternTimeoutFunction。</p><p>从名字上可以看出，输出可以分为两类：select 和 flatSelect 指定输出一条还是多条，timeoutFunction 和不带 timeout 的 Function 指定可不可以对超时事件进行旁路输出。</p><p>下图是输出的综合示例代码：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111048098.png" alt="image-20210824111048098"></p><h4 id="2-7-状态存储优化"><a href="#2-7-状态存储优化" class="headerlink" title="2.7 状态存储优化"></a>2.7 状态存储优化</h4><p>当一个事件到来时，如果这个事件同时符合多个输出的结果集，那么这个事件是如何保存的？</p><p>Flink CEP 通过 Dewey 计数法在多个结果集中共享同一个事件副本，以实现对事件副本进行资源共享。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111200461.png" alt="image-20210824111200461"></p><h2 id="Flink-CEP-的扩展"><a href="#Flink-CEP-的扩展" class="headerlink" title="Flink CEP 的扩展"></a>Flink CEP 的扩展</h2><p>本章主要介绍一些 Flink CEP 的扩展，讲述如何做到超时机制的精确管理，以及规则的动态加载与更新。</p><h3 id="1-超时触发机制扩展"><a href="#1-超时触发机制扩展" class="headerlink" title="1.超时触发机制扩展"></a>1.超时触发机制扩展</h3><p>原生 Flink CEP 中超时触发的功能可以通过 within+outputtag 结合来实现，但是在复杂的场景下处理存在问题，如下图所示，在下单事件后还有一个预付款事件，想要得到下单并且预付款后超时未被接单的订单，该如何表示呢？</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111320576.png" alt="image-20210824111320576"></p><p>参照下单后超时未被接单的做法，把下单并且预付款后超时未被接单规则表示为下单**.followedBy(预付款).followedBy(接单).within(time)**，那么这样实现会存在问题吗？</p><p>这种做法的计算结果是会存在脏数据的，因为这个规则不仅匹配到了下单并且预付款后超时未被接单的订单（想要的结果），同样还匹配到了只有下单行为后超时未被接单的订单（脏数据，没有预付款）。原因是因为超时 within 是控制在整个规则上，而不是某一个状态节点上，所以不论当前的状态是处在哪个状态节点，超时后都会被旁路输出。</p><p>那么就需要考虑能否通过时间来直接对状态转移做到精确的控制，而不是通过规则超时这种曲线救国的方式。于是乎，在通过消息触发状态的转移之外，需要增加通过时间触发状态的转移支持。要实现此功能，需要在原来的状态以及状态转移中，增加时间属性的概念。如下图所示，通过 wait 算子来得到 waiting 状态，然后在 waiting 状态上设置一个十秒的时间属性以定义一个十秒的时间窗口。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111450980.png" alt="image-20210824111450980"></p><p>wait 算子对应 NFA 中的 ignore 状态，将在没有到达时间窗口结束时间时自旋，在 ComputationState 中记录 wait 的开始时间，在 NFA 的 doProcess 中，将到来的数据与waiting 状态处理，如果到了 waiting 的结束时间，则进行状态转移。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111558511.png" alt="image-20210824111558511"></p><p>上图中红色方框中为 waiting 状态设置了两条 ignore 边：</p><ol><li>waitingStatus.addIgnore(lastSink,waitingCondition)，waitingCondition 中的逻辑是获取当前的时间（支持事件时间），判断有没有超过设置的 waiting 阈值，如果超过就把状态向后转移。</li><li>waitingStatus.addIgnore(waitingCondition)，waitingCondition 中如果未达到设置的 waiting 阈值，就会自旋在当前的 waiting 状态不变。</li></ol><h3 id="2-规则动态注入"><a href="#2-规则动态注入" class="headerlink" title="2.规则动态注入"></a>2.规则动态注入</h3><p>线上运行的 CEP 中肯定经常遇到规则变更的情况，如果每次变更时都将任务重启、重新发布是非常不优雅的。尤其在营销或者风控这种对实时性要求比较高的场景，如果规则窗口过长（一两个星期），状态过大，就会导致重启时间延长，期间就会造成一些想要处理的异常行为不能及时发现。</p><p>那么要怎么样做到规则的动态更新和加载呢？</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111719312.png" alt="image-20210824111719312"></p><p>梳理一下整体架构，Flink CEP 是运行在 Flink Job 里的，而规则库是放在外部存储中的。首先，需要在运行的 Job 中能及时发现外部存储中规则的变化，即需要在 Job 中提供访问外部库的能力。其次，需要将规则库中变更的规则动态加载到 CEP 中，即把外部规则的描述解析成 Flink CEP 所能识别的 pattern 结构体。最后，把生成的 pattern 转化成 NFA，替换历史 NFA，这样对新到来的消息，就会使用新的规则进行匹配。</p><p>下图就是一个支持将外部规则动态注入、更新的接口。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111806820.png" alt="image-20210824111806820"></p><p>这个接口里面主要实现了四个方法：</p><ul><li><strong>initialize</strong>：初始化方法，进行外部库连接的初始化。</li><li><strong>inject</strong>：和外部数据库交互的主要方法，监听外部库变化，获取最新的规则并通过 Groovy 动态加载，返回 pattern。</li><li><strong>getPeriod</strong>：设置轮巡周期，在一些比较简单的实时性要求不高的场景，可以采用轮巡的方式，定期对外部数据库进行检测。</li><li><strong>getNfaKeySelector</strong>：和动态更新无关，用来支持一个流对应多个规则组。</li></ul><h3 id="3-历史匹配结果清理"><a href="#3-历史匹配结果清理" class="headerlink" title="3.历史匹配结果清理"></a>3.历史匹配结果清理</h3><p>新规则动态加载到 Flink CEP 的 Job 中，替换掉原来的 NFA 之后，还需要对历史匹配的结果集进行清理。在 AbstractKeyedCEPPatternOperator 中实现刷新 NFA，注意，历史状态是否需要清理和业务相关：</p><ol><li>修改的逻辑对规则中事件的匹配没有影响，保留历史结果集中的状态。</li><li>修改的逻辑影响到了之前匹配的部分，需要将之前匹配的结果集中的状态数据清除，防止错误的输出。</li></ol><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111921774.png" alt="image-20210824111921774"></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111953602.png" alt="image-20210824111953602"></p><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>使用 Flink CEP，熟知其原理是很重要的，特别是 NFA 的状态转移流程，然后再去看源码中的状态图的构建就会很清晰了。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语日常学习</title>
      <link href="/2021/08/23/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/23/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="英语日常学习"><a href="#英语日常学习" class="headerlink" title="英语日常学习"></a>英语日常学习</h1><h2 id="生活最常用英语口语"><a href="#生活最常用英语口语" class="headerlink" title="生活最常用英语口语"></a>生活最常用英语口语</h2><p><a href="https://www.youtube.com/watch?v=EXMTbXdQ63Q" target="_blank" rel="noopener">https://www.youtube.com/watch?v=EXMTbXdQ63Q</a></p><pre class=" language-properties"><code class="language-properties">1、今天的天气如何<span class="token attr-name">The</span> <span class="token attr-value">Weather is very good.</span>2、今天的天气很糟糕<span class="token attr-name">The</span> <span class="token attr-value">Weather is very bad.</span>3、今天的天气如何<span class="token attr-name">How</span> <span class="token attr-value">is the weather today?</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 英语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive高级进阶</title>
      <link href="/2021/08/23/Hive%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
      <url>/2021/08/23/Hive%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive高级优化"><a href="#Hive高级优化" class="headerlink" title="Hive高级优化"></a>Hive高级优化</h1><h2 id="第-1-章-Explain-查看执行计划（重点）"><a href="#第-1-章-Explain-查看执行计划（重点）" class="headerlink" title="第 1 章 Explain 查看执行计划（重点）"></a>第 1 章 Explain 查看执行计划（重点）</h2><h3 id="1-1-创建测试用表"><a href="#1-1-创建测试用表" class="headerlink" title="1.1  创建测试用表"></a>1.1  创建测试用表</h3><h4 id="1）建大表、小表和-JOIN-后表的语句"><a href="#1）建大表、小表和-JOIN-后表的语句" class="headerlink" title="1）建大表、小表和 JOIN 后表的语句"></a>1）建大表、小表和 JOIN 后表的语句</h4><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">// 创建大表</span><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 创建小表</span><span class="token keyword">create</span> <span class="token keyword">table</span> smalltable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 创建 JOIN 后表</span><span class="token keyword">create</span> <span class="token keyword">table</span> jointable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><h4 id="2）分别向大表和小表中导入数据"><a href="#2）分别向大表和小表中导入数据" class="headerlink" title="2）分别向大表和小表中导入数据"></a>2）分别向大表和小表中导入数据</h4><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">load</span> <span class="token attr-value">data local inpath '/opt/flink/hive/data/bigtable' into table bigtable;</span><span class="token attr-name">load</span> <span class="token attr-value">data local inpath '/opt/flink/hive/data/smalltable' into table smalltable;</span></code></pre><h3 id="1-2-基本语法"><a href="#1-2-基本语法" class="headerlink" title="1.2 基本语法"></a>1.2 基本语法</h3><pre class=" language-sql"><code class="language-sql"><span class="token keyword">EXPLAIN</span> <span class="token punctuation">[</span><span class="token keyword">EXTENDED</span> <span class="token operator">|</span> DEPENDENCY <span class="token operator">|</span> <span class="token keyword">AUTHORIZATION</span><span class="token punctuation">]</span> query<span class="token operator">-</span>sql</code></pre><h3 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h3><h4 id="1）查看下面这条语句的执行计划"><a href="#1）查看下面这条语句的执行计划" class="headerlink" title="1）查看下面这条语句的执行计划"></a>1）查看下面这条语句的执行计划</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> bigtable<span class="token punctuation">;</span><span class="token keyword">explain</span> <span class="token keyword">select</span> click_url<span class="token punctuation">,</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> ct <span class="token keyword">from</span> bigtable <span class="token keyword">group</span> <span class="token keyword">by</span> click_url<span class="token punctuation">;</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">hive</span> <span class="token attr-value">(wmy)> explain select * from bigtable;</span>OKExplain<span class="token attr-name">STAGE</span> <span class="token attr-value">DEPENDENCIES: 阶段的依赖关系</span><span class="token attr-name">  Stage-0</span> <span class="token attr-value">is a root stage</span><span class="token attr-name">STAGE</span> <span class="token attr-value">PLANS:</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-0 </span><span class="token attr-name">    Fetch</span> <span class="token attr-value">Operator 抓取操作</span><span class="token attr-name">      limit</span><span class="token punctuation">:</span> <span class="token attr-value">-1 没有做任何的限制</span><span class="token attr-name">      Processor</span> <span class="token attr-value">Tree: 操作树</span><span class="token attr-name">        TableScan</span> <span class="token attr-value">扫描的表</span><span class="token attr-name">          alias</span><span class="token punctuation">:</span> <span class="token attr-value">bigtable</span><span class="token attr-name">          Select</span> <span class="token attr-value">Operator 操作列表</span><span class="token attr-name">            expressions</span><span class="token punctuation">:</span> <span class="token attr-value">id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)</span><span class="token attr-name">            outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1, _col2, _col3, _col4, _col5, _col6 输出的列名</span>            ListSink<span class="token attr-name">Time</span> <span class="token attr-value">taken: 0.167 seconds, Fetched: 15 row(s)</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">hive</span> <span class="token attr-value">(wmy)> explain select click_url, count(*) ct from bigtable group by click_url;</span>OKExplain<span class="token attr-name">STAGE</span> <span class="token attr-value">DEPENDENCIES:</span><span class="token attr-name">  Stage-1</span> <span class="token attr-value">is a root stage 根阶段</span><span class="token attr-name">  Stage-0</span> <span class="token attr-value">depends on stages: Stage-1 依赖于第一阶段</span><span class="token attr-name">STAGE</span> <span class="token attr-value">PLANS:</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-1 MR任务</span>    Spark<span class="token attr-name">      Edges</span><span class="token punctuation">:</span><span class="token attr-name">        Reducer</span> <span class="token attr-value">2 &lt;- Map 1 (GROUP, 11)</span><span class="token attr-name">      DagName</span><span class="token punctuation">:</span> <span class="token attr-value">root_20210824125146_d0b9a768-686b-48c3-8e62-cfdb35023df2:2</span><span class="token attr-name">      Vertices</span><span class="token punctuation">:</span><span class="token attr-name">        Map</span> <span class="token attr-value">1 </span><span class="token attr-name">            Map</span> <span class="token attr-value">Operator Tree:</span>                TableScan<span class="token attr-name">                  alias</span><span class="token punctuation">:</span> <span class="token attr-value">bigtable</span><span class="token attr-name">                  Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                  Select</span> <span class="token attr-value">Operator</span><span class="token attr-name">                    expressions</span><span class="token punctuation">:</span> <span class="token attr-value">click_url (type: string)</span><span class="token attr-name">                    outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">click_url</span><span class="token attr-name">                    Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE 这个是内部直接给定的1</span><span class="token attr-name">                    Group</span> <span class="token attr-value">By Operator</span><span class="token attr-name">                      aggregations</span><span class="token punctuation">:</span> <span class="token attr-value">count()</span><span class="token attr-name">                      keys</span><span class="token punctuation">:</span> <span class="token attr-value">click_url (type: string)</span><span class="token attr-name">                      mode</span><span class="token punctuation">:</span> <span class="token attr-value">hash</span><span class="token attr-name">                      outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1</span><span class="token attr-name">                      Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                      Reduce</span> <span class="token attr-value">Output Operator</span><span class="token attr-name">                        key</span> <span class="token attr-value">expressions: _col0 (type: string)</span><span class="token attr-name">                        sort</span> <span class="token attr-value">order: + 正序排序</span><span class="token attr-name">                        Map-reduce</span> <span class="token attr-value">partition columns: _col0 (type: string)</span><span class="token attr-name">                        Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                        value</span> <span class="token attr-value">expressions: _col1 (type: bigint)</span><span class="token attr-name">            Execution</span> <span class="token attr-value">mode: vectorized</span><span class="token attr-name">        Reducer</span> <span class="token attr-value">2 </span><span class="token attr-name">            Execution</span> <span class="token attr-value">mode: vectorized</span><span class="token attr-name">            Reduce</span> <span class="token attr-value">Operator Tree:</span><span class="token attr-name">              Group</span> <span class="token attr-value">By Operator</span><span class="token attr-name">                aggregations</span><span class="token punctuation">:</span> <span class="token attr-value">count(VALUE._col0) 做一个累加的结果</span><span class="token attr-name">                keys</span><span class="token punctuation">:</span> <span class="token attr-value">KEY._col0 (type: string)</span><span class="token attr-name">                mode</span><span class="token punctuation">:</span> <span class="token attr-value">mergepartial</span><span class="token attr-name">                outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1</span><span class="token attr-name">                Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                File</span> <span class="token attr-value">Output Operator</span><span class="token attr-name">                  compressed</span><span class="token punctuation">:</span> <span class="token attr-value">false</span><span class="token attr-name">                  Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                  table</span><span class="token punctuation">:</span><span class="token attr-name">                      input</span> <span class="token attr-value">format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><span class="token attr-name">                      output</span> <span class="token attr-value">format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><span class="token attr-name">                      serde</span><span class="token punctuation">:</span> <span class="token attr-value">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-0</span><span class="token attr-name">    Fetch</span> <span class="token attr-value">Operator</span><span class="token attr-name">      limit</span><span class="token punctuation">:</span> <span class="token attr-value">-1 将全部的结果给打印</span><span class="token attr-name">      Processor</span> <span class="token attr-value">Tree:</span>        ListSink<span class="token attr-name">Time</span> <span class="token attr-value">taken: 0.223 seconds, Fetched: 56 row(s)</span></code></pre><p>从简单的SQL了解起，写hive sql ，如果不用hive sql来进行实现，写MR程序如何实现，执行计划就是将hive sql翻译成MR的程序</p><p>是如何翻译成MR任务的</p><h4 id="2）查看详细执行计划"><a href="#2）查看详细执行计划" class="headerlink" title="2）查看详细执行计划"></a>2）查看详细执行计划</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">extended</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> bigtable<span class="token punctuation">;</span><span class="token keyword">explain</span> <span class="token keyword">extended</span> <span class="token keyword">select</span> click_url<span class="token punctuation">,</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> ct <span class="token keyword">from</span> bigtable <span class="token keyword">group</span> <span class="token keyword">by</span> click_url<span class="token punctuation">;</span></code></pre><p>一般的我们是不会去这样使用，多出的部分的数据我们并不是很关心的</p><h2 id="第-2-章-Hive-建表优化"><a href="#第-2-章-Hive-建表优化" class="headerlink" title="第 2 章 Hive 建表优化"></a>第 2 章 Hive 建表优化</h2><h3 id="2-1-分区表"><a href="#2-1-分区表" class="headerlink" title="2.1 分区表"></a>2.1 分区表</h3><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所 有的数据文件。</p><p>Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。 </p><p>在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率 会提高很多，所以我们需要把常常用在 WHERE 语句中的字段指定为表的分区字段。</p><h4 id="2-1-1-分区表基本操作"><a href="#2-1-1-分区表基本操作" class="headerlink" title="2.1.1 分区表基本操作"></a>2.1.1 分区表基本操作</h4><h5 id="1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）"><a href="#1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）" class="headerlink" title="1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）"></a>1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）</h5><pre class=" language-properties"><code class="language-properties">dept_20200401.logdept_20200402.logdept_20200403.log</code></pre><h5 id="2）创建分区表语法"><a href="#2）创建分区表语法" class="headerlink" title="2）创建分区表语法"></a>2）创建分区表语法</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition             <span class="token punctuation">(</span>                          deptno <span class="token keyword">int</span>                        <span class="token punctuation">,</span> dname string                        <span class="token punctuation">,</span> loc string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          day string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p><h5 id="3）加载数据到分区表中"><a href="#3）加载数据到分区表中" class="headerlink" title="3）加载数据到分区表中"></a>3）加载数据到分区表中</h5><h6 id="（1）数据准备"><a href="#（1）数据准备" class="headerlink" title="（1）数据准备"></a>（1）数据准备</h6><p>dept_20200401.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">10</span> <span class="token attr-value">ACCOUNTING 1700</span><span class="token attr-name">20</span> <span class="token attr-value">RESEARCH 1800</span></code></pre><p>dept_20200402.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">30</span> <span class="token attr-value">SALES 1900</span><span class="token attr-name">40</span> <span class="token attr-value">OPERATIONS 1700</span></code></pre><p>dept_20200403.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">50</span> <span class="token attr-value">TEST 2000</span><span class="token attr-name">60</span> <span class="token attr-value">DEV 1900</span></code></pre><h6 id="（2）加载数据"><a href="#（2）加载数据" class="headerlink" title="（2）加载数据"></a>（2）加载数据</h6><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200401.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200402.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200402'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200403.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200403'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>注意：分区表加载数据时，必须指定分区</p><h5 id="4）查询分区表中数据"><a href="#4）查询分区表中数据" class="headerlink" title="4）查询分区表中数据"></a>4）查询分区表中数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 单分区查询</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 多分区查询</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token keyword">union</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200402'</span><span class="token keyword">union</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200403'</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824131047329.png" alt="分区中的数据"></p><h5 id="5）增加分区"><a href="#5）增加分区" class="headerlink" title="5）增加分区"></a>5）增加分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 增加单个分区</span><span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200404'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 增加多个分区</span> <span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200405'</span><span class="token punctuation">)</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200406'</span><span class="token punctuation">)</span> <span class="token punctuation">;</span></code></pre><h5 id="6）删除分区"><a href="#6）删除分区" class="headerlink" title="6）删除分区"></a>6）删除分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 删除单个分区</span><span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200406'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 删除多个分区</span> <span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200404'</span><span class="token punctuation">)</span>           <span class="token punctuation">,</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day                     <span class="token operator">=</span><span class="token string">'20200405'</span><span class="token punctuation">)</span> <span class="token punctuation">;</span></code></pre><h5 id="7）查看分区表有多少分区"><a href="#7）查看分区表有多少分区" class="headerlink" title="7）查看分区表有多少分区"></a>7）查看分区表有多少分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">show</span> partitions dept_partition<span class="token punctuation">;</span></code></pre><h5 id="8）查看分区表结构"><a href="#8）查看分区表结构" class="headerlink" title="8）查看分区表结构"></a>8）查看分区表结构</h5><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">desc</span> formatted dept_partition<span class="token punctuation">;</span></code></pre><p>思考: 如果一天的日志数据量也很大，如何再将数据拆分?</p><h4 id="2-1-2-二级分区"><a href="#2-1-2-二级分区" class="headerlink" title="2.1.2 二级分区"></a>2.1.2 二级分区</h4><h5 id="1）创建二级分区表"><a href="#1）创建二级分区表" class="headerlink" title="1）创建二级分区表"></a>1）创建二级分区表</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition2             <span class="token punctuation">(</span>                          deptno <span class="token keyword">int</span>                        <span class="token punctuation">,</span> dname string                        <span class="token punctuation">,</span> loc string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          day string                        <span class="token punctuation">,</span> hour string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><h5 id="2）正常的加载数据"><a href="#2）正常的加载数据" class="headerlink" title="2）正常的加载数据"></a>2）正常的加载数据</h5><h5 id="（1）加载数据到二级分区表中"><a href="#（1）加载数据到二级分区表中" class="headerlink" title="（1）加载数据到二级分区表中"></a>（1）加载数据到二级分区表中</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200401.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition2 <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">,</span> hour<span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h5 id="（2）查询分区数据"><a href="#（2）查询分区数据" class="headerlink" title="（2）查询分区数据"></a>（2）查询分区数据</h5><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span>        dept_partition2 <span class="token keyword">where</span>        day     <span class="token operator">=</span><span class="token string">'20200401'</span>        <span class="token operator">and</span> hour<span class="token operator">=</span><span class="token string">'12'</span> <span class="token punctuation">;</span></code></pre><h4 id="2-1-3-动态分区"><a href="#2-1-3-动态分区" class="headerlink" title="2.1.3 动态分区"></a>2.1.3 动态分区</h4><p>关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据 插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过， 使用 Hive 的动态分区，需要进行相应的配置。</p><h5 id="1）开启动态分区参数设置"><a href="#1）开启动态分区参数设置" class="headerlink" title="1）开启动态分区参数设置"></a>1）开启动态分区参数设置</h5><p>（1）开启动态分区功能（默认 true，开启）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为 静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>mode<span class="token operator">=</span>nonstrict<span class="token punctuation">;</span></code></pre><p>（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">;</span></code></pre><p>（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。</p><p>该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token punctuation">.</span>pernode<span class="token operator">=</span><span class="token number">100</span></code></pre><p>（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>created<span class="token punctuation">.</span>files<span class="token operator">=</span><span class="token number">100000</span></code></pre><p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>error<span class="token punctuation">.</span><span class="token keyword">on</span><span class="token punctuation">.</span>empty<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">false</span></code></pre><h5 id="2）案例实操"><a href="#2）案例实操" class="headerlink" title="2）案例实操"></a>2）案例实操</h5><p>需求：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分 区中。</p><p>（1）创建目标分区表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition_dy             <span class="token punctuation">(</span>                          id <span class="token keyword">int</span>                        <span class="token punctuation">,</span> name string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          loc string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>（2）设置动态分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>mode <span class="token operator">=</span> nonstrict<span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition_dy <span class="token keyword">partition</span>       <span class="token punctuation">(</span>loc       <span class="token punctuation">)</span><span class="token keyword">select</span>       deptno     <span class="token punctuation">,</span> dname     <span class="token punctuation">,</span> loc<span class="token keyword">from</span>       dept<span class="token punctuation">;</span></code></pre><p>（3）查看目标分区表的分区情况</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">show</span> partitions dept_partition<span class="token punctuation">;</span></code></pre><p>记住，使用分区表，分区表的里面的字段和查询出来的字段的类型是必须要一致的，否则的话会出现问题的</p><h3 id="2-2-分桶表"><a href="#2-2-分桶表" class="headerlink" title="2.2 分桶表"></a>2.2 分桶表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理 的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围 划分。 </p><p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。分区针对的是数据的存储 路径，分桶针对的是数据文件。</p><h4 id="2-2-1-创建分桶表"><a href="#2-2-1-创建分桶表" class="headerlink" title="2.2.1 创建分桶表"></a>2.2.1 创建分桶表</h4><p>（1）数据准备</p><pre class=" language-properties"><code class="language-properties">1001,ss11002,ss21003,ss31004,ss41005,ss51006,ss61007,ss71008,ss81009,ss91010,ss101011,ss111012,ss121013,ss131014,ss141015,ss151016,ss16</code></pre><p>（2）创建分桶表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">4</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p>（3）查看表结构</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">desc</span> formatted stu_buck<span class="token punctuation">;</span></code></pre><p>（4）导入数据到分桶表中，load 的方式</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/student.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">;</span></code></pre><p>（5）查看创建的分桶表中是否分成 4 个桶</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824141808132.png" alt="image-20210824141808132"></p><p>（6）查询分桶的数据</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> stu_buck<span class="token punctuation">;</span></code></pre><p>（7）分桶规则：</p><p>根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方 式决定该条记录存放在哪个桶当中</p><p>2）分桶表操作需要注意的事项</p><p>（1）reduce 的个数设置为-1，让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个 数设置为大于等于分桶表的桶数 </p><p>（2）从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题 </p><p>（3）不要使用本地模式</p><p>3）insert 方式将数据导入分桶表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student_insert<span class="token punctuation">;</span></code></pre><h4 id="2-2-2-抽样查询"><a href="#2-2-2-抽样查询" class="headerlink" title="2.2.2 抽样查询"></a>2.2.2 抽样查询</h4><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结 果。</p><p>Hive 可以通过对表进行抽样来满足这个需求。 </p><p>语法: TABLESAMPLE(BUCKET x OUT OF y) 查询表 stu_buck 中的数据。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> stu_buck tablesample<span class="token punctuation">(</span>bucket <span class="token number">1</span> <span class="token keyword">out</span> <span class="token keyword">of</span> <span class="token number">4</span> <span class="token keyword">on</span> id<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>注意：x 的值必须小于等于 y 的值，否则</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">FAILED</span><span class="token punctuation">:</span> <span class="token attr-value">SemanticException [Error 10061]: Numerator should not be bigger</span><span class="token attr-name">than</span> <span class="token attr-value">denominator in sample clause for table stu_buck</span></code></pre><h3 id="2-3-合适的文件格式"><a href="#2-3-合适的文件格式" class="headerlink" title="2.3 合适的文件格式"></a>2.3 合适的文件格式</h3><p>Hive 支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h4 id="2-3-1-列式存储和行式存储"><a href="#2-3-1-列式存储和行式存储" class="headerlink" title="2.3.1 列式存储和行式存储"></a>2.3.1 列式存储和行式存储</h4><p>1）行存储的特点</p><p>​    查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列 的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度 更快。</p><p>​    TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；</p><p>2）列存储的特点</p><p>​    因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的 数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算 法。</p><p>​    ORC 和 PARQUET 是基于列式存储的。</p><h4 id="2-3-2-TextFile格式"><a href="#2-3-2-TextFile格式" class="headerlink" title="2.3.2 TextFile格式"></a>2.3.2 TextFile格式</h4><p>​    默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合 Gzip、Bzip2 使用， 但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。</p><h4 id="2-3-3-Orc格式"><a href="#2-3-3-Orc格式" class="headerlink" title="2.3.3 Orc格式"></a>2.3.3 Orc格式</h4><p>​    Orc (Optimized Row Columnar)是 Hive 0.11 版里引入的新的存储格式。</p><h4 id="2-3-4-Parquet格式"><a href="#2-3-4-Parquet格式" class="headerlink" title="2.3.4 Parquet格式"></a>2.3.4 Parquet格式</h4><p>​    Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的 数据和元数据，因此 Parquet 格式文件是自解析的。</p><h3 id="2-4-合适的压缩格式"><a href="#2-4-合适的压缩格式" class="headerlink" title="2.4 合适的压缩格式"></a>2.4 合适的压缩格式</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825131543316.png" alt="压缩数据格式"></p><p>为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器，如下表所示。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825131735589.png" alt="编码和解码器"></p><p>压缩性能的比较</p><p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">Snappy压缩算法</a></p><p>在64位模式的酷睿i7处理器的单核上，Snappy压缩速度约为250mb /秒，解压缩速度约为500mb /秒。  </p><h2 id="第-3-章-HQL-语法优化"><a href="#第-3-章-HQL-语法优化" class="headerlink" title="第 3 章 HQL 语法优化"></a>第 3 章 HQL 语法优化</h2><h3 id="3-1-列裁剪与分区裁剪"><a href="#3-1-列裁剪与分区裁剪" class="headerlink" title="3.1 列裁剪与分区裁剪"></a>3.1 列裁剪与分区裁剪</h3><p>​    列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者 数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。 Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做 可以节省读取开销：中间表存储开销和数据整合开销。</p><h3 id="3-2-Group-By"><a href="#3-2-Group-By" class="headerlink" title="3.2 Group By"></a>3.2 Group By</h3><p>​    默认情况下，Map 阶段同一 Key 数据分发给一个 Reduce，当一个 key 数据过大时就倾 斜了。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825132110865.png" alt="数据倾斜"></p><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行 部分聚合，最后在 Reduce 端得出最终结果。</p><h4 id="3-2-1-开启-Map-端聚合参数设置"><a href="#3-2-1-开启-Map-端聚合参数设置" class="headerlink" title="3.2.1 开启 Map 端聚合参数设置"></a>3.2.1 开启 Map 端聚合参数设置</h4><p>（1）是否在 Map 端进行聚合，默认为 True</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.map.aggr = true;</span></code></pre><p>（2）在 Map 端进行聚合操作的条目数目</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.groupby.mapaggr.checkinterval = 100000;</span></code></pre><p>（3）有数据倾斜的时候进行负载均衡（默认是 false）</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.groupby.skewindata = true;</span></code></pre><p>​    当选项设定为 true，生成的查询计划会有两个 MR Job。</p><p>​    第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合 操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；</p><p>​    第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然 能解决数据倾斜，但是不能让运行速度的更快）。</p><h3 id="3-3-Vectorization"><a href="#3-3-Vectorization" class="headerlink" title="3.3 Vectorization"></a>3.3 Vectorization</h3><p>​    vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization 技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825133949490.png" alt="矢量查询"></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><h3 id="3-4-多重模式"><a href="#3-4-多重模式" class="headerlink" title="3.4 多重模式"></a>3.4 多重模式</h3><p>​    如果你碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不 同的逻辑。有可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">from</span> student<span class="token keyword">insert</span> <span class="token keyword">int</span> t_ptn <span class="token keyword">partition</span><span class="token punctuation">(</span>city<span class="token operator">=</span>A<span class="token punctuation">)</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span>name<span class="token punctuation">,</span>sex<span class="token punctuation">,</span> age <span class="token keyword">where</span> city<span class="token operator">=</span> A<span class="token keyword">insert</span> <span class="token keyword">int</span> t_ptn <span class="token keyword">partition</span><span class="token punctuation">(</span>city<span class="token operator">=</span>B<span class="token punctuation">)</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span>name<span class="token punctuation">,</span>sex<span class="token punctuation">,</span> age <span class="token keyword">where</span> city<span class="token operator">=</span> B</code></pre><p>​    如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高， 多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后， 要多次利用。</p><h3 id="3-5-in-exists-语句"><a href="#3-5-in-exists-语句" class="headerlink" title="3.5 in/exists 语句"></a>3.5 in/exists 语句</h3><p>​    在 Hive 的早期版本中，in/exists 语法是不被支持的，但是从 hive-0.8x 以后就开始支持 这个语法。但是不推荐使用这个语法。虽然经过测验，Hive-2.3.6 也支持 in/exists 操作，但 还是推荐使用 Hive 的一个高效替代方案：left semi join</p><p>​    比如说：– in / exists 实现    </p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">in</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token number">b</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>可以使用 join 来改写：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>应该转换成： – left semi join 实现</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">left</span> semi <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-6-CBO-优化"><a href="#3-6-CBO-优化" class="headerlink" title="3.6 CBO 优化"></a>3.6 CBO 优化</h3><p>join 的时候表的顺序的关系：前面的表都会被加载到内存中。后面的表进行磁盘扫描</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">c</span><span class="token punctuation">.</span><span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">join</span> <span class="token number">c</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">c</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>​    Hive 自 0.14.0 开始，加入了一项 “Cost based Optimizer” 来对 HQL 执行计划进行优化， 这个功能通过 “hive.cbo.enable” 来开启。    在 Hive 1.1.0 之后，这个 feature 是默认开启的， 它可以 自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。 CBO，成本优化器，代价最小的执行计划就是最好的执行计划。传统的数据库，成本优 化器做出最优化的执行计划是依据统计信息来计算的。 Hive 的成本优化器也一样，Hive 在提供最终执行前，优化每个查询的执行逻辑和物理 执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生 潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。</p><p>​    要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>cbo<span class="token punctuation">.</span><span class="token keyword">enable</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">compute</span><span class="token punctuation">.</span>query<span class="token punctuation">.</span><span class="token keyword">using</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">column</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><h3 id="3-7-谓词下推"><a href="#3-7-谓词下推" class="headerlink" title="3.7 谓词下推"></a>3.7 谓词下推</h3><p>​    将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻 辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。 案例实操：</p><p>1）打开谓词下推优化属性</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>ppd <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 谓词下推，默认是 true</span></code></pre><p>2）查看先关联两张表，再用 where 条件过滤的执行计划</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> o<span class="token punctuation">.</span>id <span class="token keyword">from</span> bigtable <span class="token number">b</span> <span class="token keyword">join</span> bigtable o <span class="token keyword">on</span> o<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> o<span class="token punctuation">.</span>id <span class="token operator">&lt;=</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p>3）查看子查询后，再关联表的执行计划</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">from</span> bigtable <span class="token number">b</span> <span class="token keyword">join</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> bigtable <span class="token keyword">where</span> id <span class="token operator">&lt;=</span> <span class="token number">10</span><span class="token punctuation">)</span> o <span class="token keyword">on</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">=</span> o<span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-8-MapJoin"><a href="#3-8-MapJoin" class="headerlink" title="3.8 MapJoin"></a>3.8 MapJoin</h3><p>​    MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map 端进行 Join，避免 Reducer 处理。</p><p>1）开启 MapJoin 参数设置</p><p>（1）设置自动选择 MapJoin</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 默认为 true</span></code></pre><p>（2）大表小表的阈值设置（默认 25M 以下认为是小表）：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>smalltable<span class="token punctuation">.</span>filesize<span class="token operator">=</span><span class="token number">25000000</span><span class="token punctuation">;</span></code></pre><p>2）MapJoin 工作机制</p><p>​    MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进 程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</p><p>3）案例实操</p><p>（1）开启 MapJoin 功能</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span> <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 默认为 true</span></code></pre><p>（2）执行小表 JOIN 大表语句</p><p>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin 失效</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> smalltable s<span class="token keyword">left</span> <span class="token keyword">join</span> bigtable <span class="token number">b</span><span class="token keyword">on</span> s<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>（3）执行大表 JOIN 小表语句</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">Explain</span> <span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable <span class="token number">b</span><span class="token keyword">left</span> <span class="token keyword">join</span> smalltable s<span class="token keyword">on</span> s<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-9-大表、大表-SMB-Join（重点）"><a href="#3-9-大表、大表-SMB-Join（重点）" class="headerlink" title="3.9 大表、大表 SMB Join（重点）"></a>3.9 大表、大表 SMB Join（重点）</h3><p>SMB Join ：Sort Merge Bucket Join</p><p>1）创建第二张大表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable2<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable2<span class="token punctuation">;</span></code></pre><p>2）测试大表直接 JOIN</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable <span class="token number">a</span><span class="token keyword">join</span> bigtable2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>3）创建分通表 1</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable_buck1<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>sorted <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">6</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable_buck1<span class="token punctuation">;</span></code></pre><p>4）创建分通表 2，分桶数和第一张表的分桶数为倍数关系</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable_buck2<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>sorted <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">6</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable_buck2<span class="token punctuation">;</span></code></pre><p>5）设置参数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin<span class="token punctuation">.</span>sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>BucketizedHiveInputFormat<span class="token punctuation">;</span></code></pre><p>6）测试 Time taken: 34.685 seconds</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable_buck1 s<span class="token keyword">join</span> bigtable_buck2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">=</span> s<span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>3.10 笛卡尔积</p><p>​    Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用 1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode=strict，nonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p><h2 id="第-4-章-数据倾斜（重点）"><a href="#第-4-章-数据倾斜（重点）" class="headerlink" title="第 4 章 数据倾斜（重点）"></a>第 4 章 数据倾斜（重点）</h2><p>​    绝大部分任务都很快完成，只有一个或者少数几个任务执行的很慢甚至最终执行失败， 这样的现象为数据倾斜现象。 </p><p>​    一定要和数据过量导致的现象区分开，数据过量的表现为所有任务都执行的很慢，这个 时候只有提高执行资源才可以优化 HQL 的执行效率。 </p><p>​    综合来看，导致数据倾斜的原因在于按照 Key 分组以后，少量的任务负责绝大部分数据 的计算，也就是说产生数据倾斜的 HQL 中一定存在分组操作，那么从 HQL 的角度，我们可 以将数据倾斜分为单表携带了 GroupBy 字段的查询和两表（或者多表）Join 的查询。</p><h3 id="4-1-单表数据倾斜优化"><a href="#4-1-单表数据倾斜优化" class="headerlink" title="4.1 单表数据倾斜优化"></a>4.1 单表数据倾斜优化</h3><h4 id="4-1-1-使用参数"><a href="#4-1-1-使用参数" class="headerlink" title="4.1.1 使用参数"></a>4.1.1 使用参数</h4><p>​    当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据 倾斜问题。</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 是否在 Map 端进行聚合，默认为 True</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 在 Map 端进行聚合操作的条目数目</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>mapaggr<span class="token punctuation">.</span>checkinterval <span class="token operator">=</span> <span class="token number">100000</span><span class="token punctuation">;</span></code></pre><p>有数据倾斜的时候进行负载均衡（默认是 false）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>skewindata <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>当选项设定为 true，生成的查询计划会有两个 MR Job。</p><h4 id="4-1-2-增加-Reduce-数量（多个-Key-同时导致数据倾斜）"><a href="#4-1-2-增加-Reduce-数量（多个-Key-同时导致数据倾斜）" class="headerlink" title="4.1.2 增加 Reduce 数量（多个 Key 同时导致数据倾斜）"></a>4.1.2 增加 Reduce 数量（多个 Key 同时导致数据倾斜）</h4><p>1）调整 reduce 个数方法一</p><p>（1）每个 Reduce 处理的数据量默认是 256MB</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer <span class="token operator">=</span> <span class="token number">256000000</span></code></pre><p>（2）每个任务最大的 reduce 数，默认为 1009</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>max <span class="token operator">=</span> <span class="token number">1009</span></code></pre><p>（3）计算 reducer 数的公式</p><pre class=" language-sql"><code class="language-sql">N<span class="token operator">=</span><span class="token function">min</span><span class="token punctuation">(</span>参数 <span class="token number">2</span>，总输入数据量<span class="token operator">/</span>参数 <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>参数 <span class="token number">2</span> 指的是上面的 <span class="token number">1009</span>，参数 <span class="token number">1</span> 值得是 256M<span class="token punctuation">)</span></code></pre><p>2）调整 reduce 个数方法二</p><p>在 hadoop 的 mapred-default.xml 文件中修改</p><p>设置每个 job 的 Reduce 个数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span></code></pre><h3 id="4-2-Join-数据倾斜优化"><a href="#4-2-Join-数据倾斜优化" class="headerlink" title="4.2 Join 数据倾斜优化"></a>4.2 Join 数据倾斜优化</h3><h4 id="4-2-1-使用参数"><a href="#4-2-1-使用参数" class="headerlink" title="4.2.1 使用参数"></a>4.2.1 使用参数</h4><p>​    在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># join 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span><span class="token keyword">key</span><span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true"># 如果是 join 过程出现倾斜应该设置为 true</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>skewjoin<span class="token operator">=</span><span class="token boolean">false</span><span class="token punctuation">;</span></code></pre><p>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的 倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过 hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>map<span class="token punctuation">.</span>tasks<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">;</span></code></pre><h4 id="4-2-2-MapJoin"><a href="#4-2-2-MapJoin" class="headerlink" title="4.2.2 MapJoin"></a>4.2.2 MapJoin</h4><p>详情见 3.8 节</p><h2 id="第-5-章-Hive-Job-优化"><a href="#第-5-章-Hive-Job-优化" class="headerlink" title="第 5 章 Hive Job 优化"></a>第 5 章 Hive Job 优化</h2><h3 id="5-1-Hive-Map-优化"><a href="#5-1-Hive-Map-优化" class="headerlink" title="5.1 Hive Map 优化"></a>5.1 Hive Map 优化</h3><h4 id="5-1-1-复杂文件增加-Map-数"><a href="#5-1-1-复杂文件增加-Map-数" class="headerlink" title="5.1.1 复杂文件增加 Map 数"></a>5.1.1 复杂文件增加 Map 数</h4><p>​    当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据<br>$$<br>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M<br>$$<br>调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p><p>1）执行查询</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span></code></pre><p>2）设置最大切片值为 100 个字节</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>input<span class="token punctuation">.</span>fileinputformat<span class="token punctuation">.</span>split<span class="token punctuation">.</span>maxsize<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">;</span> <span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span></code></pre><h4 id="5-1-2-小文件进行合并"><a href="#5-1-2-小文件进行合并" class="headerlink" title="5.1.2 小文件进行合并"></a>5.1.2 小文件进行合并</h4><p>1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span></code></pre><p>2）在 Map-Reduce 的任务结束时合并小文件的设置：</p><p>在 map-only 任务结束时合并小文件，默认 true</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>在 map-reduce 任务结束时合并小文件，默认 false</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapredfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>合并文件的大小，默认 256M</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>task <span class="token operator">=</span> <span class="token number">268435456</span><span class="token punctuation">;</span></code></pre><p>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>smallfiles<span class="token punctuation">.</span>avgsize <span class="token operator">=</span> <span class="token number">16777216</span><span class="token punctuation">;</span></code></pre><h4 id="5-1-3-Map-端聚合"><a href="#5-1-3-Map-端聚合" class="headerlink" title="5.1.3 Map 端聚合"></a>5.1.3 Map 端聚合</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 相当于 map 端执行 combiner</span></code></pre><h4 id="5-1-4-推测执行"><a href="#5-1-4-推测执行" class="headerlink" title="5.1.4 推测执行"></a>5.1.4 推测执行</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapred<span class="token punctuation">.</span>map<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">-- #默认是 true</span></code></pre><h3 id="5-2-Hive-Reduce-优化"><a href="#5-2-Hive-Reduce-优化" class="headerlink" title="5.2 Hive Reduce 优化"></a>5.2 Hive Reduce 优化</h3><h4 id="5-2-1-合理设置-Reduce-数"><a href="#5-2-1-合理设置-Reduce-数" class="headerlink" title="5.2.1 合理设置 Reduce 数"></a>5.2.1 合理设置 Reduce 数</h4><p>1）调整 reduce 个数方法一</p><p>（1）每个 Reduce 处理的数据量默认是 256MB</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer <span class="token operator">=</span> <span class="token number">256000000</span></code></pre><p>（2）每个任务最大的 reduce 数，默认为 1009</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>max <span class="token operator">=</span> <span class="token number">1009</span></code></pre><p>（3）计算 reducer 数的公式<br>$$<br>N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)<br>$$<br>2）调整 reduce 个数方法二</p><p>​    在 hadoop 的 mapred-default.xml 文件中修改 设置每个 job 的 Reduce 个数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span></code></pre><p>3）reduce 个数并不是越多越好</p><p>（1）过多的启动和初始化 reduce 也会消耗时间和资源； </p><p>（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那 么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； 在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数； 使单个 reduce 任务处理数据量大小要合适；</p><h4 id="5-2-2-推测执行"><a href="#5-2-2-推测执行" class="headerlink" title="5.2.2  推测执行"></a>5.2.2  推测执行</h4><pre class=" language-sql"><code class="language-sql">mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution （hadoop 里面的）hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution（hive 里面相同的参数，效果和hadoop 里面的一样两个随便哪个都行）</code></pre><h3 id="5-3-Hive-任务整体优化"><a href="#5-3-Hive-任务整体优化" class="headerlink" title="5.3 Hive 任务整体优化"></a>5.3 Hive 任务整体优化</h3><h4 id="5-3-1-Fetch-抓取"><a href="#5-3-1-Fetch-抓取" class="headerlink" title="5.3.1 Fetch 抓取"></a>5.3.1 Fetch 抓取</h4><p>​    Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。</p><p>​    例如：SELECT * FROM emp;在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出 查询结果到控制台。 </p><p>​    在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive 默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走</p><p>mapreduce。</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.fetch.task.conversion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>more<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>     期望一个[没有，最少，更多]。      一些选择查询可以转换为单个FETCH任务最小化      延迟。      当前查询应该是单源的，没有任何子查询      不应该有任何聚合或区别(这会导致RS)，      侧面视图和连接。      0.  无:关闭hive.fetch.task.conversion      1.  最小:SELECT STAR，对分区列进行过滤，仅限      2.  more: SELECT, FILTER, LIMIT only(支持TABLESAMPLE和  虚拟列)   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>1）案例实操：</p><p>（1）把 hive.fetch.task.conversion 设置成 none，然后执行查询语句，都会执行 mapreduce 程序。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>none<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp <span class="token keyword">limit</span> <span class="token number">3</span><span class="token punctuation">;</span></code></pre><p>（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不 会执行 mapreduce 程序。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>more<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp <span class="token keyword">limit</span> <span class="token number">3</span><span class="token punctuation">;</span></code></pre><h4 id="5-3-2-本地模式"><a href="#5-3-2-本地模式" class="headerlink" title="5.3.2 本地模式"></a>5.3.2 本地模式</h4><p>​    大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。</p><p>​    不过， 有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能 会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机 器上处理所有的任务。</p><p>​    对于小数据集，执行时间可以明显被缩短。 用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动 启动这个优化。</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=true; //开启本地 mr//设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;//设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4set hive.exec.mode.local.auto.input.files.max=10;</code></pre><p>1）案例实操：</p><p>（1）开启本地模式，并执行查询语句</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=true;select * from emp cluster by deptno;</code></pre><p>（2）关闭本地模式，并执行查询语句</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=false;select * from emp cluster by deptno;</code></pre><h4 id="5-3-3-并行执行"><a href="#5-3-3-并行执行" class="headerlink" title="5.3.3 并行执行"></a>5.3.3 并行执行</h4><p>​    Hive 会将一个查询转化成一个或者多个阶段。</p><p>​    这样的阶段可以是 MapReduce 阶段、抽 样阶段、合并阶段、limit 阶段。</p><p>​    或者 Hive 执行过程中可能需要的其他阶段。默认情况下， Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能 并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行 时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。</p><p>通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中， 需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//打开任务并行执行，默认为 false</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>thread<span class="token punctuation">.</span>number<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//同一个 sql 允许最大并行度，默认为 8</span></code></pre><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来</p><p>(建议在数据量大,sql 很长的时候使用,数据量小,sql 比较的小开启有可能还不如之前快)。</p><h4 id="5-3-4-严格模式"><a href="#5-3-4-严格模式" class="headerlink" title="5.3.4 严格模式"></a>5.3.4 严格模式</h4><p>Hive 可以通过设置防止一些危险操作：</p><p>1）分区表不使用分区过滤</p><p>​    将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含 有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分 区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有 进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</p><p>2）使用 order by 没有 limit 过滤</p><p>​    将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要 求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间(开启了 limit 可以在数据进入到 reduce 之前就减少一部分数据)。</p><p>3）笛卡尔积</p><p>​    将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数 据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语 句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸 的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情 况。</p><h4 id="5-3-5-JVM-重用"><a href="#5-3-5-JVM-重用" class="headerlink" title="5.3.5 JVM 重用"></a>5.3.5 JVM 重用</h4><p>​    小文件过多的时候使用。</p><h2 id="第-6-章-Hive-On-Spark"><a href="#第-6-章-Hive-On-Spark" class="headerlink" title="第 6 章 Hive On Spark"></a>第 6 章 Hive On Spark</h2><h3 id="6-1-Executor-参数"><a href="#6-1-Executor-参数" class="headerlink" title="6.1 Executor 参数"></a>6.1 Executor 参数</h3><p>​    以单台服务器 128G 内存，32 线程为例。</p><h4 id="6-1-1-spark-executor-cores"><a href="#6-1-1-spark-executor-cores" class="headerlink" title="6.1.1 spark.executor.cores"></a>6.1.1 spark.executor.cores</h4><p>​    该参数表示每个 Executor 可利用的 CPU 核心数。其值不宜设定过大，因为 Hive 的底层 以 HDFS 存储，而 HDFS 有时对高并发写入处理不太好，容易造成 竞争条件。根据经验 实践，设定在 3~6 之间比较合理。</p><p>​    假设我们使用的服务器单节点有 32 个 CPU 核心可供使用。考虑到系统基础服务和 HDFS 等组件的余量，一般会将 YARN NodeManager 的 yarn.nodemanager.resource.cpu-vcores 参数 设为 28，也就是 YARN 能够利用其中的 28 核，此时将 spark.executor.cores 设为 4 最合适， 最多可以正好分配给 7 个 Executor 而不造成浪费。又假设 yarn.nodemanager.resource.cpuvcores 为 26，那么将 spark.executor.cores 设为 5 最合适，只会剩余 1 个核。</p><p>​    由于一个 Executor 需要一个 YARN Container 来运行，所以还需保证 spark.executor.cores 的值不能大于单个 Container 能申请到的最大核心数，即 yarn.scheduler.maximum-allocationvcores 的值。</p><h4 id="6-1-2-spark-executor-memory-spark-yarn-executor-memoryOverhead"><a href="#6-1-2-spark-executor-memory-spark-yarn-executor-memoryOverhead" class="headerlink" title="6.1.2 spark.executor.memory/spark.yarn.executor.memoryOverhead"></a>6.1.2 spark.executor.memory/spark.yarn.executor.memoryOverhead</h4><p>​    这两个参数分别表示每个 Executor 可利用的堆内内存量和堆外内存量。堆内内存越大Executor 就能缓存更多的数据，在做诸如 map join 之类的操作时就会更快，但同时也会使得 GC 变得更麻烦。spark.yarn.executor.memoryOverhead 的默认值是 executorMemory * 0.10， 最小值为 384M(每个 Executor)</p><p>​    Hive 官方提供了一个计算 Executor 总内存量的经验公式，如下：</p><p>​    yarn.nodemanager.resource.memory-mb*(spark.executor.cores/ yarn.nodemanager.resource.cpu-vcores)</p><p>​    其实就是按核心数的比例分配。在计算出来的总内存量中，80%~85%划分给堆内内存， 剩余的划分给堆外内存。</p><p>​    假设集群中单节点有 128G 物理内存，yarn.nodemanager.resource.memory-mb（即单个 NodeManager 能够利用的主机内存量）设为 100G，那么每个 Executor 大概就是 100*(4/28)= 约 14G。</p><p>​    再 按 8:2 比 例 划 分 的 话 ， 最 终 spark.executor.memory 设 为 约 11.2G ， spark.yarn.executor.memoryOverhead 设为约 2.8G。</p><p>​    通过这些配置，每个主机一次可以运行多达 7 个 executor。每个 executor 最多可以运行 4 个 task(每个核一个)。因此，每个 task 平均有 3.5 GB(14 / 4)内存。在 executor 中运行的所 有 task 共享相同的堆空间。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memory<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">.</span>2g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">.</span>8g<span class="token punctuation">;</span></code></pre><p>同理，这两个内存参数相加的总量也不能超过单个 Container 最多能申请到的内存量， 即 yarn.scheduler.maximum-allocation-mb 配置的值。</p><h4 id="6-1-3-spark-executor-instances"><a href="#6-1-3-spark-executor-instances" class="headerlink" title="6.1.3 spark.executor.instances"></a>6.1.3 spark.executor.instances</h4><p>​    该参数表示执行查询时一共启动多少个 Executor 实例，这取决于每个节点的资源分配 情况以及集群的节点数。若我们一共有 10 台 32C/128G 的节点，并按照上述配置（即每个节 点承载 7 个 Executor），那么理论上讲我们可以将 spark.executor.instances 设为 70，以使集群 资源最大化利用。但是实际上一般都会适当设小一些（推荐是理论值的一半左右，比如 40）， 因为 Driver 也要占用资源，并且一个 YARN 集群往往还要承载除了 Hive on Spark 之外的其他 业务。</p><h4 id="6-1-4-spark-dynamicAllocation-enabled"><a href="#6-1-4-spark-dynamicAllocation-enabled" class="headerlink" title="6.1.4 spark.dynamicAllocation.enabled"></a>6.1.4 spark.dynamicAllocation.enabled</h4><p>​    上面所说的固定分配 Executor 数量的方式可能不太灵活，尤其是在 Hive 集群面向很多,用户提供分析服务的情况下。所以更推荐将 spark.dynamicAllocation.enabled 参数设为 true， 以启用 Executor 动态分配。</p><h4 id="6-1-5-参数配置样例参考"><a href="#6-1-5-参数配置样例参考" class="headerlink" title="6.1.5 参数配置样例参考"></a>6.1.5 参数配置样例参考</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>execution<span class="token punctuation">.</span><span class="token keyword">engine</span><span class="token operator">=</span>spark<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memory<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">.</span>2g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">.</span>8g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>cores<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>instances<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>dynamicAllocation<span class="token punctuation">.</span>enabled<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>serializer<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>serializer<span class="token punctuation">.</span>KryoSerializer<span class="token punctuation">;</span></code></pre><h3 id="6-2-Driver-参数"><a href="#6-2-Driver-参数" class="headerlink" title="6.2 Driver 参数"></a>6.2 Driver 参数</h3><h4 id="6-2-1-spark-driver-cores"><a href="#6-2-1-spark-driver-cores" class="headerlink" title="6.2.1 spark.driver.cores"></a>6.2.1 spark.driver.cores</h4><p>​    该参数表示每个 Driver 可利用的 CPU 核心数。绝大多数情况下设为 1 都够用。</p><h4 id="6-2-2-spark-driver-memory-spark-driver-memoryOverhead"><a href="#6-2-2-spark-driver-memory-spark-driver-memoryOverhead" class="headerlink" title="6.2.2 spark.driver.memory/spark.driver.memoryOverhead"></a>6.2.2 spark.driver.memory/spark.driver.memoryOverhead</h4><p>​    这两个参数分别表示每个 Driver 可利用的堆内内存量和堆外内存量。根据资源富余程 度和作业的大小，一般是将总量控制在 512MB~4GB 之间，并且沿用 Executor 内存的“二八分 配方式”。例如，spark.driver.memory 可以设为约 819MB，spark.driver.memoryOverhead 设为 约 205MB，加起来正好 1G。</p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink提交流程以及Flinkdebug流程分享</title>
      <link href="/2021/08/23/Flink%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8AFlinkdebug%E6%B5%81%E7%A8%8B%E5%88%86%E4%BA%AB/"/>
      <url>/2021/08/23/Flink%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8AFlinkdebug%E6%B5%81%E7%A8%8B%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink提交流程以及Flinkdebug流程分享"><a href="#Flink提交流程以及Flinkdebug流程分享" class="headerlink" title="Flink提交流程以及Flinkdebug流程分享"></a>Flink提交流程以及Flinkdebug流程分享</h1><h2 id="Flink三种提交流程"><a href="#Flink三种提交流程" class="headerlink" title="Flink三种提交流程"></a>Flink三种提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130021872.png" alt="Flink提交流程"></p><h2 id="为什么引入Application模式"><a href="#为什么引入Application模式" class="headerlink" title="为什么引入Application模式"></a>为什么引入Application模式</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130250657.png" alt="三种模式"></p><p>目前我们公司使用的是PerJob模式，必须得使用接口机来进行上传，会给服务器造成很大的压力，大量的网络IO开销，我们任务是非常多的，Flink1.11之后，引入了Application模式，全部都上传到master上，可以把jar指定在hdfs上，减少上传和部署作业的时间</p><h2 id="Flink提交流程概览"><a href="#Flink提交流程概览" class="headerlink" title="Flink提交流程概览"></a>Flink提交流程概览</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130937087.png" alt="Flink提交流程"></p><h2 id="Flink-Stream-sql客户端提交流程"><a href="#Flink-Stream-sql客户端提交流程" class="headerlink" title="Flink Stream sql客户端提交流程"></a>Flink Stream sql客户端提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823131345554.png" alt="Flink SQL提交流程"></p><p>Flink Stream SQL的源码不是很多，有时间的话，可以去看看</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823132035655.png" alt="Flink SQL主类"></p><p>Flink SQL中的executor是非常重要的，Flink client提交SQL的一个流程</p><h2 id="Flink-Session-Runtime"><a href="#Flink-Session-Runtime" class="headerlink" title="Flink Session Runtime"></a>Flink Session Runtime</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823132427416.png" alt="Flink Session Runtime"></p><p>这些都是可以好好的把Flink 1.12的源码给好好阅读一下，并做好自己的笔记是非常的重要的</p><p>提交和调度，7大点调度任务，作业调度，对象不明确</p><p>Flink 提交源码可以总结成一个流程图，可以根据自己写的类图，启动集群，提交任务，任务调度都可以进行一个完整的程序</p><h2 id="Flink-Debug-流程"><a href="#Flink-Debug-流程" class="headerlink" title="Flink Debug 流程"></a>Flink Debug 流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823133735059.png" alt="Flink Debug流程"></p><p>多去尝试，多去打断点就好了，可以根据yarn session上面的ip 来进行查看</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink电商实时数仓DWM层业务实现</title>
      <link href="/2021/08/23/Flink%E7%94%B5%E5%95%86%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93DWM%E5%B1%82%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0/"/>
      <url>/2021/08/23/Flink%E7%94%B5%E5%95%86%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93DWM%E5%B1%82%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink电商实时数仓DWM层业务实现"><a href="#Flink电商实时数仓DWM层业务实现" class="headerlink" title="Flink电商实时数仓DWM层业务实现"></a>Flink电商实时数仓DWM层业务实现</h1><p>DWS是每天的聚合结果，按天做分区，DWT是一个累计的结果聚合</p><p>DWD是不考虑需求，和需求是没有关系的，DWM,DWS是需要考虑需求，日志数据的分流，业务数据的分流</p><p>实时计算指标计算：</p><p>​    1、实时的运维和资源的要求都是很高的</p><p>​    2、在公司中可能是按照主题来进行分析和计算</p><p>​    3、在离线的数仓当中是没有关键词分析的，这里的话，加入了搜索关键字主题分析</p><h2 id="第1章-DWS层与DWM层的设计"><a href="#第1章-DWS层与DWM层的设计" class="headerlink" title="第1章 DWS层与DWM层的设计"></a>第1章 DWS层与DWM层的设计</h2><table><thead><tr><th align="center"><strong>统计主题</strong></th><th align="center"><strong>需求指标</strong></th><th align="center"><strong>输出方式</strong></th><th align="center"><strong>计算来源</strong></th><th align="center"><strong>来源层级</strong></th></tr></thead><tbody><tr><td align="center">访客</td><td align="center">pv</td><td align="center">可视化大屏</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">uv</td><td align="center">可视化大屏</td><td align="center">需要用page_log过滤去重</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">跳出率</td><td align="center">可视化大屏</td><td align="center">需要通过page_log行为判断</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">进入页面数</td><td align="center">可视化大屏</td><td align="center">需要识别开始访问标识</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">连续访问时长</td><td align="center">可视化大屏</td><td align="center">page_log直接可求</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">商品</td><td align="center">点击</td><td align="center">多维分析</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">收藏</td><td align="center">多维分析</td><td align="center">收藏表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">加入购物车</td><td align="center">多维分析</td><td align="center">购物车表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">下单</td><td align="center">可视化大屏</td><td align="center">订单宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">支付</td><td align="center">多维分析</td><td align="center">支付宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">退款</td><td align="center">多维分析</td><td align="center">退款表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">评论</td><td align="center">多维分析</td><td align="center">评论表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">地区</td><td align="center">pv</td><td align="center">多维分析</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">uv</td><td align="center">多维分析</td><td align="center">需要用page_log过滤去重</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">下单</td><td align="center">可视化大屏</td><td align="center">订单宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">关键词</td><td align="center">搜索关键词</td><td align="center">可视化大屏</td><td align="center">页面访问日志 直接可求</td><td align="center">dwd</td></tr><tr><td align="center">点击商品关键词</td><td align="center">可视化大屏</td><td align="center">商品主题下单再次聚合</td><td align="center">dws</td><td align="center"></td></tr><tr><td align="center">下单商品关键词</td><td align="center">可视化大屏</td><td align="center">商品主题下单再次聚合</td><td align="center">dws</td><td align="center"></td></tr></tbody></table><ul><li><input disabled="" type="checkbox"> 访问UV计算<ul><li><input disabled="" type="checkbox"> 按用户去重，在天的基础上，安装各种维度信息来进行统计UV，DWM作为一个中间表，复用性，中心思想</li></ul></li><li><input disabled="" type="checkbox"> 跳出明细计算</li><li><input disabled="" type="checkbox"> 订单宽表</li><li><input disabled="" type="checkbox"> 支付宽表</li></ul><h2 id="第2章-DWM层-访客UV的计算"><a href="#第2章-DWM层-访客UV的计算" class="headerlink" title="第2章 DWM层-访客UV的计算"></a>第2章 DWM层-访客UV的计算</h2><h3 id="2-1-需求描述"><a href="#2-1-需求描述" class="headerlink" title="2.1 需求描述"></a>2.1 需求描述</h3><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">UV</span> <span class="token attr-value">全称是Unique Visitor，即访客数，对于实时计算中，也可以称为DAU(Daily Active User)，即每日的活跃用户数，因为实时计算中心的UV通常是指当日的访客数</span>那么如何从用户行为日志识别当日的访客：    1、识别出该用户打开的第一个页面，表示这个访客开始进入我们的应用    2、由于访客可以在一天中多次进入应用，我们需要要在一天的范围内进行去重</code></pre><h3 id="2-2-思考点"><a href="#2-2-思考点" class="headerlink" title="2.2 思考点"></a>2.2 思考点</h3><pre class=" language-properties"><code class="language-properties">使用状态编程来对多次进入的用户进行过滤，那么问题来了，状态里面存入什么才能更好的对用户进行一个过滤勒？状态如果为null，说明这个数据是新的，如果状态中存入的是时间，那么和数据中的时间进行比较就可以判断是否是重复的数据</code></pre><h3 id="2-3-如何去清空状态"><a href="#2-3-如何去清空状态" class="headerlink" title="2.3 如何去清空状态"></a>2.3 如何去清空状态</h3><pre class=" language-properties"><code class="language-properties">我们使用TTL的方式来进行清空状态昨天的状态，状态有没有关系都是没有关系，判断的时候一定要进行双重判断，否则会进行数据丢失按天去重，最少也是要保存一天的</code></pre><h3 id="2-4-两种方式的实现"><a href="#2-4-两种方式的实现" class="headerlink" title="2.4 两种方式的实现"></a>2.4 两种方式的实现</h3><pre class=" language-properties"><code class="language-properties">有的公司加入了session会话</code></pre><h3 id="2-5-测试流程"><a href="#2-5-测试流程" class="headerlink" title="2.5 测试流程"></a>2.5 测试流程</h3><pre class=" language-properties"><code class="language-properties">我们在进行测试的时候，一定得清楚数据流是如何进行处理得，如果不能清楚这个得化，是很难能够了解整个业务得，最后可以加一个测试，因为就是那个第一条和第二条都是可以加入测试的<span class="token attr-name">总之这个Flink</span> <span class="token attr-value">UV统计是流计算当中最简单的一个，因为我们只需要判断它是否是活跃或者访问的是进入页面的首页面，然后判断去重就可以直接将数据进行一个输出的操作</span><span class="token attr-name">其中设计到的kafka</span> <span class="token attr-value">topic</span><span class="token attr-name">Window</span><span class="token punctuation">:</span>    BaseLogAPP    UniqueVisitAPPLinux：<span class="token attr-name">    java</span> <span class="token attr-value">-jar gmall2020-mock-log-2020-12-18.jar</span><span class="token attr-name">    java</span> <span class="token attr-value">-jar userBehaviorLog-0.0.1-SNAPSHOT.jar</span>    dwd_page_log    dwm_unique_visit目前系统内存占比是</code></pre><h3 id="2-6-能力提升和项目问题疑问"><a href="#2-6-能力提升和项目问题疑问" class="headerlink" title="2.6 能力提升和项目问题疑问"></a>2.6 能力提升和项目问题疑问</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/state/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/state/</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/custom_serialization/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/custom_serialization/</a></p><h4 id="2-6-1-Flink对状态清空的方法"><a href="#2-6-1-Flink对状态清空的方法" class="headerlink" title="2.6.1 Flink对状态清空的方法"></a>2.6.1 Flink对状态清空的方法</h4><ul><li>使用自动的TTL来对状态进行清空</li><li>使用定时器来对状态进行手动的清空</li></ul><h4 id="2-6-2-案例演示"><a href="#2-6-2-案例演示" class="headerlink" title="2.6.2 案例演示"></a>2.6.2 案例演示</h4><h3 id="2-7-使用到的技术点"><a href="#2-7-使用到的技术点" class="headerlink" title="2.7 使用到的技术点"></a>2.7 使用到的技术点</h3><pre class=" language-properties"><code class="language-properties">状态检查点删除检查点动态分流</code></pre><h3 id="2-8-思考"><a href="#2-8-思考" class="headerlink" title="2.8 思考"></a>2.8 思考</h3><pre class=" language-properties"><code class="language-properties">如果业务数据库不是MySQL的话，我们可以采取定时去读取的那种方式，今天把它给弄明白，然后彻底的掌握清楚</code></pre><h2 id="第3章-DWM层-跳出明细计算"><a href="#第3章-DWM层-跳出明细计算" class="headerlink" title="第3章 DWM层-跳出明细计算"></a>第3章 DWM层-跳出明细计算</h2><h3 id="3-1-需求分析与思路"><a href="#3-1-需求分析与思路" class="headerlink" title="3.1   需求分析与思路"></a>3.1   需求分析与思路</h3><h4 id="3-1-1-什么是跳出"><a href="#3-1-1-什么是跳出" class="headerlink" title="3.1.1 什么是跳出"></a>3.1.1 什么是跳出</h4><pre class=" language-properties"><code class="language-properties">跳出就是用户成功访问了网站的一个页面后就退出，不在继续访问网站的其它页面。而跳出率就是用跳出次数除以访问次数。关注跳出率，可以看出引流过来的访客是否能很快的被吸引，渠道引流过来的用户之间的质量对比，对于应用优化前后跳出率的对比也能看出优化改进的成果。</code></pre><h4 id="3-1-2-计算跳出行为的思路"><a href="#3-1-2-计算跳出行为的思路" class="headerlink" title="3.1.2 计算跳出行为的思路"></a>3.1.2 计算跳出行为的思路</h4><pre class=" language-properties"><code class="language-properties">首先要识别哪些是跳出行为，要把这些跳出的访客最后一个访问的页面识别出来。那么要抓住几个特征：<span class="token attr-name"></span> <span class="token attr-value">   该页面是用户近期访问的第一个页面</span>这个可以通过该页面是否有上一个页面（last_page_id）来判断，如果这个表示为空，就说明这是这个访客这次访问的第一个页面。<span class="token attr-name"></span> <span class="token attr-value">   首次访问之后很长一段时间（自己设定），用户没继续再有其他页面的访问。</span>这第一个特征的识别很简单，保留last_page_id为空的就可以了。但是第二个访问的判断，其实有点麻烦，首先这不是用一条数据就能得出结论的，需要组合判断，要用一条存在的数据和不存在的数据进行组合判断。而且要通过一个不存在的数据求得一条存在的数据。更麻烦的他并不是永远不存在，而是在一定时间范围内不存在。那么如何识别有一定失效的组合行为呢？最简单的办法就是Flink自带的CEP技术。这个CEP非常适合通过多条数据组合来识别某个事件。用户跳出事件，本质上就是一个条件事件加一个超时事件的组合。</code></pre><h3 id="3-2-代码实现"><a href="#3-2-代码实现" class="headerlink" title="3.2    代码实现"></a>3.2    代码实现</h3><h4 id="3-2-1-从kafka的dwd-page-log主题中读取页面日志"><a href="#3-2-1-从kafka的dwd-page-log主题中读取页面日志" class="headerlink" title="3.2.1 从kafka的dwd_page_log主题中读取页面日志"></a>3.2.1 从kafka的dwd_page_log主题中读取页面日志</h4><h4 id="3-2-2-通过Flink的CEP完成跳出判断"><a href="#3-2-2-通过Flink的CEP完成跳出判断" class="headerlink" title="3.2.2 通过Flink的CEP完成跳出判断"></a>3.2.2 通过Flink的CEP完成跳出判断</h4><h5 id="1-确认添加了CEP的依赖包"><a href="#1-确认添加了CEP的依赖包" class="headerlink" title="1)  确认添加了CEP的依赖包"></a>1)  确认添加了CEP的依赖包</h5><h5 id="2-设定时间语义为事件时间并指定数据中的ts字段为事件时间"><a href="#2-设定时间语义为事件时间并指定数据中的ts字段为事件时间" class="headerlink" title="2)  设定时间语义为事件时间并指定数据中的ts字段为事件时间"></a>2)  设定时间语义为事件时间并指定数据中的ts字段为事件时间</h5><h5 id="3-根据日志数据的mid进行分组"><a href="#3-根据日志数据的mid进行分组" class="headerlink" title="3)  根据日志数据的mid进行分组"></a>3)  根据日志数据的mid进行分组</h5><h5 id="4-配置CEP表达式"><a href="#4-配置CEP表达式" class="headerlink" title="4)  配置CEP表达式"></a>4)  配置CEP表达式</h5><h5 id="5-根据表达式筛选流"><a href="#5-根据表达式筛选流" class="headerlink" title="5)  根据表达式筛选流"></a>5)  根据表达式筛选流</h5><h5 id="6-提取命中的数据"><a href="#6-提取命中的数据" class="headerlink" title="6)  提取命中的数据"></a>6)  提取命中的数据</h5><h4 id="3-2-3-将跳出数据写回到kafka的DWM层"><a href="#3-2-3-将跳出数据写回到kafka的DWM层" class="headerlink" title="3.2.3 将跳出数据写回到kafka的DWM层"></a>3.2.3 将跳出数据写回到kafka的DWM层</h4><h4 id="3-2-4-测试"><a href="#3-2-4-测试" class="headerlink" title="3.2.4 测试"></a>3.2.4 测试</h4><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_11","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263563000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_11","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263564000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_12","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263564000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_13","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263570000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_13","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263574000}</span></code></pre><p>超时事件处理不是很难，主要是业务是比较难得，我们通过处理放到了kafka层</p><h2 id="第4章-DWM层-订单宽表"><a href="#第4章-DWM层-订单宽表" class="headerlink" title="第4章 DWM层-订单宽表"></a>第4章 DWM层-订单宽表</h2><p>重点：旁路缓存，异步IO</p><p>GMV做分时统计</p><p>这里有一个疑问就是，table_process表里面的数据和kafka的字段的信息是不一样的，出现了问题了，这里需要排查一下</p><p>订单宽表是整个数仓当中最难的整个了</p><p>Phoenix构建表的时候也是非常关键的</p><p>列，值，对象，如何去结合在一起，将属性和值设置给对象 commonBeanutils</p><h2 id="第5章-DWM层支付宽表（练习）"><a href="#第5章-DWM层支付宽表（练习）" class="headerlink" title="第5章 DWM层支付宽表（练习）"></a>第5章 DWM层支付宽表（练习）</h2><h2 id="第6章-总结"><a href="#第6章-总结" class="headerlink" title="第6章 总结"></a>第6章 总结</h2>]]></content>
      
      
      <categories>
          
          <category> 企业级数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 企业级数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL面试必刷</title>
      <link href="/2021/08/22/MySQL%E9%9D%A2%E8%AF%95%E5%BF%85%E5%88%B7/"/>
      <url>/2021/08/22/MySQL%E9%9D%A2%E8%AF%95%E5%BF%85%E5%88%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="数据库-MySQL"><a href="#数据库-MySQL" class="headerlink" title="数据库-MySQL"></a>数据库-MySQL</h1><p><a href="https://www.mysql.com/" target="_blank" rel="noopener">https://www.mysql.com/</a></p><p><a href="https://www.cs.usfca.edu/~galles/visualization/Algorithms.html" target="_blank" rel="noopener">https://www.cs.usfca.edu/~galles/visualization/Algorithms.html</a></p><h2 id="1、索引的底层数据结构是B还是B-树"><a href="#1、索引的底层数据结构是B还是B-树" class="headerlink" title="1、索引的底层数据结构是B还是B+树"></a>1、索引的底层数据结构是B还是B+树</h2><ul><li>为什么要设计索引</li><li>如果是你，会如何设计索引</li><li>设计索引的时候使用什么数据结构</li><li>MySQL是如何实现的</li></ul><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">OLTP：</span> <span class="token attr-value">业务系统的支持，及时性非常高;</span>OLAP：对历史数据的分析MySQL中使用的是B+树索引，为什么要使用B+树，面试中可能会聊到树的概念官网中有B树，底层是使用的B+树？他们之间有什么区别？MySQL中除了B,B+有其它数据结构？<span class="token attr-name">Hash</span> <span class="token attr-name">存储引擎</span> <span class="token attr-value">---> 不同的数据文件在物理磁盘上的不同的组织形式</span>MySQL的存储引擎</code></pre><p>MySQL中的存储引擎</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822175643586.png" alt="MySQL的存储引擎"></p><p>默认是使用InnoDB，Memory使用的是Hash索引</p><pre class=" language-properties"><code class="language-properties">InnoDB支持Hash索引，但是是自适应Hash，为什么要使用自适应的Hash</code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822175834821.png" alt="索引的数据结构"></p><p>存储引擎到底是个啥，可以去存储的目录下面去查看这些文件的存储格式，不同的存储引擎的格式是不一样的</p><h2 id="2、MySQL的索引系统"><a href="#2、MySQL的索引系统" class="headerlink" title="2、MySQL的索引系统"></a>2、MySQL的索引系统</h2><pre class=" language-properties"><code class="language-properties">操作系统的支持，数据都是会放到磁盘上，但是对数据进行实际读取的时候，加载内存，然后才能读取到数据，磁盘和内存的读取的速度是不一样的硬件的延迟对应图，内存的数据很快的，需要把磁盘的数据加载到内存中，如果内存是无限大，可以全量的加载，内存的数据是不安全的，因此还是会进行持久化的操作预加载的过程，取一行记录，操作系统的一方面就是磁盘预读，局部性的原理程序有聚集趋向，时间局部性，磁盘预读，读的是某一个页的数据，页的大小和操作系统大小是有关系，一般都是4K，一般都是页的整数倍Hash表和二叉树hash，二叉树，B树，B+树，这个非常重要的，面试面的很多，得举例<span class="token attr-name">Hash</span> <span class="token attr-value">:</span><span class="token attr-name">    1、存在hash碰撞，必须要设置一个非常完美的hash算法，必须有一个</span> <span class="token attr-value">扰动函数</span>    2、hash适合等值查询，不适合范围查询红黑树、B树、B+树红黑树的数据格式：    HashMap里面的东西得结合起来</code></pre><h2 id="3、前缀索引实例说明"><a href="#3、前缀索引实例说明" class="headerlink" title="3、前缀索引实例说明"></a>3、前缀索引实例说明</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">degree</span> <span class="token attr-value">度</span>磁盘块的放取，16K，是磁盘预读，</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java设计模式</title>
      <link href="/2021/08/22/java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
      <url>/2021/08/22/java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="java设计模式"><a href="#java设计模式" class="headerlink" title="java设计模式"></a>java设计模式</h1><h2 id="1、工厂模式"><a href="#1、工厂模式" class="headerlink" title="1、工厂模式"></a>1、工厂模式</h2><h3 id="1-1-需求设计"><a href="#1-1-需求设计" class="headerlink" title="1.1 需求设计"></a>1.1 需求设计</h3><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">我们将创建一个</span> <span class="token attr-value">Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。</span><span class="token attr-name">FactoryPatternDemo</span> <span class="token attr-value">类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（circle / rectangle / square），以便获取它所需对象的类型。</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822165337756.png" alt="工厂模式设计"></p><h3 id="1-2-工厂设计模式的优点"><a href="#1-2-工厂设计模式的优点" class="headerlink" title="1.2 工厂设计模式的优点"></a>1.2 工厂设计模式的优点</h3><ul><li>面向接口编程，体现了面向对象的思想；</li><li>将创建对象的工作转移到了工厂类；</li></ul><h3 id="1-3-案例实现"><a href="#1-3-案例实现" class="headerlink" title="1.3 案例实现"></a>1.3 案例实现</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822170436248.png" alt="工厂类案例实现"></p><p>这些代码我都会放到github中的，有感兴趣的都可以去查阅</p><h3 id="1-4-JDK-中的工厂设计模式实例"><a href="#1-4-JDK-中的工厂设计模式实例" class="headerlink" title="1.4 JDK 中的工厂设计模式实例"></a>1.4 JDK 中的工厂设计模式实例</h3><ul><li>java.util.Calendar, ResourceBundle and NumberFormat getInstance() 使用了工厂方法模式；</li><li>valueOf() 在包装类中，如Boolean, Integer 也使用了工厂方法模式；</li></ul><h4 id="1-4-1-Calendar"><a href="#1-4-1-Calendar" class="headerlink" title="1.4.1 Calendar"></a>1.4.1 Calendar</h4><p>让我们一起来了解一下这个Calendar这个类到底是做什么的，是否能够在我们的工作当中能够用到，Java中Calendar类的常用方法</p><h5 id="1-4-1-1-获取时间"><a href="#1-4-1-1-获取时间" class="headerlink" title="1.4.1.1  获取时间"></a>1.4.1.1  获取时间</h5><pre class=" language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">getTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 使用默认时区和语言获得一个日历</span>    Calendar calendar <span class="token operator">=</span> Calendar<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 年月日时分秒，注意月份要从小标开始，所以取月份要+1</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>DAY_OF_WEEK<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 1</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>DAY_OF_YEAR<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 234</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>DAY_OF_WEEK_IN_MONTH<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 4</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"==========================================="</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"年："</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>YEAR<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 2021</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"月："</span> <span class="token operator">+</span> <span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>MONTH<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 8</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"日："</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>DAY_OF_MONTH<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 22</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"时:"</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>HOUR_OF_DAY<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 5</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"分："</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>MINUTE<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 24</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"秒："</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>SECOND<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 8</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>YEAR<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> <span class="token punctuation">(</span>calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>MONTH<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>DAY_OF_MONTH<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" "</span>                       <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>HOUR_OF_DAY<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">":"</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>MINUTE<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">":"</span> <span class="token operator">+</span> calendar<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>SECOND<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h5 id="1-4-1-2-设置时间"><a href="#1-4-1-2-设置时间" class="headerlink" title="1.4.1.2 设置时间"></a>1.4.1.2 设置时间</h5><pre class=" language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    Calendar cal <span class="token operator">=</span> Calendar<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 如果想设置为某个日期，可以一次设置年月日时分秒，由于月份下标从0开始赋值月份要-1</span>    <span class="token comment" spellcheck="true">// cal.set(year, month, date, hourOfDay, minute, second);</span>    cal<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token number">2018</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 或者6个字段分别进行设置，由于月份下标从0开始赋值月份要-1</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>cal<span class="token punctuation">.</span><span class="token function">getTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cal<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>SECOND<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cal<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>Calendar<span class="token punctuation">.</span>MONTH<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>cal<span class="token punctuation">.</span><span class="token function">getTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>这个是简单的时间的一些计算方式，用到的时候在进行一个查看就好了</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客维护问题</title>
      <link href="/2021/08/22/%E5%8D%9A%E5%AE%A2%E7%BB%B4%E6%8A%A4%E9%97%AE%E9%A2%98/"/>
      <url>/2021/08/22/%E5%8D%9A%E5%AE%A2%E7%BB%B4%E6%8A%A4%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="博客维护"><a href="#博客维护" class="headerlink" title="博客维护"></a>博客维护</h1><h2 id="1、解决博客上传问题"><a href="#1、解决博客上传问题" class="headerlink" title="1、解决博客上传问题"></a>1、解决博客上传问题</h2><p>有时间会上传失败的话，多尝试几次就好了，可能是网速不太好</p><h2 id="2、上传图床的问题"><a href="#2、上传图床的问题" class="headerlink" title="2、上传图床的问题"></a>2、上传图床的问题</h2><p>我选择的是gitee做为自己的图床，来进行保存图片，图片就不需要本地来进行保存了，这样很方便的就行文件的复制以及和移动</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822163427850.png" alt="图床的准备"></p><h2 id="3、解决问题超时的问题"><a href="#3、解决问题超时的问题" class="headerlink" title="3、解决问题超时的问题"></a>3、解决问题超时的问题</h2><p>网上的资料好多都很乱，只需要更改一下下面这个就好了，特别是要注意的自己的配置</p><h1 id="这个是上传到github超时的时候给配置的"><a href="#这个是上传到github超时的时候给配置的" class="headerlink" title="这个是上传到github超时的时候给配置的"></a>这个是上传到github超时的时候给配置的</h1><p>151.101.185.194 github.global-ssl.fastly.net</p>]]></content>
      
      
      <categories>
          
          <category> 博客维护 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客维护 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis单节点部署</title>
      <link href="/2021/08/19/Redis%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/19/Redis%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Redis单节点部署"><a href="#大数据环境搭建-Redis单节点部署" class="headerlink" title="大数据环境搭建-Redis单节点部署"></a>大数据环境搭建-Redis单节点部署</h1><h2 id="安装编译器"><a href="#安装编译器" class="headerlink" title="安装编译器"></a>安装编译器</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">yum</span> <span class="token attr-value">install centos-release-scl scl-utils-build</span><span class="token attr-name">yum</span> <span class="token attr-value">install -y devtoolset-8-toolchain</span><span class="token attr-name">scl</span> <span class="token attr-value">enable devtoolset-8 bash</span></code></pre><h2 id="安装Redis"><a href="#安装Redis" class="headerlink" title="安装Redis"></a>安装Redis</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">tar</span> <span class="token attr-value">-zxvf redis-6.2.1.tar.gz -C /opt/module/</span><span class="token attr-name">cd</span> <span class="token attr-value">redis-6.2.1</span>make如果出错了的话，就执行这个，出错的原因可能就是编译器的安装没有准备好<span class="token attr-name">make</span> <span class="token attr-value">distclean  ---> 如果出错了，先执行make，然后在执行make install</span><span class="token attr-name">make</span> <span class="token attr-value">install</span>安装目录：/usr/local/bin查看默认安装目录：<span class="token attr-name">redis-benchmark</span><span class="token punctuation">:</span><span class="token attr-value">性能测试工具，可以在自己本子运行，看看自己本子性能如何</span>redis-check-aof：修复有问题的AOF文件，rdb和aof后面讲redis-check-dump：修复有问题的dump.rdb文件redis-sentinel：Redis集群使用redis-server：Redis服务器启动命令redis-cli：客户端，操作入口<span class="token attr-name">mv</span> <span class="token attr-value">redis.conf redis</span><span class="token attr-name">daemonize</span> <span class="token attr-value">no改成yes</span></code></pre><h2 id="配置文件的更改"><a href="#配置文件的更改" class="headerlink" title="配置文件的更改"></a>配置文件的更改</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">daemonize</span> <span class="token attr-value">yes ---> 后台启动</span><span class="token attr-name">bind</span> <span class="token attr-value">0.0.0.0  ---> 表任何IP地址都能连接上，以前只有127.0.0.1表示主机连接</span><span class="token attr-name">c</span> <span class="token attr-value">no ---> 将也是改成no，将安全连接关闭，不然也连接不上</span><span class="token attr-name">dir</span> <span class="token attr-value">/opt/module/redis/db</span><span class="token attr-name">dbfilename</span> <span class="token attr-value">wmy.rdb</span><span class="token attr-name">pidfile</span> <span class="token attr-value">/opt/module/redis/redis_6379.pid</span>查看是否关闭防火墙<span class="token attr-name">systemctl</span> <span class="token attr-value">status firewalld</span><span class="token attr-name">systemctl</span> <span class="token attr-value">stop firewalld.service</span><span class="token attr-name">redis-server</span> <span class="token attr-value">/opt/module/redis/redis.conf</span><span class="token attr-name">ps</span> <span class="token attr-value">-ef | grep redis</span><span class="token attr-name">redis-cli</span> <span class="token attr-value">-h flink01</span><span class="token attr-name">redis-cli</span> <span class="token attr-value">shutdown</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBASE2.0.5分布式部署</title>
      <link href="/2021/08/18/HBASE2-0-5%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/HBASE2-0-5%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-HBASE2-0-5分布式部署"><a href="#大数据环境搭建-HBASE2-0-5分布式部署" class="headerlink" title="大数据环境搭建-HBASE2.0.5分布式部署"></a>大数据环境搭建-HBASE2.0.5分布式部署</h1><h2 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h2><pre class=" language-xml"><code class="language-xml">export HBASE_MANAGES_ZK=false</code></pre><h2 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.rootdir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820/hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.cluster.distributed<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01,flink02,flink03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.master.maxclockskew<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>180000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Time difference of regionserver from master<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="kylin"><a href="#kylin" class="headerlink" title="kylin"></a>kylin</h2><p>这个安装很简单，只需要解压完成就好了</p><h2 id="phoenix安装"><a href="#phoenix安装" class="headerlink" title="phoenix安装"></a>phoenix安装</h2><p>安装成功了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark3.0.0-hadoop3.2分布式部署</title>
      <link href="/2021/08/18/Spark3-0-0-hadoop3-2%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Spark3-0-0-hadoop3-2%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Spark3-0-0-hadoop3-2分布式部署"><a href="#大数据环境搭建-Spark3-0-0-hadoop3-2分布式部署" class="headerlink" title="大数据环境搭建-Spark3.0.0-hadoop3.2分布式部署"></a>大数据环境搭建-Spark3.0.0-hadoop3.2分布式部署</h1><h2 id="spark-env-sh"><a href="#spark-env-sh" class="headerlink" title="spark-env.sh"></a>spark-env.sh</h2><pre class=" language-shell"><code class="language-shell">SPARK_MASTER_HOST=flink01SPARK_MASTER_PORT=7077export JAVA_HOME=/opt/module/jdk1.8.0_144</code></pre><h2 id="spark-defaults-conf"><a href="#spark-defaults-conf" class="headerlink" title="spark-defaults.conf"></a>spark-defaults.conf</h2><pre class=" language-shell"><code class="language-shell">spark.master    yarnspark.eventLog.enabled    truespark.eventLog.dir    hdfs://flink01:9820/spark-historyspark.executor.memory    1gspark.driver.memory    1gspark.yarn.jars hdfs://flink01:9820/spark-jars/*</code></pre><h2 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h2><pre class=" language-xml"><code class="language-xml">flink01flink02flink03</code></pre><p>在这里，我只是把Spark当作了Hive的计算引擎了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveOnSpark分布式搭建</title>
      <link href="/2021/08/18/hiveOnSpark%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/hiveOnSpark%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建HiveOnSpark"><a href="#大数据环境搭建HiveOnSpark" class="headerlink" title="大数据环境搭建HiveOnSpark"></a>大数据环境搭建HiveOnSpark</h1><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><pre class=" language-shell"><code class="language-shell">mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bakmv mysql-connector-java-5.1.27-bin.jar hive-3.1.2/lib/</code></pre><h2 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162352155.png" alt="image-20210822162352155"></p><p>其实很奇怪的事情就是，我们hive把spark给当作引擎，我们可以不用启动，但是还是必须要有的，不过这个也不奇怪，毕竟在工作当中，一个平台不可能没有Spark的</p><h2 id="capacity-scheduler-xml"><a href="#capacity-scheduler-xml" class="headerlink" title="capacity-scheduler.xml"></a>capacity-scheduler.xml</h2><p>记住，这个要和hadoop配置文件中的都必须是类似的</p><pre class=" language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--     Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License. See accompanying LICENSE file.--></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Maximum number of applications that can be pending and running.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>0.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Maximum percent of resources in the cluster which can be used to run   application masters i.e. controls number of concurrent running  applications.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.resource-calculator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ResourceCalculator implementation to be used to compare   Resources in the scheduler.  The default i.e. DefaultResourceCalculator only uses Memory while  DominantResourceCalculator uses dominant-resource to compare   multi-dimensional resources such as Memory, CPU etc.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.queues<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>default,hive<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The queues at the this level (root is the root queue).<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>50<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Default queue target capacity.yuan 50 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.user-limit-factor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Default queue user limit a percentage from 0.0 to 1.0.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>100<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The maximum capacity of the default queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.state<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>RUNNING<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The state of the default queue. State can be one of RUNNING or STOPPED.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_submit_applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can submit jobs to the default queue.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_administer_queue<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can administer jobs on the default queue.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_application_max_priority<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can submit applications with configured priority.  For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.maximum-application-lifetime <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>    Maximum lifetime of an application which is submitted to a queue    in seconds. Any value less than or equal to zero will be considered as    disabled.    This will be a hard time limit for all applications in this    queue. If positive value is configured then any application submitted    to this queue will be killed after exceeds the configured lifetime.    User can also specify lifetime per application basis in    application submission context. But user lifetime will be    overridden if it exceeds queue maximum lifetime. It is point-in-time    configuration.    Note : Configuring too low value will result in killing application    sooner. This feature is applicable only for leaf queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.default-application-lifetime <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>    Default lifetime of an application which is submitted to a queue    in seconds. Any value less than or equal to zero will be considered as    disabled.    If the user has not submitted application with lifetime value then this    value will be taken. It is point-in-time configuration.    Note : Default lifetime can't exceed maximum lifetime. This feature is    applicable only for leaf queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.node-locality-delay<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>40<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Number of missed scheduling opportunities after which the CapacityScheduler   attempts to schedule rack-local containers.  When setting this parameter, the size of the cluster should be taken into account.  We use 40 as the default value, which is approximately the number of nodes in one rack.  Note, if this value is -1, the locality constraint in the container request  will be ignored, which disables the delay scheduling.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.rack-locality-additional-delay<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Number of additional missed scheduling opportunities over the node-locality-delay  ones, after which the CapacityScheduler attempts to schedule off-switch containers,  instead of rack-local ones.  Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will  attempt rack-local assignments after 40 missed opportunities, and off-switch assignments  after 40+20=60 missed opportunities.  When setting this parameter, the size of the cluster should be taken into account.  We use -1 as the default value, which disables this feature. In this case, the number  of missed opportunities for assigning off-switch containers is calculated based on  the number of containers and unique locations specified in the resource request,  as well as the size of the cluster.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.queue-mappings<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  A list of mappings that will be used to assign jobs to queues  The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*  Typically this list will be used to map users to queues,  for example, u:%user:%user maps all users to queues with the same name  as the user.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.queue-mappings-override.enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  If a queue mapping is present, will it override the value specified  by the user? This can be used by administrators to place jobs in queues  that are different than the one specified by the user.  The default is false.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Controls the number of OFF_SWITCH assignments allowed  during a node's heartbeat. Increasing this value can improve  scheduling rate for OFF_SWITCH containers. Lower values reduce  "clumping" of applications on particular nodes. The default is 1.  Legal values are 1-MAX_INT. This config is refreshable.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.application.fail-fast<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Whether RM should fail during recovery if previous applications'  queue is no longer valid.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>0.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  集群中用于运行应用程序ApplicationMaster的资源比例上限，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的ApplicationMaster资源比例上限可通过参数yarn.scheduler.capacity.maximum-am-resource-percent设置，而单个队列可通过参数yarn.scheduler.capacity. queue-path .maximum-am-resource-percent设置适合自己的值。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>50<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列的容量为50%<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  一个用户最多能够获取该队列资源容量的比例，取值0-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>80<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列的最大容量（自己队列资源不够，可以使用其他队列资源上限）<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.state<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>RUNNING<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  开启hive队列运行，不设置队列不能使用<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  访问控制，控制谁可以将任务提交到该队列,*表示任何人<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  指定哪个用户可以提交配置任务优先级<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>            org.apache.hadoop.io.compress.GzipCodec,            org.apache.hadoop.io.compress.DefaultCodec,            org.apache.hadoop.io.compress.BZip2Codec,            org.apache.hadoop.io.compress.SnappyCodec,            com.hadoop.compression.lzo.LzoCodec,            com.hadoop.compression.lzo.LzopCodec        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink03:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 测试环境指定HDFS副本的数量1 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hive-site-xml-1"><a href="#hive-site-xml-1" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://flink01:3306/wmyHive?useSSL=false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>000000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.warehouse.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/user/hive/warehouse<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.schema.verification<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.server2.thrift.port<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.server2.thrift.bind.host<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.event.db.notification.api.auth<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.header<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.current.db<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>spark.yarn.jars<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820/spark-jars/*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>      <span class="token comment" spellcheck="true">&lt;!--Hive执行引擎--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.execution.engine<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--Hive和Spark连接超时时间--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.spark.client.connect.timeout<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1000000ms<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.uris<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>thrift://flink01:9083<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 历史服务器端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 历史服务器web端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="spark-defaults-conf"><a href="#spark-defaults-conf" class="headerlink" title="spark-defaults.conf"></a>spark-defaults.conf</h2><pre class=" language-xml"><code class="language-xml">spark.master    yarnspark.eventLog.enabled    truespark.eventLog.dir    hdfs://flink01:9820/spark-historyspark.executor.memory    1gspark.driver.memory    1gspark.yarn.jars hdfs://flink01:9820/spark-jars/*</code></pre><h2 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对虚拟内存的限制检查 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 开启日志聚集功能 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志聚集服务器地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log.server.url<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>http://flink01:19888/jobhistory/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志保留时间为7天 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation.retain-seconds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>604800<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>如果想要看Spark是怎么安装的也可以写一篇博客来进行记录</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162412732.png" alt="image-20210822162412732"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop3.1.3分布式环境搭建</title>
      <link href="/2021/08/18/hadoop3-1-3%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/hadoop3-1-3%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-hadoop3-1-3"><a href="#大数据环境搭建-hadoop3-1-3" class="headerlink" title="大数据环境搭建-hadoop3.1.3"></a>大数据环境搭建-hadoop3.1.3</h1><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th></th><th>flink01</th><th>flink02</th><th>flink03</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode | DataNode</td><td>DataNode</td><td>SecondaryNameNode | DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager | NodeManager</td><td>NodeManager</td></tr></tbody></table><h2 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>            org.apache.hadoop.io.compress.GzipCodec,            org.apache.hadoop.io.compress.DefaultCodec,            org.apache.hadoop.io.compress.BZip2Codec,            org.apache.hadoop.io.compress.SnappyCodec,            com.hadoop.compression.lzo.LzoCodec,            com.hadoop.compression.lzo.LzopCodec        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink03:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 测试环境指定HDFS副本的数量1 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对虚拟内存的限制检查 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 开启日志聚集功能 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志聚集服务器地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log.server.url<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>http://flink01:19888/jobhistory/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志保留时间为7天 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation.retain-seconds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>604800<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 历史服务器端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 历史服务器web端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hadoop-env-sh-yarn-env-sh-mapred-env-sh"><a href="#hadoop-env-sh-yarn-env-sh-mapred-env-sh" class="headerlink" title="hadoop-env.sh,yarn-env.sh,mapred-env.sh"></a>hadoop-env.sh,yarn-env.sh,mapred-env.sh</h2><pre class=" language-xml"><code class="language-xml">export JAVA_HOME=/opt/flink/platform/jdk1.8.0_144</code></pre><h2 id="将start-dfs-sh，stop-dfs-sh两个文件顶部添加以下参数"><a href="#将start-dfs-sh，stop-dfs-sh两个文件顶部添加以下参数" class="headerlink" title="将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数"></a>将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数</h2><pre class=" language-shell"><code class="language-shell">HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root</code></pre><h2 id="还有，start-yarn-sh，stop-yarn-sh顶部也需添加以下"><a href="#还有，start-yarn-sh，stop-yarn-sh顶部也需添加以下" class="headerlink" title="还有，start-yarn.sh，stop-yarn.sh顶部也需添加以下"></a>还有，start-yarn.sh，stop-yarn.sh顶部也需添加以下</h2><pre class=" language-shell"><code class="language-shell">YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>平台节点的免密登录</title>
      <link href="/2021/08/18/%E5%B9%B3%E5%8F%B0%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/"/>
      <url>/2021/08/18/%E5%B9%B3%E5%8F%B0%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-免密节点设置"><a href="#大数据环境搭建-免密节点设置" class="headerlink" title="大数据环境搭建-免密节点设置"></a>大数据环境搭建-免密节点设置</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>免密设置的初心就是大数据节点在平台之间的传输的过程当中不会进行一个密码的输入，简化开发，其中设置和配置是非常简单的，但是需要使用一个技巧就是必须的发送所有命令到所有的会话，这样copy的次数的就会比较少</p><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">192.168.21.151</span> <span class="token attr-value">flink01</span><span class="token attr-name">192.168.21.152</span> <span class="token attr-value">flink02</span><span class="token attr-name">192.168.21.153</span> <span class="token attr-value">flink03</span><span class="token attr-name">ssh-keygen</span> <span class="token attr-value">-t rsa </span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink01</span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink02</span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink03</span></code></pre><p>这样就可以搭建成功了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JDK1.8分布式环境搭建</title>
      <link href="/2021/08/18/JDK1-8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/JDK1-8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-JDK环境搭建"><a href="#大数据环境搭建-JDK环境搭建" class="headerlink" title="大数据环境搭建-JDK环境搭建"></a>大数据环境搭建-JDK环境搭建</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>大数据平台环境搭建基本的原理都是一个类似的操作</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>jdk-8u144-linux-x64.tar.gz</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><p>这个的话只需要进行一个解压操作就好了</p><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>java</p><p>javac</p><p>java -version</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据组件Shell脚本</title>
      <link href="/2021/08/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6Shell%E8%84%9A%E6%9C%AC/"/>
      <url>/2021/08/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6Shell%E8%84%9A%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<h2 id="大数据平台组件的命令"><a href="#大数据平台组件的命令" class="headerlink" title="大数据平台组件的命令"></a>大数据平台组件的命令</h2><h2 id="查看进程脚本"><a href="#查看进程脚本" class="headerlink" title="查看进程脚本"></a>查看进程脚本</h2><pre class=" language-shell"><code class="language-shell">#!/bin/bashfor i in flink01 flink02 flink02do   echo "====================== $i JPS ======================="   ssh $i /opt/module/jdk1.8.0_144/bin/jpsdone</code></pre><h2 id="传输文件"><a href="#传输文件" class="headerlink" title="传输文件"></a>传输文件</h2><p>yum -y install rsync，记住这个话必须得先安装这个，要不然得就会报错。。。</p><pre class=" language-shell"><code class="language-shell">#!/bin/bash#1. 判断参数个数if [ $# -lt 1 ]then  echo Not Enough Arguement!  exit;fi#2. 遍历集群所有机器for host in flink01 flink02 flink03do  echo ====================  $host  ====================  #3. 遍历所有目录，挨个发送  for file in $@  do    #4 判断文件是否存在    if [ -e $file ]    then      #5. 获取父目录      pdir=$(cd -P $(dirname $file); pwd)      #6. 获取当前文件的名称      fname=$(basename $file)      ssh $host "mkdir -p $pdir"      rsync -av $pdir/$fname $host:$pdir    else      echo $file does not exists!    fi  donedone</code></pre><h2 id="查看kafka-topic个数"><a href="#查看kafka-topic个数" class="headerlink" title="查看kafka topic个数"></a>查看kafka topic个数</h2><pre class=" language-shell"><code class="language-shell">/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper flink01:2181/kafka --list</code></pre><h2 id="启动kafka"><a href="#启动kafka" class="headerlink" title="启动kafka"></a>启动kafka</h2><pre class=" language-properties"><code class="language-properties"><span class="token comment" spellcheck="true">#! /bin/bash</span><span class="token attr-name">case</span> <span class="token attr-value">$1 in</span>"start"){<span class="token attr-name">    for</span> <span class="token attr-value">i in flink01 flink02 flink03</span>    do<span class="token attr-name">        echo</span> <span class="token attr-value">"------------------- kafka $i -------------------"</span><span class="token attr-name">        ssh</span> <span class="token attr-value">$i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties"</span>    done};;"stop"){<span class="token attr-name">        for</span> <span class="token attr-value">i in flink01 flink02 flink03</span>        do<span class="token attr-name">                echo</span> <span class="token attr-value">"------------------- kafka $i -------------------"</span><span class="token attr-name">                ssh</span> <span class="token attr-value">$i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh"</span>        done};;esac</code></pre><p>注意，这里有一个问题就是我们启动Kafka的时候使用这个是启动不起来的，但是可以使用单台进行一个启动操作</p><h2 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh</span> <span class="token attr-value">--zookeeper flink01:2181,flink02:2181,flink03:2181/kafka  --create --replication-factor 1 --partitions 1 --topic $1</span></code></pre><h2 id="启动Flink"><a href="#启动Flink" class="headerlink" title="启动Flink"></a>启动Flink</h2><pre class=" language-shell"><code class="language-shell">#! /bin/bashcase $1 in"start"){    echo "------------------- 启动flink -------------------"    /opt/module/flink-1.13.2/bin/start-cluster.sh};;"stop"){    echo "------------------- 关闭flink -------------------"    /opt/module/flink-1.13.2/bin/stop-cluster.sh};;esac</code></pre><h2 id="启动hadoop"><a href="#启动hadoop" class="headerlink" title="启动hadoop"></a>启动hadoop</h2><pre class=" language-shell"><code class="language-shell">#!/bin/bashif [ $# -lt 1 ]then    echo "No Args Input..."    exit ;ficase $1 in"start")        echo " =================== 启动 hadoop集群 ==================="        echo " --------------- 启动 hdfs ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"        echo " --------------- 启动 yarn ---------------"        ssh flink02 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"        echo " --------------- 启动 historyserver ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver";;"stop")        echo " =================== 关闭 hadoop集群 ==================="        echo " --------------- 关闭 historyserver ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"        echo " --------------- 关闭 yarn ---------------"        ssh flink02 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"        echo " --------------- 关闭 hdfs ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh";;*)    echo "Input Args Error...";;esac</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.13.2环境分布式部署</title>
      <link href="/2021/08/18/Flink1-13-2%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Flink1-13-2%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>Flink是不依赖于任何组件的，但是它可以去集成周边的组件化，来看一下如何去搭建一个Flink-1.13.2的环境勒，这里的话，我是搭建的Flink Standalone的方式来进行一个测试操作</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>flink1.13.2</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>使用启动命令来进行一个测试</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka-2-11-2.4.1环境分布式部署</title>
      <link href="/2021/08/18/Kafka-2-11-2-4-1%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Kafka-2-11-2-4-1%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-kafka-2-11-2-4-1"><a href="#大数据环境搭建-kafka-2-11-2-4-1" class="headerlink" title="大数据环境搭建-kafka-2.11-2.4.1"></a>大数据环境搭建-kafka-2.11-2.4.1</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>这个和以前有点区别，它是一依赖于zookeeper的，所以必须在zookeeper安装之后才能进行一个安装的处理</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>kafka_2.11-2.4.1.tgz</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><p>这个安装步骤很简单，但是要注意的就是必须和myid的id号一致，否则的话就会启动失败的操作</p><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162430215.png" alt="image-20210822162430215"></p><p>这个测试的方便一些，搭建的过程当中一定要注意细节性的问题，如果不注意的话就会花大量的时间查找错误的，这些错误是不值得去犯错的</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper3.5.7环境分布式部署</title>
      <link href="/2021/08/18/Zookeeper3-5-7%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Zookeeper3-5-7%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Zookeeper"><a href="#大数据环境搭建-Zookeeper" class="headerlink" title="大数据环境搭建-Zookeeper"></a>大数据环境搭建-Zookeeper</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>这个和之前都是一样的，首先得准备需要安装的节点的机器</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>zookeeper-3.5.7</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><pre class=" language-properties"><code class="language-properties">注意：必须在bin/zkEnv.sh<span class="token comment" spellcheck="true"># 如果不配置这个的话启动的时候就会报错误</span><span class="token attr-name">export</span> <span class="token attr-value">JAVA_HOME=/opt/flink/platform/jdk1.8.0_144</span><span class="token attr-name">dataDir</span><span class="token punctuation">=</span><span class="token attr-value">/opt/module/zookeeper-3.5.7/zkData</span><span class="token attr-name">server.1</span><span class="token punctuation">=</span><span class="token attr-value">flink01:2888:3888</span><span class="token attr-name">server.2</span><span class="token punctuation">=</span><span class="token attr-value">flink02:2888:3888</span><span class="token attr-name">server.3</span><span class="token punctuation">=</span><span class="token attr-value">flink03:2888:3888</span></code></pre><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>记住，这里的话就会有一个小坑，每一次都是myid不一致导致错误的，这里和kafka哪里也要注意，否则会花大量的时间进行一个排查错误的情况</p><pre class=" language-properties"><code class="language-properties"><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink01 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2795</span> <span class="token attr-value">Jps</span><span class="token attr-name">2525</span> <span class="token attr-value">QuorumPeerMain</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink02 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2038</span> <span class="token attr-value">QuorumPeerMain</span><span class="token attr-name">2086</span> <span class="token attr-value">Jps</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink02 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2038</span> <span class="token attr-value">QuorumPeerMain</span><span class="token attr-name">2104</span> <span class="token attr-value">Jps</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL5.7-Linux环境下部署</title>
      <link href="/2021/08/18/MySQL5-7-Linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/MySQL5-7-Linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-MySQL环境部署"><a href="#大数据环境搭建-MySQL环境部署" class="headerlink" title="大数据环境搭建-MySQL环境部署"></a>大数据环境搭建-MySQL环境部署</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>需要一台虚拟机，能够连网，防火墙关闭，各方面正常的虚拟机就可以开始MySQL的环境搭建了</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162500471.png" alt="image-20210822162500471"></p><h2 id="安装所需要的命令和步骤如何"><a href="#安装所需要的命令和步骤如何" class="headerlink" title="安装所需要的命令和步骤如何"></a>安装所需要的命令和步骤如何</h2><pre class=" language-properties"><code class="language-properties">1、查看是都有安装过MySQL或者是默认的mariadb<span class="token attr-name">rpm</span> <span class="token attr-value">-qa | grep mysql</span><span class="token attr-name">rpm</span> <span class="token attr-value">-qa | grep mariadb</span>2、删除自带的mariadba<span class="token attr-name">rpm</span> <span class="token attr-value">-e --nodeps mariadb-libs-5.5.56-2.el7.x86_64</span>3、解压MySQL的配置<span class="token attr-name">tar</span> <span class="token attr-value">-xvf mysql-5.7.19-1.el7.x86_64.rpm-bundle.tar</span>4、安装MySQL的rpm<span class="token attr-name">rpm</span> <span class="token attr-value">-ivh --nodeps mysql-community-server-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh --nodeps mysql-community-client-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm</span>5、查看是否启动以及启动MySQL<span class="token attr-name">systemctl</span> <span class="token attr-value">status mysqld</span><span class="token attr-name">systemctl</span> <span class="token attr-value">start mysqld</span>6、查看生成的密码<span class="token attr-name">cat</span> <span class="token attr-value">/var/log/mysqld.log | grep password</span>7、第一次登录MySQL<span class="token attr-name">mysql</span> <span class="token attr-value">-uroot -p</span>注意，如果出现了密码里面有一些特殊符号的，可以回车之后在进行输入密码就可以了8、对MySQL进行配置和设置密码以及对远程连接进行一个设置的操作<span class="token attr-name">set</span> <span class="token attr-value">global validate_password_policy=0;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_mixed_case_count=0; </span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_number_count=3;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_special_char_count=0;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_length=3;</span><span class="token attr-name">SET</span> <span class="token attr-value">PASSWORD FOR 'root'@'localhost' = PASSWORD('000000');</span><span class="token attr-name">update</span> <span class="token attr-value">mysql.user set host = '%' where user = 'root';</span><span class="token attr-name">flush</span> <span class="token attr-value">privileges;</span>quit<span class="token attr-name">mysql</span> <span class="token attr-value">-uroot -p000000;</span></code></pre><h2 id="使用Navicat来进行链接MySQL"><a href="#使用Navicat来进行链接MySQL" class="headerlink" title="使用Navicat来进行链接MySQL"></a>使用Navicat来进行链接MySQL</h2><h2 id="开启binlog日志"><a href="#开启binlog日志" class="headerlink" title="开启binlog日志"></a>开启binlog日志</h2><pre class=" language-properties"><code class="language-properties"><span class="token comment" spellcheck="true"># binlog start</span><span class="token attr-name">server-id</span><span class="token punctuation">=</span><span class="token attr-value">1</span><span class="token attr-name">log-bin</span><span class="token punctuation">=</span><span class="token attr-value">mysql-bin</span><span class="token attr-name">binlog_format</span><span class="token punctuation">=</span><span class="token attr-value">row</span><span class="token attr-name">binlog-do-db</span><span class="token punctuation">=</span><span class="token attr-value">flink</span></code></pre><p>systemctl restart mysqld</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
