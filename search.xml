<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink作业执行深度解析</title>
      <link href="/2021/09/02/Flink%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2021/09/02/Flink%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-作业执行深度解析"><a href="#Flink-作业执行深度解析" class="headerlink" title="Flink 作业执行深度解析"></a>Flink 作业执行深度解析</h1><p><img src="https://img.alicdn.com/imgextra/i3/O1CN0163Pp9Y1fDgFXL506L_!!6000000003973-0-tps-1024-555.jpg" alt="Apache Flink 进阶教程（六）：Flink 作业执行深度解析"></p><p>本文根据 Apache Flink 系列直播课程整理而成，由 Apache Flink Contributor、网易云音乐实时计算平台研发工程师岳猛分享。主要分享内容为 Flink Job 执行作业的流程，文章将从两个方面进行分享：一是如何从 Program 到物理执行计划，二是生成物理执行计划后该如何调度和执行。</p><p><strong>#技术探索</strong><strong>#进阶教程</strong></p><p>本文根据 Apache Flink 系列直播课程整理而成，由 Apache Flink Contributor、网易云音乐实时计算平台研发工程师岳猛分享。主要分享内容为 Flink Job 执行作业的流程，文章将从两个方面进行分享：一是如何从 Program 到物理执行计划，二是生成物理执行计划后该如何调度和执行。</p><h2 id="Flink-四层转化流程"><a href="#Flink-四层转化流程" class="headerlink" title="Flink 四层转化流程"></a>Flink 四层转化流程</h2><p>Flink 有四层转换流程，第一层为 Program 到 StreamGraph；第二层为 StreamGraph 到 JobGraph；第三层为 JobGraph 到 ExecutionGraph；第四层为 ExecutionGraph 到物理执行计划。通过对 Program 的执行，能够生成一个 DAG 执行图，即逻辑执行图。如下：</p><p><img src="https://img.alicdn.com/imgextra/i3/O1CN0163Pp9Y1fDgFXL506L_!!6000000003973-0-tps-1024-555.jpg" alt="null"></p><p>第一部分将先讲解四层转化的流程，然后将以详细案例讲解四层的具体转化。</p><ul><li>第一层 StreamGraph 从 Source 节点开始，每一次 transform 生成一个 StreamNode，两个 StreamNode 通过 StreamEdge 连接在一起,形成 StreamNode 和 StreamEdge 构成的DAG。</li><li>第二层 JobGraph，依旧从 Source 节点开始，然后去遍历寻找能够嵌到一起的 operator，如果能够嵌到一起则嵌到一起，不能嵌到一起的单独生成 jobVertex，通过 JobEdge 链接上下游 JobVertex，最终形成 JobVertex 层面的 DAG。</li><li>JobVertex DAG 提交到任务以后，从 Source 节点开始排序,根据 JobVertex 生成ExecutionJobVertex，根据 jobVertex的IntermediateDataSet 构建IntermediateResult，然后 IntermediateResult 构建上下游的依赖关系，形成 ExecutionJobVertex 层面的 DAG 即 ExecutionGraph。</li><li>最后通过 ExecutionGraph 层到物理执行层。</li></ul><h3 id="Program-到-StreamGraph-的转化"><a href="#Program-到-StreamGraph-的转化" class="headerlink" title="Program 到 StreamGraph 的转化"></a>Program 到 StreamGraph 的转化</h3><p>Program 转换成 StreamGraph 具体分为三步：</p><ul><li>从 StreamExecutionEnvironment.execute 开始执行程序，将 transform 添加到 StreamExecutionEnvironment 的 transformations。</li><li>调用 StreamGraphGenerator 的 generateInternal 方法，遍历 transformations 构建 StreamNode 及 StreamEage。</li><li>通过 StreamEdge 连接 StreamNode。</li></ul><p><img src="https://img.alicdn.com/imgextra/i4/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><p>通过 WindowWordCount 来看代码到 StreamGraph 的转化，在 flatMap transform 设置 slot 共享组为 flatMap_sg，并发设置为 4，在聚合的操作中设置 slot 共享组为 sum_sg， sum() 和 counts() 并发设置为 3，这样设置主要是为了演示后面如何嵌到一起的，跟上下游节点的并发以及上游的共享组有关。</p><p>WindowWordCount 代码中可以看到，在 readTextFile() 中会生成一个 transform，且 transform 的 ID 是 1；然后到 flatMap() 会生成一个 transform， transform 的 ID 是 2；接着到 keyBy() 生成一个 transform 的 ID 是 3；再到 sum() 生成一个 transform 的 ID 是 4；最后到 counts()生成 transform 的 ID 是 5。</p><p><img src="https://img.alicdn.com/imgextra/i1/O1CN01veYa2X1ToWimfPajv_!!6000000002429-2-tps-852-233.png" alt="null"></p><p>transform 的结构如图所示，第一个是 flatMap 的 transform，第二个是 window 的 transform，第三个是 SinkTransform 的 transform。除此之外，还能在 transform 的结构中看到每个 transform 的 input 是什么。</p><p>接下来介绍一下 StreamNode 和 StreamEdge。</p><ul><li>StreamNode 是用来描述 operator 的逻辑节点，其关键成员变量有 slotSharingGroup、jobVertexClass、inEdges、outEdges以及transformationUID；</li><li>StreamEdge 是用来描述两个 operator 逻辑的链接边，其关键变量有 sourceVertex、targetVertex。</li></ul><p><img src="https://img.alicdn.com/imgextra/i4/O1CN0125MujZ24UxaLq6sX5_!!6000000007395-2-tps-671-368.png" alt="null"></p><p>WindowWordCount transform 到 StreamGraph 转化如图所示，StreamExecutionEnvironment 的 transformations 存在 3 个 transform，分别是 Flat Map（Id 2）、Window（Id 4）、Sink（Id 5）。</p><p>transform 的时候首先递归处理 transform 的 input，生成 StreamNode，然后通过 StreamEdge 链接上下游 StreamNode。需要注意的是，有些 transform 操作并不会生成StreamNode 如 PartitionTransformtion，而是生成个虚拟节点。</p><p><img src="https://img.alicdn.com/imgextra/i2/O1CN01u3IClc25Hxxt7SMGC_!!6000000007502-2-tps-858-242.png" alt="null"></p><p>在转换完成后可以看到，streamNodes 有四种 transform 形式，分别为 Source、Flat Map、Window、Sink。</p><p><img src="https://img.alicdn.com/imgextra/i3/O1CN019RQ7Pb1evonklQhvy_!!6000000003934-2-tps-862-352.png" alt="null"></p><p>每个 streamNode 对象都携带并发个数、slotSharingGroup、执行类等运行信息。</p><h3 id="StreamGraph-到-JobGraph-的转化"><a href="#StreamGraph-到-JobGraph-的转化" class="headerlink" title="StreamGraph 到 JobGraph 的转化"></a>StreamGraph 到 JobGraph 的转化</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><p>StreamGraph 到 JobGraph 的转化步骤：</p><ul><li>设置调度模式，Eager 所有节点立即启动。</li><li>广度优先遍历 StreamGraph，为每个 streamNode 生成 byte 数组类型的 hash 值。</li><li>从 source 节点开始递归寻找嵌到一起的 operator，不能嵌到一起的节点单独生成 jobVertex，能够嵌到一起的开始节点生成 jobVertex，其他节点以序列化的形式写入到 StreamConfig，然后 merge 到 CHAINED_TASK_CONFIG，再通过 JobEdge 链接上下游 JobVertex。</li><li>将每个 JobVertex 的入边(StreamEdge)序列化到该 StreamConfig。</li><li>根据 group name 为每个 JobVertext 指定 SlotSharingGroup。</li><li>配置 checkpoint。</li><li>将缓存文件存文件的配置添加到 configuration 中。</li><li>设置 ExecutionConfig。</li></ul><p>从 source 节点递归寻找嵌到一起的 operator 中，嵌到一起需要满足一定的条件，具体条件介绍如下：</p><ul><li>下游节点只有一个输入。</li><li>下游节点的操作符不为 null。</li><li>上游节点的操作符不为 null。</li><li>上下游节点在一个槽位共享组内。</li><li>下游节点的连接策略是 ALWAYS。</li><li>上游节点的连接策略是 HEAD 或者 ALWAYS。</li><li>edge 的分区函数是 ForwardPartitioner 的实例。</li><li>上下游节点的并行度相等。</li><li>可以进行节点连接操作。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><p>JobGraph 对象结构如上图所示，taskVertices 中只存在 Window、Flat Map、Source 三个 TaskVertex，Sink operator 被嵌到 window operator 中去了。</p><h4 id="为什么要为每个-operator-生成-hash-值？"><a href="#为什么要为每个-operator-生成-hash-值？" class="headerlink" title="为什么要为每个 operator 生成 hash 值？"></a>为什么要为每个 operator 生成 hash 值？</h4><p>Flink 任务失败的时候，各个 operator 是能够从 checkpoint 中恢复到失败之前的状态的，恢复的时候是依据 JobVertexID（hash 值)进行状态恢复的。相同的任务在恢复的时候要求 operator 的 hash 值不变，因此能够获取对应的状态。</p><h4 id="每个-operator-是怎样生成-hash-值的？"><a href="#每个-operator-是怎样生成-hash-值的？" class="headerlink" title="每个 operator 是怎样生成 hash 值的？"></a>每个 operator 是怎样生成 hash 值的？</h4><p>如果用户对节点指定了一个散列值，则基于用户指定的值能够产生一个长度为 16 的字节数组。如果用户没有指定，则根据当前节点所处的位置，产生一个散列值。</p><p>考虑的因素主要有三点：</p><ul><li>一是在当前 StreamNode 之前已经处理过的节点的个数，作为当前 StreamNode 的 id，添加到 hasher 中；</li><li>二是遍历当前 StreamNode 输出的每个 StreamEdge，并判断当前 StreamNode 与这个 StreamEdge 的目标 StreamNode 是否可以进行链接，如果可以，则将目标 StreamNode 的 id 也放入 hasher 中，且这个目标 StreamNode 的 id 与当前 StreamNode 的 id 取相同的值；</li><li>三是将上述步骤后产生的字节数据，与当前 StreamNode 的所有输入 StreamNode 对应的字节数据，进行相应的位操作，最终得到的字节数据，就是当前 StreamNode 对应的长度为 16 的字节数组。</li></ul><h3 id="JobGraph-到-ExexcutionGraph-以及物理执行计划"><a href="#JobGraph-到-ExexcutionGraph-以及物理执行计划" class="headerlink" title="JobGraph 到 ExexcutionGraph 以及物理执行计划"></a>JobGraph 到 ExexcutionGraph 以及物理执行计划</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><p>JobGraph 到 ExexcutionGraph 以及物理执行计划的流程：</p><ul><li>将 JobGraph 里面的 jobVertex 从 Source 节点开始排序。</li><li>在 executionGraph.attachJobGraph(sortedTopology)方法里面，根据 JobVertex 生成 ExecutionJobVertex，在 ExecutionJobVertex 构造方法里面，根据 jobVertex 的 IntermediateDataSet 构建 IntermediateResult，根据 jobVertex 并发构建 ExecutionVertex，ExecutionVertex 构建的时候，构建 IntermediateResultPartition（每一个 Execution 构建 IntermediateResult 数个IntermediateResultPartition ）；将创建的 ExecutionJobVertex 与前置的 IntermediateResult 连接起来。</li><li>构建 ExecutionEdge ，连接到前面的 IntermediateResultPartition，最终从 ExecutionGraph 到物理执行计划。</li></ul><h2 id="Flink-Job-执行流程"><a href="#Flink-Job-执行流程" class="headerlink" title="Flink Job 执行流程"></a>Flink Job 执行流程</h2><h3 id="Flink-On-Yarn-模式"><a href="#Flink-On-Yarn-模式" class="headerlink" title="Flink On Yarn 模式"></a>Flink On Yarn 模式</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><p>基于 Yarn 层面的架构类似 Spark on Yarn 模式，都是由 Client 提交 App 到 RM 上面去运行，然后 RM 分配第一个 container 去运行 AM，然后由 AM 去负责资源的监督和管理。需要说明的是，Flink 的 Yarn 模式更加类似 Spark on Yarn 的 cluster 模式，在 cluster 模式中，dirver 将作为 AM 中的一个线程去运行。Flink on Yarn 模式也是会将 JobManager 启动在 container 里面，去做个 driver 类似的任务调度和分配，Yarn AM 与 Flink JobManager 在同一个 Container 中，这样 AM 可以知道 Flink JobManager 的地址，从而 AM 可以申请 Container 去启动 Flink TaskManager。待 Flink 成功运行在 Yarn 集群上，Flink Yarn Client 就可以提交 Flink Job 到 Flink JobManager，并进行后续的映射、调度和计算处理。</p><h4 id="Fink-on-Yarn-的缺陷"><a href="#Fink-on-Yarn-的缺陷" class="headerlink" title="Fink on Yarn 的缺陷"></a>Fink on Yarn 的缺陷</h4><ul><li>资源分配是静态的，一个作业需要在启动时获取所需的资源并且在它的生命周期里一直持有这些资源。这导致了作业不能随负载变化而动态调整，在负载下降时无法归还空闲的资源，在负载上升时也无法动态扩展。</li><li>On-Yarn 模式下，所有的 container 都是固定大小的，导致无法根据作业需求来调整 container 的结构。譬如 CPU 密集的作业或许需要更多的核，但不需要太多内存，固定结构的 container 会导致内存被浪费。</li><li>与容器管理基础设施的交互比较笨拙，需要两个步骤来启动 Flink 作业: 1.启动 Flink 守护进程；2.提交作业。如果作业被容器化并且将作业部署作为容器部署的一部分，那么将不再需要步骤2。</li><li>On-Yarn 模式下，作业管理页面会在作业完成后消失不可访问。</li><li>Flink 推荐 <code>**per job clusters**</code> 的部署方式，但是又支持可以在一个集群上运行多个作业的 session 模式，令人疑惑。</li></ul><p>在 Flink 版本 1.5 中引入了 Dispatcher，Dispatcher 是在新设计里引入的一个新概念。Dispatcher 会从 Client 端接受作业提交请求并代表它在集群管理器上启动作业。</p><h4 id="引入-Dispatcher-的原因主要有两点"><a href="#引入-Dispatcher-的原因主要有两点" class="headerlink" title="引入 Dispatcher 的原因主要有两点:"></a>引入 Dispatcher 的原因主要有两点:</h4><ul><li>第一，一些集群管理器需要一个中心化的作业生成和监控实例；</li><li>第二，能够实现 Standalone 模式下 JobManager 的角色，且等待作业提交。在一些案例中，Dispatcher 是可选的(Yarn)或者不兼容的(kubernetes)。</li></ul><h3 id="资源调度模型重构下的-Flink-On-Yarn-模式"><a href="#资源调度模型重构下的-Flink-On-Yarn-模式" class="headerlink" title="资源调度模型重构下的 Flink On Yarn 模式"></a>资源调度模型重构下的 Flink On Yarn 模式</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><h4 id="没有-Dispatcher-job-运行过程"><a href="#没有-Dispatcher-job-运行过程" class="headerlink" title="没有 Dispatcher job 运行过程"></a>没有 Dispatcher job 运行过程</h4><p>客户端提交 JobGraph 以及依赖 jar 包到 YarnResourceManager，接着 Yarn ResourceManager 分配第一个 container 以此来启动 AppMaster，Application Master 中会启动一个 FlinkResourceManager 以及 JobManager，JobManager 会根据 JobGraph 生成的 ExecutionGraph 以及物理执行计划向 FlinkResourceManager 申请 slot，FlinkResoourceManager 会管理这些 slot 以及请求，如果没有可用 slot 就向 Yarn 的 ResourceManager 申请 container，container 启动以后会注册到 FlinkResourceManager，最后 JobManager 会将 subTask deploy 到对应 container 的 slot 中去。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01xTMqBf1RCOVzoUEe5_!!6000000002075-2-tps-742-414.png" alt="null"></p><h4 id="在有-Dispatcher-的模式下"><a href="#在有-Dispatcher-的模式下" class="headerlink" title="在有 Dispatcher 的模式下"></a>在有 Dispatcher 的模式下</h4><p>会增加一个过程，就是 Client 会直接通过 HTTP Server 的方式，然后用 Dispatcher 将这个任务提交到 Yarn ResourceManager 中。</p><p>新框架具有四大优势，详情如下：</p><ul><li>client 直接在 Yarn 上启动作业，而不需要先启动一个集群然后再提交作业到集群。因此 client 再提交作业后可以马上返回。</li><li>所有的用户依赖库和配置文件都被直接放在应用的 classpath，而不是用动态的用户代码 classloader 去加载。</li><li>container 在需要时才请求，不再使用时会被释放。</li><li>“需要时申请”的 container 分配方式允许不同算子使用不同 profile (CPU 和内存结构)的 container。</li></ul><h3 id="新的资源调度框架下-single-cluster-job-on-Yarn-流程介绍"><a href="#新的资源调度框架下-single-cluster-job-on-Yarn-流程介绍" class="headerlink" title="新的资源调度框架下 single cluster job on Yarn 流程介绍"></a>新的资源调度框架下 single cluster job on Yarn 流程介绍</h3><p><img src="https://img.alicdn.com/imgextra/i1/O1CN01ihoFhY23zoeSGeWCE_!!6000000007327-2-tps-858-363.png" alt="null"></p><p>single cluster job on Yarn 模式涉及三个实例对象：</p><ul><li>clifrontend<ul><li>Invoke App code；</li><li>生成 StreamGraph，然后转化为 JobGraph；</li></ul></li><li>YarnJobClusterEntrypoint（Master）<ul><li>依次启动 YarnResourceManager、MinDispatcher、JobManagerRunner 三者都服从分布式协同一致的策略；</li><li>JobManagerRunner 将 JobGraph 转化为 ExecutionGraph ，然后转化为物理执行任务Execution，然后进行 deploy，deploy 过程会向 YarnResourceManager 请求 slot，如果有直接 deploy 到对应的 YarnTaskExecutiontor 的 slot 里面，没有则向 Yarn 的 ResourceManager 申请，带 container 启动以后 deploy。</li></ul></li><li>YarnTaskExecutorRunner (slave)<ul><li>负责接收 subTask，并运行。</li></ul></li></ul><p>整个任务运行代码调用流程如下图：</p><p><img src="https://img.alicdn.com/imgextra/i2/O1CN01rRr3lX1qE6O4LEnDI_!!6000000005463-2-tps-863-475.png" alt="null"></p><h3 id="subTask-在执行时是怎么运行的？"><a href="#subTask-在执行时是怎么运行的？" class="headerlink" title="subTask 在执行时是怎么运行的？"></a>subTask 在执行时是怎么运行的？</h3><pre class=" language-java"><code class="language-java">调用 StreamTask 的 invoke 方法，执行步骤如下：<span class="token operator">*</span> <span class="token function">initializeState</span><span class="token punctuation">(</span><span class="token punctuation">)</span>即operator的<span class="token function">initializeState</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span> <span class="token function">openAllOperators</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 即operator的<span class="token function">open</span><span class="token punctuation">(</span><span class="token punctuation">)</span>方法<span class="token operator">*</span> 最后调用 run 方法来进行真正的任务处理</code></pre><p>我们来看下 flatMap 对应的 OneInputStreamTask 的 run 方法具体是怎么处理的。</p><pre class=" language-java"><code class="language-java">    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// cache processor reference on the stack, to make the code more JIT friendly</span>        <span class="token keyword">final</span> StreamInputProcessor<span class="token operator">&lt;</span>IN<span class="token operator">></span> inputProcessor <span class="token operator">=</span> <span class="token keyword">this</span><span class="token punctuation">.</span>inputProcessor<span class="token punctuation">;</span>        <span class="token keyword">while</span> <span class="token punctuation">(</span>running <span class="token operator">&amp;&amp;</span> inputProcessor<span class="token punctuation">.</span><span class="token function">processInput</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// all the work happens in the "processInput" method</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span></code></pre><p>最终是调用 StreamInputProcessor 的 processInput() 做数据的处理，这里面包含用户的处理逻辑。</p><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">processInput</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>isFinished<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>numRecordsIn <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">try</span> <span class="token punctuation">{</span>                numRecordsIn <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>OperatorMetricGroup<span class="token punctuation">)</span> streamOperator<span class="token punctuation">.</span><span class="token function">getMetricGroup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getIOMetricGroup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getNumRecordsInCounter</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                LOG<span class="token punctuation">.</span><span class="token function">warn</span><span class="token punctuation">(</span><span class="token string">"An exception occurred during the metrics setup."</span><span class="token punctuation">,</span> e<span class="token punctuation">)</span><span class="token punctuation">;</span>                numRecordsIn <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SimpleCounter</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>currentRecordDeserializer <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>                DeserializationResult result <span class="token operator">=</span> currentRecordDeserializer<span class="token punctuation">.</span><span class="token function">getNextRecord</span><span class="token punctuation">(</span>deserializationDelegate<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>result<span class="token punctuation">.</span><span class="token function">isBufferConsumed</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    currentRecordDeserializer<span class="token punctuation">.</span><span class="token function">getCurrentBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">recycleBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    currentRecordDeserializer <span class="token operator">=</span> null<span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>result<span class="token punctuation">.</span><span class="token function">isFullRecord</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    StreamElement recordOrMark <span class="token operator">=</span> deserializationDelegate<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">//处理watermark</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">isWatermark</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token comment" spellcheck="true">// handle watermark</span>                        <span class="token comment" spellcheck="true">//watermark处理逻辑，这里可能引起timer的trigger</span>                        statusWatermarkValve<span class="token punctuation">.</span><span class="token function">inputWatermark</span><span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">asWatermark</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> currentChannel<span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token keyword">continue</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">isStreamStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token comment" spellcheck="true">// handle stream status</span>                        statusWatermarkValve<span class="token punctuation">.</span><span class="token function">inputStreamStatus</span><span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">asStreamStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> currentChannel<span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token keyword">continue</span><span class="token punctuation">;</span>                        <span class="token comment" spellcheck="true">//处理latency watermark</span>                    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">isLatencyMarker</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token comment" spellcheck="true">// handle latency marker</span>                        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>lock<span class="token punctuation">)</span> <span class="token punctuation">{</span>                            streamOperator<span class="token punctuation">.</span><span class="token function">processLatencyMarker</span><span class="token punctuation">(</span>recordOrMark<span class="token punctuation">.</span><span class="token function">asLatencyMarker</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token punctuation">}</span>                        <span class="token keyword">continue</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                        <span class="token comment" spellcheck="true">//用户的真正的代码逻辑</span>                        <span class="token comment" spellcheck="true">// now we can do the actual processing</span>                        StreamRecord<span class="token operator">&lt;</span>IN<span class="token operator">></span> record <span class="token operator">=</span> recordOrMark<span class="token punctuation">.</span><span class="token function">asRecord</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>lock<span class="token punctuation">)</span> <span class="token punctuation">{</span>                            numRecordsIn<span class="token punctuation">.</span><span class="token function">inc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            streamOperator<span class="token punctuation">.</span><span class="token function">setKeyContextElement1</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span><span class="token punctuation">;</span>                            <span class="token comment" spellcheck="true">//处理数据</span>                            streamOperator<span class="token punctuation">.</span><span class="token function">processElement</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token punctuation">}</span>                        <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">//这里会进行checkpoint barrier的判断和对齐，以及不同partition 里面checkpoint barrier不一致时候的，数据buffer，</span>            <span class="token keyword">final</span> BufferOrEvent bufferOrEvent <span class="token operator">=</span> barrierHandler<span class="token punctuation">.</span><span class="token function">getNextNonBlocked</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>bufferOrEvent <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>bufferOrEvent<span class="token punctuation">.</span><span class="token function">isBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    currentChannel <span class="token operator">=</span> bufferOrEvent<span class="token punctuation">.</span><span class="token function">getChannelIndex</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    currentRecordDeserializer <span class="token operator">=</span> recordDeserializers<span class="token punctuation">[</span>currentChannel<span class="token punctuation">]</span><span class="token punctuation">;</span>                    currentRecordDeserializer<span class="token punctuation">.</span><span class="token function">setNextBuffer</span><span class="token punctuation">(</span>bufferOrEvent<span class="token punctuation">.</span><span class="token function">getBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token keyword">else</span> <span class="token punctuation">{</span>                    <span class="token comment" spellcheck="true">// Event received</span>                    <span class="token keyword">final</span> AbstractEvent event <span class="token operator">=</span> bufferOrEvent<span class="token punctuation">.</span><span class="token function">getEvent</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> EndOfPartitionEvent<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token keyword">throw</span> <span class="token keyword">new</span> <span class="token class-name">IOException</span><span class="token punctuation">(</span><span class="token string">"Unexpected event: "</span> <span class="token operator">+</span> event<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            <span class="token keyword">else</span> <span class="token punctuation">{</span>                isFinished <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>barrierHandler<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token keyword">throw</span> <span class="token keyword">new</span> <span class="token class-name">IllegalStateException</span><span class="token punctuation">(</span><span class="token string">"Trailing data in checkpoint barrier handler."</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span></code></pre><p>streamOperator.processElement(record) 最终会调用用户的代码处理逻辑，假如 operator 是 StreamFlatMap 的话，</p><pre class=" language-java"><code class="language-java">    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">processElement</span><span class="token punctuation">(</span>StreamRecord<span class="token operator">&lt;</span>IN<span class="token operator">></span> element<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        collector<span class="token punctuation">.</span><span class="token function">setTimestamp</span><span class="token punctuation">(</span>element<span class="token punctuation">)</span><span class="token punctuation">;</span>        userFunction<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>element<span class="token punctuation">.</span><span class="token function">getValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> collector<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//用户代码</span>    <span class="token punctuation">}</span></code></pre><p>如有不正确的地方，欢迎指正，<a href="http://www.whitewood.me/2018/06/17/FLIP6-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E9%87%8D%E6%9E%84/" target="_blank" rel="noopener">关于 Flink 资源调度架构调整，网上有一篇非常不错的针对 FLIP-6 的翻译</a>，推荐给大家。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink实时数仓面试题总结</title>
      <link href="/2021/09/02/Flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/"/>
      <url>/2021/09/02/Flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink实时数仓面试题总结"><a href="#Flink实时数仓面试题总结" class="headerlink" title="Flink实时数仓面试题总结"></a>Flink实时数仓面试题总结</h1><h2 id="Flink用什么监控"><a href="#Flink用什么监控" class="headerlink" title="Flink用什么监控"></a>Flink用什么监控</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">Webui</span> <span class="token attr-value">+ prometheus + grafana</span>flink自带了prometheus监控<span class="token attr-name">flink</span> <span class="token attr-value">on yarn节点不固定，如何监控，我们走的是pushgateway</span>监控的指标有那些？job重启、失败、网络延迟、jobmanager，JVM、内存、IO背压情况：背压和反压是一样的checkpointrocksdbkafka<span class="token attr-name">kafka</span><span class="token punctuation">:</span> <span class="token attr-value">source / sink</span>kafka的偏移量都是保存在状态里面PromQL专有的api语句来设置的告警：工作群开发运维根据具体来指定接口根据公司具体情况来进行定</code></pre><h2 id="监控启停脚本"><a href="#监控启停脚本" class="headerlink" title="监控启停脚本"></a>监控启停脚本</h2><pre class=" language-properties"><code class="language-properties">promethusgrafnapushgateway<span class="token attr-name">Node</span> <span class="token attr-value">Explorter</span>杀死进程：<span class="token attr-name">kill</span> <span class="token attr-value">得知道进程号</span><span class="token attr-name">ps</span> <span class="token attr-value">-ef | grep info | awk '{print \$2}' | xargs kill</span><span class="token attr-name">pgrep</span> <span class="token attr-value">-f info | xargs kill</span>不要为难自己了。。。</code></pre><p>做开发的话使用脚本来简化开发时一个高级工程师应该有的手法</p><p>案例</p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    xxxxxxx<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span><span class="token string">"stop"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    xxxxxxx<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>esac</code></pre><h2 id="安装promethus的问题"><a href="#安装promethus的问题" class="headerlink" title="安装promethus的问题"></a>安装promethus的问题</h2><pre class=" language-properties"><code class="language-properties">高级工程师的精髓没有学到，文档里面的缩进不是白加的，yaml文件的格式是非常重要的yml是同级配置，所以这个是非常重要，成熟的开发工程师</code></pre><h2 id="Maven打包插件"><a href="#Maven打包插件" class="headerlink" title="Maven打包插件"></a>Maven打包插件</h2><pre class=" language-properties"><code class="language-properties">maven-assemblyjar-with不要用上面的这个插件，实在是太low，不够灵活，有问题<span class="token attr-name">Flink</span> <span class="token attr-value">CDC KafkaToKafka</span>这样会报JDBC工程类找不到，底层会打包一个相同名字的包</code></pre><p>以前使用插件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><p>现在使用的插件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactSet</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>com.google.code.findbugs:jsr305<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org.slf4j:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>log4j:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org.apache.hadoop:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactSet</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filters</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filter</span><span class="token punctuation">></span></span>                                <span class="token comment" spellcheck="true">&lt;!-- Do not copy the signatures in the META-INF folder.Otherwise, this might cause SecurityExceptions when using the JAR. --></span>                                <span class="token comment" spellcheck="true">&lt;!-- 打包时不复制META-INF下的签名文件，避免报非法签名文件的SecurityExceptions异常--></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifact</span><span class="token punctuation">></span></span>*:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifact</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>                                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.SF<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.DSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.RSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filter</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filters</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>transformers</span> <span class="token attr-name">combine.children</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>append<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                            <span class="token comment" spellcheck="true">&lt;!-- The service transformer is needed to merge META-INF/services files --></span>                            <span class="token comment" spellcheck="true">&lt;!-- connector和format依赖的工厂类打包时会相互覆盖，需要使用ServicesResourceTransformer解决--></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>transformer</span> <span class="token attr-name">implementation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.apache.maven.plugins.shade.resource.ServicesResourceTransformer<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>transformers</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><p>实时数仓依赖，phoenix-spark，打包不打hadoop的依赖</p><p>正常时provide的</p><p>maven依赖有传递性，如果不打包hadoop，hbase依赖会出现问题的</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.SF<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.DSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.RSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org.apache.hadoop:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span></code></pre><p>添加exclude就好了</p><h2 id="如何查找报错"><a href="#如何查找报错" class="headerlink" title="如何查找报错"></a>如何查找报错</h2><p>从最后往前看</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210902094405803.png" alt="案例讲解"></p><p>Ctrl + N 查找类的方法</p><p>Ctrl + H 找实现</p><p>如果出现找不到的情况下，说明我们打包是有问题的，如果底层使用相同的，就会自打包一个，所以拒绝使用maven-assembly</p><h2 id="打包插件-hive-exec的影响"><a href="#打包插件-hive-exec的影响" class="headerlink" title="打包插件-hive-exec的影响"></a>打包插件-hive-exec的影响</h2><p>1、classNotFond 没有依赖</p><p>2、ClassNotSuch 依赖冲突</p><p>都会排查java编译器的问题，janinoCompilerFactory</p><p>hive-exec神坑</p><p>Flink和hive集成，出现问题，hive-exec的问题，</p><p>Dependency Analyzer依赖分析器</p><p>工作当中会经常遇到这种错误，掌握这种错误的排查是非常的重要的，所以插件的选择是非常重要的</p><h2 id="maven-helper插件"><a href="#maven-helper插件" class="headerlink" title="maven helper插件"></a>maven helper插件</h2><p>直接安装maven helper就行，重启idea就好了</p><p>pom文件下面有Dependency Analyzer</p><h2 id="远程debug"><a href="#远程debug" class="headerlink" title="远程debug"></a>远程debug</h2><p>生成环境基于yarn，Idea集群(mincluster)和flink集群是不一样的</p>]]></content>
      
      
      <categories>
          
          <category> 企业级数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 企业级数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink实时处理之构造者模式</title>
      <link href="/2021/09/01/Flink%E5%AE%9E%E6%97%B6%E5%A4%84%E7%90%86%E4%B9%8B%E6%9E%84%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"/>
      <url>/2021/09/01/Flink%E5%AE%9E%E6%97%B6%E5%A4%84%E7%90%86%E4%B9%8B%E6%9E%84%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink实时处理之构造者模式"><a href="#Flink实时处理之构造者模式" class="headerlink" title="Flink实时处理之构造者模式"></a>Flink实时处理之构造者模式</h1><p>我们在工作中写代码会遇到一种情况，就是设置一个对象属性值，通常方式有两种</p><pre class=" language-java"><code class="language-java"><span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">User</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> fields<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Integer<span class="token punctuation">.</span><span class="token function">parseInt</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fields<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> User<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">name</span><span class="token punctuation">(</span><span class="token string">"abc"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong>第一种方式:</strong><br>  相当于在构造函数里传递参数，但这样加入参数的时候，不能明确的知道往这个对象里加入了什么属性的内容。</p><p><strong>第二种方式:</strong><br>  虽然可以根据set函数名看到将要设置的值是什么值，但是这种写发，略显冗余。</p><p>这个在实时数仓的场景中用到特别多，设计到大量的对象和表结构</p><p><strong>在设计模式中有构造者模式(builder),在类的构造器或静态工厂具有多个参数。设计这种类时，builder模式就是个不错的选择。</strong></p><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>design<span class="token punctuation">;</span><span class="token keyword">import</span> lombok<span class="token punctuation">.</span>Builder<span class="token punctuation">;</span><span class="token keyword">import</span> lombok<span class="token punctuation">.</span>Data<span class="token punctuation">;</span><span class="token keyword">import</span> lombok<span class="token punctuation">.</span>ToString<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>connector<span class="token punctuation">.</span>jdbc<span class="token punctuation">.</span>JdbcConnectionOptions<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>connector<span class="token punctuation">.</span>jdbc<span class="token punctuation">.</span>JdbcExecutionOptions<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>connector<span class="token punctuation">.</span>jdbc<span class="token punctuation">.</span>JdbcSink<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>connector<span class="token punctuation">.</span>jdbc<span class="token punctuation">.</span>JdbcStatementBuilder<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>DataStreamSource<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>SingleOutputStreamOperator<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>environment<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span>SinkFunction<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>annotation<span class="token punctuation">.</span>ElementType<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>annotation<span class="token punctuation">.</span>Retention<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>annotation<span class="token punctuation">.</span>RetentionPolicy<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>annotation<span class="token punctuation">.</span>Target<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>reflect<span class="token punctuation">.</span>Field<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>PreparedStatement<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>SQLException<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.design * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: 构造者设计模式 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ConstructorDesign</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        DataStreamSource<span class="token operator">&lt;</span>String<span class="token operator">></span> socketTextStream <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"192.168.21.113"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        SingleOutputStreamOperator<span class="token operator">&lt;</span>User<span class="token operator">></span> streamOperator <span class="token operator">=</span> socketTextStream<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>            String<span class="token punctuation">[</span><span class="token punctuation">]</span> fields <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// 测试编写的效果</span>            <span class="token comment" spellcheck="true">//return new User(fields[0], fields[1], Integer.parseInt(fields[2]), fields[3]);</span>            <span class="token keyword">return</span> User                    <span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                    <span class="token punctuation">.</span><span class="token function">name</span><span class="token punctuation">(</span><span class="token string">"abc"</span><span class="token punctuation">)</span>                    <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        streamOperator<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        streamOperator<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span>ClickHouseUtil<span class="token punctuation">.</span>&lt;User<span class="token operator">></span><span class="token function">getSinkFunc</span><span class="token punctuation">(</span><span class="token string">"insert into user values(?,?,?)"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        env<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">ClickHouseUtil</span> <span class="token punctuation">{</span>        <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token operator">&lt;</span>T<span class="token operator">></span> SinkFunction<span class="token operator">&lt;</span>T<span class="token operator">></span> <span class="token function">getSinkFunc</span><span class="token punctuation">(</span>String sql<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> JdbcSink<span class="token punctuation">.</span><span class="token function">sink</span><span class="token punctuation">(</span>sql<span class="token punctuation">,</span>                    <span class="token keyword">new</span> <span class="token class-name">JdbcStatementBuilder</span><span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token annotation punctuation">@Override</span>                        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">accept</span><span class="token punctuation">(</span>PreparedStatement preparedStatement<span class="token punctuation">,</span> T t<span class="token punctuation">)</span> <span class="token keyword">throws</span> SQLException <span class="token punctuation">{</span>                            <span class="token comment" spellcheck="true">//通过反射的方式获取数据中的内容</span>                            <span class="token comment" spellcheck="true">//1.获取所有的列信息(包含私有属性)</span>                            Field<span class="token punctuation">[</span><span class="token punctuation">]</span> declaredFields <span class="token operator">=</span> t<span class="token punctuation">.</span><span class="token function">getClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getDeclaredFields</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            <span class="token comment" spellcheck="true">//2.遍历列信息</span>                            <span class="token keyword">int</span> offset <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> declaredFields<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                                Field field <span class="token operator">=</span> declaredFields<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                                <span class="token comment" spellcheck="true">//设置私有属性信息可获取</span>                                field<span class="token punctuation">.</span><span class="token function">setAccessible</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                <span class="token comment" spellcheck="true">//获取字段上的注解信息</span>                                TransientSink transientSink <span class="token operator">=</span> field<span class="token punctuation">.</span><span class="token function">getAnnotation</span><span class="token punctuation">(</span>TransientSink<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                <span class="token keyword">if</span> <span class="token punctuation">(</span>transientSink <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>                                    offset<span class="token operator">++</span><span class="token punctuation">;</span>                                    <span class="token keyword">continue</span><span class="token punctuation">;</span>                                <span class="token punctuation">}</span>                                <span class="token keyword">try</span> <span class="token punctuation">{</span>                                    Object o <span class="token operator">=</span> field<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">;</span>                                    preparedStatement<span class="token punctuation">.</span><span class="token function">setObject</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">-</span> offset<span class="token punctuation">,</span> o<span class="token punctuation">)</span><span class="token punctuation">;</span>                                <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">IllegalAccessException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                                    e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                <span class="token punctuation">}</span>                            <span class="token punctuation">}</span>                        <span class="token punctuation">}</span>                    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">JdbcExecutionOptions</span>                            <span class="token punctuation">.</span><span class="token function">Builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                            <span class="token punctuation">.</span><span class="token function">withBatchSize</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                            <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    <span class="token keyword">new</span> <span class="token class-name">JdbcConnectionOptions<span class="token punctuation">.</span>JdbcConnectionOptionsBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                            <span class="token punctuation">.</span><span class="token function">withDriverName</span><span class="token punctuation">(</span><span class="token string">"ru.yandex.clickhouse.ClickHouseDriver"</span><span class="token punctuation">)</span>                            <span class="token punctuation">.</span><span class="token function">withUrl</span><span class="token punctuation">(</span><span class="token string">"jdbc:clickhouse://192.168.21.113:8123/clickhouse"</span><span class="token punctuation">)</span>                            <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Data</span>    <span class="token annotation punctuation">@Builder</span>    <span class="token annotation punctuation">@ToString</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">User</span> <span class="token punctuation">{</span>        <span class="token annotation punctuation">@Builder</span><span class="token punctuation">.</span>Default        <span class="token annotation punctuation">@TransientSink</span>        <span class="token keyword">private</span> String id <span class="token operator">=</span> <span class="token string">"1001"</span><span class="token punctuation">;</span>        <span class="token keyword">private</span> String name<span class="token punctuation">;</span>        <span class="token annotation punctuation">@Builder</span><span class="token punctuation">.</span>Default        <span class="token keyword">private</span> <span class="token keyword">int</span> age <span class="token operator">=</span> <span class="token number">22</span><span class="token punctuation">;</span>        <span class="token annotation punctuation">@Builder</span><span class="token punctuation">.</span>Default        <span class="token keyword">private</span> String area <span class="token operator">=</span> <span class="token string">"中国"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Target</span><span class="token punctuation">(</span>ElementType<span class="token punctuation">.</span>FIELD<span class="token punctuation">)</span>    <span class="token annotation punctuation">@Retention</span><span class="token punctuation">(</span>RetentionPolicy<span class="token punctuation">.</span>RUNTIME<span class="token punctuation">)</span>    <span class="token keyword">public</span> @<span class="token keyword">interface</span> <span class="token class-name">TransientSink</span> <span class="token punctuation">{</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210901154329140.png" alt="最终的测试效果"></p><p>通过最终的测试效果是可以能看出来这个方法是可以实现的，每天对技术增长一点</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink-Time-深度解析</title>
      <link href="/2021/09/01/Flink-Time-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2021/09/01/Flink-Time-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-Time-深度解析"><a href="#Flink-Time-深度解析" class="headerlink" title="Flink-Time-深度解析"></a>Flink-Time-深度解析</h1><p><img src="https://img.alicdn.com/imgextra/i4/O1CN01aziMSv1d79N0S78bZ_!!6000000003688-2-tps-1024-449.png" alt="Apache Flink 进阶教程（二）：Time 深度解析"></p><p>作者： </p><p><a href="https://flink-learning.org.cn/author/bc2eed97790dd4ba5f8b584f8666b24b" target="_blank" rel="noopener">崔星灿</a></p><p>整理： 沙晟阳（成阳）</p><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><strong>#技术探索</strong><strong>#进阶教程</strong></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><img src="https://img.alicdn.com/imgextra/i4/O1CN01aziMSv1d79N0S78bZ_!!6000000003688-2-tps-1024-449.png" alt="null"></p><h2 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h2><p>在不同的应用场景中时间语义是各不相同的，Flink 作为一个先进的分布式流处理引擎，它本身支持不同的时间语义。其核心是 Processing Time 和 Event Time（Row Time），这两类时间主要的不同点如下表所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN013TSsxW1FnhNy4JH84_!!6000000000532-2-tps-1024-415.png" alt="null"></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time 则是根据每一条处理记录所携带的时间戳来判定。</p><p>这两种时间在 Flink 内部的处理以及还是用户的实际使用方面，难易程度都是不同的。相对而言的 Processing Time 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用 Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，使用 Event Time 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同。如果希望结果完全相同，就只能用 Event Time；如果接受结果不同，则可以用 Processing Time。Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用 Processing Time。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01GwFwI42431QPaOx8t_!!6000000007334-2-tps-1024-499.png" alt="null"></p><h3 id="时间的特性"><a href="#时间的特性" class="headerlink" title="时间的特性"></a>时间的特性</h3><p><strong>时间的一个重要特性是：时间只能递增，不会来回穿越。</strong> 在使用时间的时候我们要充分利用这个特性。假设我们有这么一些记录，然后我们来分别看一下 Processing Time 还有 Event Time 对于时间的处理。</p><ul><li>对于 Processing Time，因为我们是使用的是本地节点的时间（假设这个节点的时钟同步没有问题），我们每一次取到的 Processing Time 肯定都是递增的，递增就代表着有序，所以说我们相当于拿到的是一个有序的数据流。</li><li>而在用 Event Time 的时候因为时间是绑定在每一条的记录上的，由于网络延迟、程序内部逻辑、或者其他一些分布式系统的原因，数据的时间可能会存在一定程度的乱序，比如上图的例子。在 Event Time 场景下，我们把每一个记录所包含的时间称作 Record Timestamp。如果 Record Timestamp 所得到的时间序列存在乱序，我们就需要去处理这种情况。</li></ul><p><img src="https://img.alicdn.com/imgextra/i2/O1CN01fvK7JG1UIISqyj8b7_!!6000000002494-2-tps-687-267.png" alt="null"></p><p>如果单条数据之间是乱序，我们就考虑对于整个序列进行更大程度的离散化。简单地讲，就是把数据按照一定的条数组成一些小批次，但这里的小批次并不是攒够多少条就要去处理，而是为了对他们进行时间上的划分。经过这种更高层次的离散化之后，我们会发现最右边方框里的时间就是一定会小于中间方框里的时间，中间框里的时间也一定会小于最左边方框里的时间。</p><p><img src="https://img.alicdn.com/imgextra/i4/O1CN01fConOF1zS94YRq8jB_!!6000000006712-2-tps-714-308.png" alt="null"></p><p>这个时候我们在整个时间序列里插入一些类似于标志位的一些特殊的处理数据，这些特殊的处理数据叫做 watermark。一个 watermark 本质上就代表了这个 watermark 所包含的 timestamp 数值，表示以后到来的数据已经再也没有小于或等于这个时间的了。</p><h2 id="Timestamp-和-Watermark-行为概览"><a href="#Timestamp-和-Watermark-行为概览" class="headerlink" title="Timestamp 和 Watermark 行为概览"></a>Timestamp 和 Watermark 行为概览</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01uP4bwE27gogW6lYOK_!!6000000007827-2-tps-1024-442.png" alt="null"></p><p>接下来我们重点看一下 Event Time 里的 Record Timestamp（简写成 timestamp）和 watermark 的一些基本信息。绝大多数的分布式流计算引擎对于数据都是进行了 DAG 图的抽象，它有自己的数据源，有处理算子，还有一些数据汇。数据在不同的逻辑算子之间进行流动。watermark 和 timestamp 有自己的生命周期，接下来我会从 watermark 和 timestamp 的产生、他们在不同的节点之间的传播、以及在每一个节点上的处理，这三个方面来展开介绍。</p><h3 id="Timestamp-分配和-Watermark-生成"><a href="#Timestamp-分配和-Watermark-生成" class="headerlink" title="Timestamp 分配和 Watermark 生成"></a>Timestamp 分配和 Watermark 生成</h3><p>Flink 支持两种 watermark 生成方式。第一种是在 SourceFunction 中产生，相当于把整个的 timestamp 分配和 watermark 生成的逻辑放在流处理应用的源头。我们可以在 SourceFunction 里面通过这两个方法产生 watermark：</p><ul><li>通过 collectWithTimestamp 方法发送一条数据，其中第一个参数就是我们要发送的数据，第二个参数就是这个数据所对应的时间戳；也可以调用 emitWatermark 方法去产生一条 watermark，表示接下来不会再有时间戳小于等于这个数值记录。</li><li>另外，有时候我们不想在 SourceFunction 里生成 timestamp 或者 watermark，或者说使用的 SourceFunction 本身不支持，我们还可以在使用 DataStream API 的时候指定，调用的 DataStream.assignTimestampsAndWatermarks 这个方法，能够接收不同的 timestamp 和 watermark 的生成器。</li></ul><p>总体上而言生成器可以分为两类：第一类是定期生成器；第二类是根据一些在流处理数据流中遇到的一些特殊记录生成的。</p><p><img src="https://img.alicdn.com/imgextra/i2/O1CN01Ep6jwp1bksUdGBpFU_!!6000000003504-2-tps-958-179.png" alt="null"></p><p>两者的区别主要有三个方面，首先定期生成是现实时间驱动的，这里的“定期生成”主要是指 watermark（因为 timestamp 是每一条数据都需要有的），即定期会调用生成逻辑去产生一个 watermark。而根据特殊记录生成是数据驱动的，即是否生成 watermark 不是由现实时间来决定，而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了，这个时候相当于每一次分配 Timestamp 之后都会调用用户实现的 watermark 生成方法，用户需要在生成方法中去实现 watermark 的生成逻辑。</p><p>大家要注意的是就是我们在分配 timestamp 和生成 watermark 的过程，虽然在 SourceFunction 和 DataStream 中都可以指定，但是还是建议生成的工作越靠近 DataSource 越好。这样会方便让程序逻辑里面更多的 operator 去判断某些数据是否乱序。Flink 内部提供了很好的机制去保证这些 timestamp 和 watermark 被正确地传递到下游的节点。</p><h3 id="Watermark-传播"><a href="#Watermark-传播" class="headerlink" title="Watermark 传播"></a>Watermark 传播</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01yvLKJv20eLEgB7EWZ_!!6000000006874-2-tps-1024-474.png" alt="null"></p><p>具体的传播策略基本上遵循这三点。</p><ul><li>首先，watermark 会以广播的形式在算子之间进行传播。比如说上游的算子，它连接了三个下游的任务，它会把自己当前的收到的 watermark 以广播的形式传到下游。</li><li>第二，如果在程序里面收到了一个 Long.MAX_VALUE 这个数值的 watermark，就表示对应的那一条流的一个部分不会再有数据发过来了，它相当于就是一个终止的一个标志。</li><li>第三，对于单流而言，这个策略比较好理解，而对于有多个输入的算子，watermark 的计算就有讲究了，一个原则是：单输入取其大，多输入取小。</li></ul><p>举个例子，假设这边蓝色的块代表一个算子的一个任务，然后它有三个输入，分别是 W1、W2、W3，这三个输入可以理解成任何的输入，这三个输入可能是属于同一个流，也可能是属于不同的流。然后在计算 watermark 的时候，对于单个输入而言是取他们的最大值，因为我们都知道 watermark 应该遵循一个单调递增的一个原则。对于多输入，它要统计整个算子任务的 watermark 时，就会取这三个计算出来的 watermark 的最小值。即一个多个输入的任务，它的 watermark 受制于最慢的那条输入流。这一点类似于木桶效应，整个木桶中装的水会就是受制于最矮的那块板。</p><p>watermark 在传播的时候有一个特点是，它的传播是幂等的。多次收到相同的 watermark，甚至收到之前的 watermark 都不会对最后的数值产生影响，因为对于单个输入永远是取最大的，而对于整个任务永远是取一个最小的。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在它没有区分你这个输入是一条流多个 partition 还是来自于不同的逻辑上的流的 JOIN。对于同一个流的不同 partition，我们对他做这种强制的时钟同步是没有问题的，因为一开始就是把一条流拆散成不同的部分，但每一个部分之间共享相同的时钟。但是如果算子的任务是在做类似于 JOIN 操作，那么要求你两个输入的时钟强制同步其实没有什么道理的，因为完全有可能是把一条离现在时间很近的数据流和一个离当前时间很远的数据流进行 JOIN，这个时候对于快的那条流，因为它要等慢的那条流，所以说它可能就要在状态中去缓存非常多的数据，这对于整个集群来说是一个很大的性能开销。</p><h3 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h3><p>在正式介绍 watermark 的处理之前，先简单介绍 ProcessFunction，因为 watermark 在任务里的处理逻辑分为内部逻辑和外部逻辑。外部逻辑其实就是通过 ProcessFunction 来体现的，如果你需要使用 Flink 提供的时间相关的 API 的话就只能写在 ProcessFunction 里。</p><p>ProcessFunction 和时间相关的功能主要有三点：</p><ul><li>第一点就是根据你当前系统使用的时间语义不同，你可以去获取当前你正在处理这条记录的 Record Timestamp，或者当前的 Processing Time。</li><li>第二点就是它可以获取当前算子的时间，可以把它理解成当前的 watermark。</li><li>第三点就是为了在 ProcessFunction 中去实现一些相对复杂的功能，允许注册一些 timer（定时器）。比如说在 watermark 达到某一个时间点的时候就触发定时器，所有的这些回调逻辑也都是由用户来提供，涉及到如下三个方法，registerEventTimeTimer、registerProcessingTimeTimer 和 onTimer。在 onTimer 方法中就需要去实现自己的回调逻辑，当条件满足时回调逻辑就会被触发。</li></ul><p>一个简单的应用是，我们在做一些时间相关的处理的时候，可能需要缓存一部分数据，但这些数据不能一直去缓存下去，所以需要有一些过期的机制，我们可以通过 timer 去设定这么一个时间，指定某一些数据可能在将来的某一个时间点过期，从而把它从状态里删除掉。所有的这些和时间相关的逻辑在 Flink 内部都是由自己的 Time Service（时间服务）完成的。</p><h3 id="Watermark-处理"><a href="#Watermark-处理" class="headerlink" title="Watermark 处理"></a>Watermark 处理</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01CBGG1A1QhFa2kn59N_!!6000000002007-2-tps-1024-376.png" alt="null"></p><p>一个算子的实例在收到 watermark 的时候，首先要更新当前的算子时间，这样的话在 ProcessFunction 里方法查询这个算子时间的时候，就能获取到最新的时间。第二步它会遍历计时器队列，这个计时器队列就是我们刚刚说到的 timer，你可以同时注册很多 timer，Flink 会把这些 Timer 按照触发时间放到一个优先队列中。第三步 Flink 得到一个时间之后就会遍历计时器的队列，然后逐一触发用户的回调逻辑。 通过这种方式，Flink 的某一个任务就会将当前的 watermark 发送到下游的其他任务实例上，从而完成整个 watermark 的传播，从而形成一个闭环。</p><h2 id="Table-API-中的时间"><a href="#Table-API-中的时间" class="headerlink" title="Table API 中的时间"></a>Table API 中的时间</h2><p>下面我们来看一看 Table/SQL API 中的时间。为了让时间参与到 Table/SQL 这一层的运算中，我们需要提前把时间属性放到表的 schema 中，这样的话我们才能够在 SQL 语句或者 Table 的一些逻辑表达式里面去使用这些时间去完成需求。</p><h3 id="Table-中指定时间列"><a href="#Table-中指定时间列" class="headerlink" title="Table 中指定时间列"></a>Table 中指定时间列</h3><p>其实之前社区就怎么在 Table/SQL 中去使用时间这个问题做过一定的讨论，是把获取当前 Processing Time 的方法是作为一个特殊的 UDF，还是把这一个列物化到整个的 schema 里面，最终采用了后者。我们这里就分开来讲一讲 Processing Time 和 Event Time 在使用的时候怎么在 Table 中指定。</p><p><img src="https://img.alicdn.com/imgextra/i2/O1CN0180nzP71zIz9PQ1mJC_!!6000000006692-2-tps-1024-487.png" alt="null"></p><p>对于 Processing Time，我们知道要得到一个 Table 对象（或者注册一个 Table）有两种手段：</p><ol><li>可以从一个 DataStream 转化成一个 Table；</li><li>直接通过 TableSource 去生成这么一个 Table；</li></ol><p>对于第一种方法而言，我们只需要在你已有的这些列中（例子中 f1 和 f2 就是两个已有的列），在最后用“列名.proctime”这种写法就可以把最后的这一列注册为一个 Processing Time，以后在写查询的时候就可以去直接使用这一列。如果 Table 是通过 TableSource 生成的，就可以通过实现这一个 DefinedRowtimeAttributes 接口，然后就会自动根据你提供的逻辑去生成对应的 Processing Time。</p><p>相对而言，在使用 Event Time 时则有一个限制，因为 Event Time 不像 Processing Time 那样是随拿随用。如果你要从 DataStream 去转化得到一个 Table，必须要提前保证原始的 DataStream 里面已经存在了 Record Timestamp 和 watermark。如果你想通过 TableSource 生成的，也一定要保证你要接入的一个数据里面存在一个类型为 long 或者 timestamp 的这么一个时间字段。</p><p>具体来说，如果你要从 DataStream 去注册一个表，和 proctime 类似，你只需要加上“列名.rowtime”就可以。需要注意的是，如果你要用 Processing Time，必须保证你要新加的字段是整个 schema 中的最后一个字段，而 Event Time 的时候你其实可以去替换某一个已有的列，然后 Flink 会自动的把这一列转化成需要的 rowtime 这个类型。 如果是通过 TableSource 生成的，只需要实现 DefinedRowtimeAttributes 接口就可以了。需要说明的一点是，在 DataStream API 这一侧其实不支持同时存在多个 Event Time（rowtime），但是在 Table 这一层理论上可以同时存在多个 rowtime。因为 DefinedRowtimeAttributes 接口的返回值是一个对于 rowtime 描述的 List，即其实可以同时存在多个 rowtime 列，在将来可能会进行一些其他的改进，或者基于去做一些相应的优化。</p><h3 id="时间列和-Table-操作"><a href="#时间列和-Table-操作" class="headerlink" title="时间列和 Table 操作"></a>时间列和 Table 操作</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/O1CN01WP5Qhw1i7ftyGs3cU_!!6000000004366-2-tps-1024-475.png" alt="null"></p><p>指定完了时间列之后，当我们要真正去查询时就会涉及到一些具体的操作。这里我列举的这些操作都是和时间列紧密相关，或者说必须在这个时间列上才能进行的。比如说“Over 窗口聚合”和“Group by 窗口聚合”这两种窗口聚合，在写 SQL 提供参数的时候只能允许你在这个时间列上进行这种聚合。第三个就是时间窗口聚合，你在写条件的时候只支持对应的时间列。最后就是排序，我们知道在一个无尽的数据流上对数据做排序几乎是不可能的事情，但因为这个数据本身到来的顺序已经是按照时间属性来进行排序，所以说我们如果要对一个 DataStream 转化成 Table 进行排序的话，你只能是按照时间列进行排序，当然同时你也可以指定一些其他的列，但是时间列这个是必须的，并且必须放在第一位。</p><p>为什么说这些操作只能在时间列上进行？因为我们有的时候可以把到来的数据流就看成是一张按照时间排列好的一张表，而我们任何对于表的操作，其实都是必须在对它进行一次顺序扫描的前提下完成的。因为大家都知道数据流的特性之一就是一过性，某一条数据处理过去之后，将来其实不太好去访问它。当然因为 Flink 中内部提供了一些状态机制，我们可以在一定程度上去弱化这个特性，但是最终还是不能超越的限制状态不能太大。所有这些操作为什么只能在时间列上进行，因为这个时间列能够保证我们内部产生的状态不会无限的增长下去，这是一个最终的前提。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java时间工具类</title>
      <link href="/2021/09/01/Java%E6%97%B6%E9%97%B4%E5%B7%A5%E5%85%B7%E7%B1%BB/"/>
      <url>/2021/09/01/Java%E6%97%B6%E9%97%B4%E5%B7%A5%E5%85%B7%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Java时间工具类"><a href="#Java时间工具类" class="headerlink" title="Java时间工具类"></a>Java时间工具类</h1><ul><li><input disabled="" type="checkbox"> LocalDate</li><li><input disabled="" type="checkbox"> LocalTime</li><li><input disabled="" type="checkbox"> LocalDateTime</li><li><input disabled="" type="checkbox"> Instant</li></ul><h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一.简介"></a>一.简介</h2><ul><li><input disabled="" type="checkbox"> LocalDate表示当前(或指定)日期，格式为：yyyy-MM-dd</li><li><input disabled="" type="checkbox"> LocalTime表示当前(或指定)时间，格式为：HH:mm:ss SSS</li><li><input disabled="" type="checkbox"> LocalDateTime表示当前(或指定)日期时间，格式为：yyyy-MM-ddTHH:mm:ss SSS ,是前2者的结合</li><li><input disabled="" type="checkbox"> Instant表示当前(或指定)时间瞬时点，或者瞬间点</li></ul><h2 id="二-使用介绍"><a href="#二-使用介绍" class="headerlink" title="二.使用介绍"></a>二.使用介绍</h2><p>jdk1.8之后对表示时间的类型进行了重新编写，表示当前日期时间的有LocalDate、LocalTime、LocalDateTime这三个类，表示瞬间时间点的是Instant api提供了时间类型构造方法、getter方法、时间加减操作、时间判断操作、指定未来时间操作、时间支持的类型操作，其使用方法基本上一致。</p><h3 id="2-1-LocalDate"><a href="#2-1-LocalDate" class="headerlink" title="2.1 LocalDate"></a>2.1 LocalDate</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>DayOfWeek<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDate<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>temporal<span class="token punctuation">.</span>ChronoField<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>temporal<span class="token punctuation">.</span>ChronoUnit<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>temporal<span class="token punctuation">.</span>TemporalAdjusters<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: LocalDate表示当前(或指定)日期，格式为：yyyy-MM-dd * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">LocalDateDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LocalDate localDate <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 获取当前时间：yyyy-MM-dd ---> 2021-09-01</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalDate localDate2 <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token number">2099</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 可以指定具体的年月日时间</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate2<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 2099-12-12</span>        <span class="token comment" spellcheck="true">//============ LoacalDate 获取当前时间属性  ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>               <span class="token comment" spellcheck="true">//获取当前年份:2019</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>              <span class="token comment" spellcheck="true">//获取当前月份，英文：DECEMBER</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getMonthValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">//获取当前月份，数字：12</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getDayOfMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">//获取当前日期是所在月的第几天7</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getDayOfWeek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">//获取当前日期是星期几（星期的英文全称）:SATURDAY</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">getDayOfYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">//获取当前日期是所在年的第几天:341</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">lengthOfYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">//获取当前日期所在年有多少天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">lengthOfMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">//获取当前日期所在月份有多少天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isLeapYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//获取当前年份是否是闰年</span>        <span class="token comment" spellcheck="true">//============ LoacalDate 当前时间的加减  ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">minusYears</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>           <span class="token comment" spellcheck="true">//将当前日期减1年</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">minusMonths</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">//将当前日期减1月</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">minusDays</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//将当前日期减1天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">plusYears</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//将当前日期加1年</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">plusMonths</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>           <span class="token comment" spellcheck="true">//将当前日期加1月</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">plusDays</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>             <span class="token comment" spellcheck="true">//将当前日期加1天</span>        <span class="token comment" spellcheck="true">//============ LoacalDate 当前时间的判断 ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"LoacalDate的判断"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isAfter</span><span class="token punctuation">(</span>localDate2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">//localDate在localDate2日期之后</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isBefore</span><span class="token punctuation">(</span>localDate2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">//localDate在localDate2日期之前</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isEqual</span><span class="token punctuation">(</span>localDate2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">//localDate和localDate2日期是否相等</span>        <span class="token comment" spellcheck="true">//============ LoacalDate 当前时间支持的类型  ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isSupported</span><span class="token punctuation">(</span>ChronoField<span class="token punctuation">.</span>DAY_OF_YEAR<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//当前时间支持的时间类型是：一年中的某一天，这个不知道应用场景</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">isSupported</span><span class="token punctuation">(</span>ChronoUnit<span class="token punctuation">.</span>DAYS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//当前日期支持的单元：天(说明当前时间是按天来算的)</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">firstDayOfMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//本月的第1天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">firstDayOfNextMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//下月的第1天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">firstDayOfNextYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">//下年的第1天</span>        <span class="token comment" spellcheck="true">//============ LocalDate 指定时间的操作  ===============　　　　</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">withDayOfMonth</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                    <span class="token comment" spellcheck="true">//本月的第几天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">lastDayOfMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>             <span class="token comment" spellcheck="true">//本月的最后一天</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">previous</span><span class="token punctuation">(</span>DayOfWeek<span class="token punctuation">.</span>SUNDAY<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">//上一周星期天是几号</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate<span class="token punctuation">.</span><span class="token function">with</span><span class="token punctuation">(</span>TemporalAdjusters<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span>DayOfWeek<span class="token punctuation">.</span>MONDAY<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token comment" spellcheck="true">//下一周星期一是几号</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="2-2-LocalTime"><a href="#2-2-LocalTime" class="headerlink" title="2.2 LocalTime"></a>2.2 LocalTime</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalTime<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: 时间的案例 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">LocalTimeDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//============ LocalTime 的构造  ============</span>        LocalTime localTime <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">//获取当前时间</span>        <span class="token comment" spellcheck="true">//LocalTime.of(int hour, int minute) 根据参数设置时间，参数分别为时，分</span>        <span class="token comment" spellcheck="true">//LocalTime.of(int hour, int minute, int second) 根据参数设置时间，参数分别为时，分，秒</span>        LocalTime localTime2 <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalTime localTime3 <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime2<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime3<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//============ LoacalDate 获取当前时间属性  ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">getHour</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">getMinute</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">getSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">plusHours</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//将当前时间加1时</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">plusMinutes</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//将当前时间加1分钟</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">plusSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//将当前时间加1秒</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">minusHours</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//将当前时间减1小时</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">minusMinutes</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//将当前时间减1分钟</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">.</span><span class="token function">minusSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//将当前时间减1秒</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="2-3-LocalDateTime"><a href="#2-3-LocalDateTime" class="headerlink" title="2.3 LocalDateTime"></a>2.3 LocalDateTime</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDate<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDateTime<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalTime<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: 日期和时间的结合 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">LocalDateTimeDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LocalDateTime localDateTime <span class="token operator">=</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDateTime<span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalDate localDate <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalTime localTime <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalDateTime localDateTime2 <span class="token operator">=</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span>localDate<span class="token punctuation">,</span> localTime<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDateTime2<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDateTime<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="2-4-Instant"><a href="#2-4-Instant" class="headerlink" title="2.4 Instant"></a>2.4 Instant</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Instant<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDateTime<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: Instant表示当前(或指定)时间瞬时点，或者瞬间点 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">InstantDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        Instant instant <span class="token operator">=</span> Instant<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>instant<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>instant<span class="token punctuation">.</span><span class="token function">getNano</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>             <span class="token comment" spellcheck="true">//纳秒数</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>instant<span class="token punctuation">.</span><span class="token function">getEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//1970年到现在的秒数</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>instant<span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">//1970年到现在的毫秒数(和new Date().getTime() System.currentTimeMillis 一样)</span>        <span class="token comment" spellcheck="true">//============ Instant 时间区间的加减 省略,用法基本一致  ============</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>String<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h2 id="三-DateTimeFormatter"><a href="#三-DateTimeFormatter" class="headerlink" title="三.DateTimeFormatter"></a>三.DateTimeFormatter</h2><p>LocalDate、LocalTime、LocalDateTime、Instant的操作与使用，下面讲解它们之间是如何进行格式化</p><p>DateTimeFormatter这个类它只提供了时间格式化的类型，就是按你指定的格式，或者按jdk默认的格式，需要进行调用的则是时间类本身来进行调用才能进行格式化</p><p>LocalDate、LocalTime 的api是有2个方法，分别是：parse()、format()方法，时间类型的转换可以调用这2个来进行日期时间类型的转换</p><pre class=" language-java"><code class="language-java">E <span class="token function">parse</span><span class="token punctuation">(</span>CharSequence text<span class="token punctuation">)</span>E <span class="token function">parse</span><span class="token punctuation">(</span>CharSequence text<span class="token punctuation">,</span> DateTimeFormatter formatter<span class="token punctuation">)</span>String <span class="token function">format</span><span class="token punctuation">(</span>DateTimeFormatter formatter<span class="token punctuation">)</span></code></pre><h3 id="1-字符串转换成日期时间类型"><a href="#1-字符串转换成日期时间类型" class="headerlink" title="1.字符串转换成日期时间类型"></a>1.字符串转换成日期时间类型</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>text<span class="token punctuation">.</span>DateFormat<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>*<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>format<span class="token punctuation">.</span>DateTimeFormatter<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: 字符串转时间类型的工具类 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">StringToLocalDateDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// String --> LocalDate</span>        LocalDate localDate1 <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span><span class="token string">"2019-12-07"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        DateTimeFormatter pattern <span class="token operator">=</span> DateTimeFormatter<span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"yyyy年MM月dd日"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>LocalDate<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span><span class="token string">"2019-10-09"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>pattern<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// String --> LocalTime</span>        LocalTime localTime <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span><span class="token string">"07:43:53"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// String -->LocalDateTime</span>        DateTimeFormatter formatter1 <span class="token operator">=</span> DateTimeFormatter<span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"yyyy-MM-dd hh:mm:ss"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 12小时</span>        DateTimeFormatter formatter2 <span class="token operator">=</span> DateTimeFormatter<span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"yyyy-MM-dd HH:mm:ss"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 24小时</span>        LocalDate localDate2 <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span><span class="token string">"2019-12-07 07:43:53"</span><span class="token punctuation">,</span>formatter2<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate1<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localTime<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDate2<span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalDateTime localDateTime <span class="token operator">=</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span><span class="token string">"2021-09-01 10:10:10"</span><span class="token punctuation">,</span> formatter2<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 2021-09-01T10:10:10</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"字符串转时间："</span> <span class="token operator">+</span> localDateTime<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">" >>> "</span> <span class="token operator">+</span> formatter2<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>localDateTime<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>localDateTime<span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span>ZoneOffset<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token string">"+8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="2、时间类型转字符串"><a href="#2、时间类型转字符串" class="headerlink" title="2、时间类型转字符串"></a>2、时间类型转字符串</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDate<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalDateTime<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>LocalTime<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>ZoneId<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>format<span class="token punctuation">.</span>DateTimeFormatter<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">LocalDateToStringDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//localDate --> String</span>        LocalDate localDate <span class="token operator">=</span> LocalDate<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        String format1 <span class="token operator">=</span> localDate<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>DateTimeFormatter<span class="token punctuation">.</span>BASIC_ISO_DATE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//yyyyMMdd</span>        String format2 <span class="token operator">=</span> localDate<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>DateTimeFormatter<span class="token punctuation">.</span>ISO_DATE<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//yyyy-MM-dd</span>        <span class="token comment" spellcheck="true">//2.LocalTime  --> String</span>        LocalTime localTime <span class="token operator">=</span> LocalTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        String format3 <span class="token operator">=</span> localTime<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>DateTimeFormatter<span class="token punctuation">.</span>ISO_TIME<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//20:19:22.42</span>        DateTimeFormatter formatter <span class="token operator">=</span> DateTimeFormatter<span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"hh:mm:ss"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        String format4 <span class="token operator">=</span> localTime<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>formatter<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//3.LocalDateTime  --> String</span>        LocalDateTime localDateTime <span class="token operator">=</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        DateTimeFormatter formatter2 <span class="token operator">=</span> DateTimeFormatter<span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"yyyy-MM-dd hh:mm:ss"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        String format5 <span class="token operator">=</span> localDateTime<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>formatter2<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>format1<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>format2<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>format3<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>format4<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>format5<span class="token punctuation">)</span><span class="token punctuation">;</span>        LocalDateTime localDateTime1 <span class="token operator">=</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">ofInstant</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ZoneId<span class="token punctuation">.</span><span class="token function">systemDefault</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"日期时间类型转换成字符串"</span> <span class="token operator">+</span> localDateTime1<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>formatter2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h2 id="四-时间类Instant及ZoneId，ZoneOffset用法"><a href="#四-时间类Instant及ZoneId，ZoneOffset用法" class="headerlink" title="四.时间类Instant及ZoneId，ZoneOffset用法"></a>四.时间类Instant及ZoneId，ZoneOffset用法</h2><p>Instant.now().getEpochSecond()<br>时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。<br>时间戳转化为Date或LocalDateTime时，需要添加ZoneId（地区）或ZoneOffset（偏移数据）来转为本地时间。</p><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>time<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>*<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.java.util.time * @Author: wmy * @Date: 2021/9/1 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: Instant表示当前(或指定)时间瞬时点，或者瞬间点 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">InstantDemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前的时间毫秒数："</span> <span class="token operator">+</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前的日期和时间："</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前时间的秒数："</span> <span class="token operator">+</span> Instant<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Instant时间瞬点："</span> <span class="token operator">+</span> Instant<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"时间转瞬点："</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"========================================"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"1 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span>ZoneOffset<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token string">"+8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"2 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atZone</span><span class="token punctuation">(</span>ZoneId<span class="token punctuation">.</span><span class="token function">systemDefault</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"3 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atZone</span><span class="token punctuation">(</span>ZoneId<span class="token punctuation">.</span><span class="token function">systemDefault</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"4 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atZone</span><span class="token punctuation">(</span>ZoneId<span class="token punctuation">.</span><span class="token function">systemDefault</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"5 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atOffset</span><span class="token punctuation">(</span>OffsetDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getOffset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"6 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atOffset</span><span class="token punctuation">(</span>OffsetDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getOffset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"7 >>> "</span> <span class="token operator">+</span> LocalDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">atOffset</span><span class="token punctuation">(</span>OffsetDateTime<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getOffset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"========================================"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        Instant instant <span class="token operator">=</span> Instant<span class="token punctuation">.</span><span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前时间的毫秒数: "</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>instant<span class="token punctuation">.</span><span class="token function">getEpochSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前时间的毫秒数: "</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"当前时间的毫秒数: "</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>LocalDateTime<span class="token punctuation">.</span><span class="token function">ofInstant</span><span class="token punctuation">(</span>instant<span class="token punctuation">,</span> ZoneId<span class="token punctuation">.</span><span class="token function">systemDefault</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toInstant</span><span class="token punctuation">(</span>ZoneOffset<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span><span class="token string">"+8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toEpochMilli</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>趣头条基于-Flink-ClickHouse构建实时数据分析平台</title>
      <link href="/2021/08/31/%E8%B6%A3%E5%A4%B4%E6%9D%A1%E5%9F%BA%E4%BA%8E-Flink-ClickHouse%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/"/>
      <url>/2021/08/31/%E8%B6%A3%E5%A4%B4%E6%9D%A1%E5%9F%BA%E4%BA%8E-Flink-ClickHouse%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/</url>
      
        <content type="html"><![CDATA[<p>title: 趣头条基于-Flink-ClickHouse构建实时数据分析平台<br>top: true<br>cover: true<br>author: 情深骚明<br>summary: 学习心得记录<br>date: 2021-08-31 14:14:33<br>categories: Flink流式引擎<br>tags: Flink流式引擎</p><h1 id="趣头条基于-Flink-ClickHouse构建实时数据分析平台"><a href="#趣头条基于-Flink-ClickHouse构建实时数据分析平台" class="headerlink" title="趣头条基于-Flink-ClickHouse构建实时数据分析平台"></a>趣头条基于-Flink-ClickHouse构建实时数据分析平台</h1><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831141514688.png" alt="业务场景分析"></p><p><strong>摘要：</strong>本文由趣头条数据平台负责人王金海分享，主要介绍趣头条 Flink-to-Hive 小时级场景和 Flink-to-ClickHouse 秒级场景，内容分为以下四部分：</p><ol><li>业务场景与现状分析</li><li>Flink-to-Hive 小时级场景</li><li>Flink-to-ClickHouse 秒级场景</li><li>未来发展与思考</li></ol><h2 id="业务场景与现状分析"><a href="#业务场景与现状分析" class="headerlink" title="业务场景与现状分析"></a>业务场景与现状分析</h2><p>趣头条查询的页面分为离线查询页面和实时查询页面。趣头条今年所实现的改造是在实时查询中接入了 ClickHouse 计算引擎。根据不同的业务场景，实时数据报表中会展现数据指标曲线图和详细的数据指标表。目前数据指标的采集和计算为每五分钟一个时间窗口，当然也存在三分钟或一分钟的特殊情况。数据指标数据全部从 Kafka 实时数据中导出，并导入 ClickHouse 进行计算。</p><h2 id="Flink-to-Hive-小时级场景"><a href="#Flink-to-Hive-小时级场景" class="headerlink" title="Flink-to-Hive 小时级场景"></a>Flink-to-Hive 小时级场景</h2><h3 id="小时级实现架构图"><a href="#小时级实现架构图" class="headerlink" title="小时级实现架构图"></a>小时级实现架构图</h3><p>如下图所示，Database 中的 Binlog 导出到 Kafka，同时 Log Server 数据也会上报到 Kafka。所有数据实时落地到 Kafka 之后，通过 Flink 抽取到 HDFS。下图中 HDFS 到 Hive 之间为虚线，即 Flink 并非直接落地到 Hive，Flink 落地到 HDFS 后，再落地到 Hive 的时间可能是小时级、半小时级甚至分钟级，需要知道数据的 Event time 已经到何时，再触发 alter table，add partition，add location 等，写入其分区。</p><p>这时需要有一个程序监控当前 Flink 任务的数据时间已经消费到什么时候，如9点的数据，落地时需要查看 Kafka 中消费的数据是否已经到达9点，然后在 Hive 中触发分区写入。</p><p><img src="C:/Users/wmy/AppData/Roaming/Typora/typora-user-images/image-20210831141843680.png" alt="小时架构图"></p><h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><p>趣头条主要使用了 Flink 高阶版本的一个特性——StreamingFileSink。StreamingFileSink 主要有几点功能。</p><ul><li>第一， forBulkFormat 支持 avro、parquet 格式，即列式存储格式。</li><li>第二， withBucketAssigner 自定义按数据时间分桶，此处会定义一个EventtimeBucket，既按数据时间进行数据落地到离线中。</li><li>第三， OnCheckPointRollingPolicy，根据 CheckPoint 时间进行数据落地，在一定的 CheckPoint 时间内数据落地并回稳。按照 CheckPoint 落地还有其它策略，如按照数据大小。</li><li>第四， StreamingFileSink 是 Exactly-Once 语义实现。</li></ul><p>Flink 中有两个 Exactly-Once 语义实现，第一个是 Kafka，第二个是 StreamingFileSink。下图为 OnCheckPointRollingPolicy 设计的每10分钟落地一次到HDFS文件中的 demo。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142021326.png" alt="实现原理"></p><h4 id="■-如何实现-Exactly-Once"><a href="#■-如何实现-Exactly-Once" class="headerlink" title="■ 如何实现 Exactly-Once"></a>■ 如何实现 Exactly-Once</h4><p>下图左侧为一个简单的二 PC 模型。Coordinator 发送一个 prepare，执行者开始触发 ack 动作，Coordinator 收到 ack 所有消息后，所有 ack 开始触发 commit，所有执行者进行落地，将其转化到 Flink 的模型中，Source 收到 checkpoint barrier 流时，开始触发一个 snapshot。</p><p>每个算子的 CheckPoint、snapshot 都完成之后，CheckPoint 会给 Job Manager 发送 notifyCheckpointComplete。下图中二阶段模型和 Flink 模型左侧三条线部分是一致的。因此用 Flink 可以实现二阶段提交协议。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142148040.png" alt="Exactly-Once"></p><h4 id="■-如何使用-Flink-实现二阶段提交协议"><a href="#■-如何使用-Flink-实现二阶段提交协议" class="headerlink" title="■ 如何使用 Flink 实现二阶段提交协议"></a>■ 如何使用 Flink 实现二阶段提交协议</h4><p>首先，StreamingFileSink 实现两个接口，CheckpointedFunction 和CheckpointListener。CheckpointedFunction 实现 initializeState 和 snapshotState 函数。CheckpointListener 是 notifyCheckpointComplete 的方法实现，因此这两个接口可以实现二阶段提交语义。</p><h5 id="initializeState"><a href="#initializeState" class="headerlink" title="initializeState"></a>initializeState</h5><p>initializeState 在任务启动时会触发三个动作。第一个是 commitPendingFile。实时数据落地到 Hdfs 上有三个状态。第一个状态是 in-progress ，正在进行状态。第二个状态是 pending 状态，第三个状态是 finished 状态。</p><p>initializeState 在任务启动时还会触发 restoreInProgressFile，算子实时写入。如果 CheckPoint 还未成功时程序出现问题，再次启动时 initializeState 会 commit PendingFile，然后采用 Hadoop 2.7+ 版本的 truncate 方式重置或截断 in-progress 文件。</p><h5 id="invoke"><a href="#invoke" class="headerlink" title="invoke"></a>invoke</h5><p>实时写入数据。</p><h5 id="snapshotState"><a href="#snapshotState" class="headerlink" title="snapshotState"></a>snapshotState</h5><p>触发 CheckPoint 时会将 in-progress 文件转化为 pending state，同时记录数据长度（truncate 方式需要截断长度）。snapshotState 并非真正将数据写入 HDFS，而是写入 ListState。Flink 在 Barrier 对齐状态时内部实现 Exactly-Once 语义，但是实现外部端到端的 Exactly-Once 语义比较困难。Flink 内部实现 Exactly-Once 通过 ListState，将数据全部存入 ListState，等待所有算子 CheckPoint 完成，再将 ListState 中的数据刷到 HDFS 中。</p><h5 id="notifyCheckpointComplete"><a href="#notifyCheckpointComplete" class="headerlink" title="notifyCheckpointComplete"></a>notifyCheckpointComplete</h5><p>notifyCheckpointComplete 会触发 pending 到 finished state 的数据写入。实现方法是 rename，Streaming 不断向 HDFS 写入临时文件，所有动作结束后通过 rename 动作写成正式文件。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142411909.png" alt="image-20210831142411909"></p><h3 id="跨集群多-nameservices"><a href="#跨集群多-nameservices" class="headerlink" title="跨集群多 nameservices"></a>跨集群多 nameservices</h3><p>趣头条的实时集群和离线集群是独立的，离线集群有多套，实时集群目前有一套。通过实时集群写入离线集群，会产生 HDFS nameservices 问题。在实时集群中将所有离线集群的 nameservices 用 namenode HA 的方式全部打入实时集群并不合适。那么如何在任务中通过实时集群提交到各个离线集群？</p><p>如下图所示，在 Flink 任务的 resource 下面，在 HDFS 的 xml 中间加入 。在 PropertyHong Kong 中添加 nameservices，如 stream 是实时集群的 namenode HA 配置，data 是即将写入的离线集群的 namenode HA 配置。那么两个集群中间的 HDFS set 不需要相互修改，直接可以在客户端实现。</p><p><img src="C:/Users/wmy/AppData/Roaming/Typora/typora-user-images/image-20210831142453719.png" alt="跨集群"></p><h3 id="多用户写入权限"><a href="#多用户写入权限" class="headerlink" title="多用户写入权限"></a>多用户写入权限</h3><p>实时要写入离线 HDFS，可能会涉及用户权限问题。实时提交的用户已经定义好该用户在所有程序中都是同一个用户，但离线中是多用户的，因此会造成实时和离线用户不对等。趣头条在 API 中添加了 withBucketUser 写 HDFS。配置好 nameservices后，接下来只需要知道该 HDFS 路径通过哪个用户来写，比如配置一个 stream 用户写入。</p><p>API 层级的好处是一个 Flink 程序可以指定多个不同的 HDFS 和不同的用户。多用户写入的实现是在 Hadoop file system 中加一个 ugi.do as ，代理用户。以上为趣头条使用 Flink 方式进行实时数据同步到 Hive 的一些工作。其中可能会出现小文件问题，小文件是后台程序进行定期 merge，如果 CheckPoint 间隔时间较短，如3分钟一次，会出现大量小文件问题。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142637667.png" alt="image-20210831142637667"></p><h2 id="Flink-to-ClickHouse-秒级场景"><a href="#Flink-to-ClickHouse-秒级场景" class="headerlink" title="Flink-to-ClickHouse 秒级场景"></a>Flink-to-ClickHouse 秒级场景</h2><h3 id="秒级实现架构图"><a href="#秒级实现架构图" class="headerlink" title="秒级实现架构图"></a>秒级实现架构图</h3><p>趣头条目前有很多实时指标，平均每五分钟或三分钟计算一次，如果每一个实时指标用一个 Flink 任务，或者一个 Flink SQL 来写，比如消费一个 Kafka Topic，需要计算其日活、新增、流程等等当用户提出一个新需求时，需要改当前的 Flink 任务或者启动一个新的 Flink 任务消费 Topic。</p><p>因此会出现 Flink 任务不断修改或者不断起新的 Flink 任务的问题。趣头条尝试在 Flink 后接入 ClickHouse，实现整体的 OLAP。下图为秒级实现架构图。从 Kafka 到 Flink，到 Hive，到 ClickHouse 集群，对接外部 Horizon（实时报表），QE（实时 adhoc 查询），千寻（数据分析），用户画像（实时圈人）。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142742269.png" alt="秒级架构图"></p><h3 id="Why-Flink-ClickHouse"><a href="#Why-Flink-ClickHouse" class="headerlink" title="Why Flink+ClickHouse"></a>Why Flink+ClickHouse</h3><ul><li><strong>指标实现 sql 化描述</strong>：分析师提出的指标基本都以 SQL 进行描述。</li><li><strong>指标的上下线互不影响</strong>：一个 Flink 任务消费 Topic，如果还需要其它指标，可以保证指标的上下线互不影响。</li><li><strong>数据可回溯，方便异常排查</strong>：当日活下降，需要回溯排查是哪些指标口径的逻辑问题，比如是报的数据差异或是数据流 Kafka 掉了，或者是因为用户没有上报某个指标导致日活下降，而 Flink 则无法进行回溯。</li><li><strong>计算快，一个周期内完成所有指标计算</strong>：需要在五分钟内将成百上千的所有维度的指标全部计算完成。</li><li><strong>支持实时流，分布式部署，运维简单</strong>：支持 Kafka 数据实时流。</li></ul><p>目前趣头条 Flink 集群有 100+ 台 32 核 128 G 3.5T SSD，日数据量 2000+ 亿，日查询量 21w+ 次，80% 查询在 1s 内完成。下图为单表测试结果。ClickHouse 单表测试速度快。但受制于架构，ClickHouse 的 Join 较弱。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142905760.png" alt="clickhouse"></p><p>下图是处理相对较为复杂的 SQL，count+group by+order by，ClickHouse 在 3.6s内完成 26 亿数据计算。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831142943666.png" alt="image-20210831142943666"></p><h3 id="Why-ClickHouse-so-Fast"><a href="#Why-ClickHouse-so-Fast" class="headerlink" title="Why ClickHouse so Fast"></a>Why ClickHouse so Fast</h3><p>ClickHouse 采用列式存储 +LZ4、ZSTD 数据压缩。其次，计算存储结合本地化+向量化执行。Presto 数据可能存储在 Hadoop 集群或者 HDFS 中，实时拉取数据进行计算。而 ClickHouse 计算存储本地化是指每一台计算机器存在本地 SSD 盘，只需要计算自己的数据，再进行节点合并。同时，LSM merge tree+Index。将数据写入 ClickHouse 之后，会在后台开始一个线程将数据进行 merge，做 Index 索引。如建常见的 DT 索引和小时级数据索引，以提高查询性能。第四，SIMD+LLVM 优化。SIMD 是单指令多数据集。第五，SQL 语法及 UDF 完善。ClickHouse 对此有很大需求。在数据分析或者维度下拽时需要更高的特性，如时间窗口的一部分功能点。</p><ul><li><p><strong>Merge Tree</strong>：如下图所示。第一层为实时数据写入。后台进行每一层级数据的merge。merge 时会进行数据排序，做 Index 索引。</p></li><li><p><strong>ClickHouse Connector</strong>：ClickHouse 有两个概念，Local table 和Distributed table。一般是写 Local table ，读 Distributed table。ClickHouse 一般以 5~10w一个批次进行数据写入，5s一个周期。趣头条还实现了 RoundRobinClickHouseDataSource。</p></li><li><p><strong>BalancedClickHouseDataSource</strong> ：MySQL 中配置一个 IP 和端口号就可以写入数据，而 BalancedClickHouseDataSource 需要写 Local 表，因此必须要知道该集群有多少个 Local 表，每一个 Local 表的 IP 和端口号。如有一百台机器，需要将一百台机器的 IP 和端口号全部配置好，再进行写入。BalancedClickHouseDataSource 有两个 schedule。scheduleActualization和 scheduleConnectionsCleaning 。配置一百台机器的 IP 和端口号，会出现某些机器不连接或者服务不响应问题，scheduleActualization 会定期发现机器无法连接的问题，触发下线或删除 IP 等动作。scheduleConnectionsCleaning 会定期清理 ClickHouse 中无用的 http 请求。</p><p><img src="C:/Users/wmy/AppData/Roaming/Typora/typora-user-images/image-20210831143140695.png" alt="image-20210831143140695"></p></li><li><p><strong>RoundRobinClickHouseDataSource</strong>：趣头条对BalancedClickHouseDataSource 进行加强的结果，实现了三个语义。testOnBorrow 设置为 true，尝试 ping 看能否获取连接。用 ClickHouse 写入时是一个 batch，再将 testOnReturn 设置为 false，testWhileIdel 设置为true，填入官方 scheduleActualization 和 scheduleConnectionsCleaning 的功能。ClickHouse 后台不断进行 merge，如果 insert 过快使后台 merge 速度变慢，跟不上 insert，出现报错。因此需要尽量不断往下写，等写完当前机器，再写下一个机器，以5s间隔进行写入，使 merge 速度能够尽量与 insert 速度保持一致。</p></li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831143227712.png" alt="image-20210831143227712"></p><h3 id="Backfill"><a href="#Backfill" class="headerlink" title="Backfill"></a>Backfill</h3><p>Flink 导入 ClickHouse，在数据查询或展示报表时，会遇到一些问题，比如 Flink 任务出现故障、报错或数据反压等，或 ClickHouse 集群出现不可响应，zk 跟不上，insert 过快或集群负载等问题，这会导致整个任务出现问题。</p><p>如果流数据量突然暴增，启动 Flink 可能出现一段时间内不断追数据的情况，需要进行调整并行度等操作帮助 Flink 追数据。但这时已经出现数据积压，若还要加大 Flink 并发度处理数据，ClickHouse 限制 insert 不能过快，否则会导致恶性循环。因此当 Flink 故障或 ClickHouse 集群故障时，等待 ClickHouse 集群恢复后，Flink 任务从最新数据开始消费，不再追过去一段时间的数据，通过 Hive 将数据导入到 ClickHouse。</p><p>由于之前已经通过 Kafka 将数据实时落地到 Hive，通过 Hive 将数据写入 ClickHouse 中。ClickHouse 有分区，只需要将上一个小时的数据删除，导入 Hive 的一小时数据，就可以继续进行数据查询操作。Backfill 提供了 Flink 任务小时级容错以及 ClickHouse 集群小时级容错机制。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831143319041.png" alt="image-20210831143319041"></p><h2 id="未来发展与思考"><a href="#未来发展与思考" class="headerlink" title="未来发展与思考"></a>未来发展与思考</h2><h3 id="Connector-SQL-化"><a href="#Connector-SQL-化" class="headerlink" title="Connector SQL 化"></a>Connector SQL 化</h3><p>目前， Flink-to-Hive 以及 Flink-to-ClickHouse 都是趣头条较为固化的场景，只需指定 HDFS 路径以及用户，其余过程都可以通过 SQL 化描述。</p><h3 id="Delta-lake"><a href="#Delta-lake" class="headerlink" title="Delta lake"></a>Delta lake</h3><p>Flink 是流批一体计算引擎，但是没有流批一体的存储。趣头条会用 HBase、Kudu、Redis 等能够与 Flink 实时交互的 KV 存储进行数据计算。如计算新增问题，目前趣头条的方案是需要将 Hive 历史用户刷到 Redis 或 HBase 中，与 Flink 进行实时交互判断用户是否新增。</p><p>但因为 Hive 中的数据和 Redis 中的数据是存储为两份数据。其次 Binlog 抽取数据会涉及 delete 动作，Hbase，Kudu 支持数据修改，定期回到 Hive 中。带来的问题是 HBase，Kudu 中存在数据，Hive 又保存了一份数据，多出一份或多份数据。如果有流批一体的存储支持上述场景，当 Flink 任务过来，可以与离线数据进行实时交互，包括实时查询 Hive 数据等，可以实时判断用户是否新增，对数据进行实时修改、更新或 delete，也能支持 Hive 的批的动作存储。</p><p>未来，趣头条考虑对 Flink 做流批的存储，使 Flink 生态统一为流批结合。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink实时OLAP</title>
      <link href="/2021/08/31/Flink%E5%AE%9E%E6%97%B6OLAP/"/>
      <url>/2021/08/31/Flink%E5%AE%9E%E6%97%B6OLAP/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink实时OLAP"><a href="#Flink实时OLAP" class="headerlink" title="Flink实时OLAP"></a>Flink实时OLAP</h1><p>作者： 高正炎</p><p>本文主要介绍 BTC.com 团队在实时 OLAP 方面的技术演进过程及生产优化实践，内容如下： BTC.com 是一家区块链技术方案提供者，我们的业务主要分为四个部分，总结来说就是 ABCD：A 是人工智能机器学习，B 是区块链，C 代表云，D 是数据。这些模块不仅相互独立的，也可以互相结合。近几年人工智能、区块链的加速发展与大数据在背后提供的支持息息相关。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135218846.png" alt="OLAP"></p><p>本文主要介绍 BTC.com 团队在实时 OLAP 方面的技术演进过程及生产优化实践，内容如下：</p><ol><li>业务背景</li><li>机遇挑战</li><li>架构演进</li><li>架构优化</li><li>未来展望</li></ol><h2 id="一、业务背景"><a href="#一、业务背景" class="headerlink" title="一、业务背景"></a>一、业务背景</h2><h3 id="1-1-业务介绍-ABCD"><a href="#1-1-业务介绍-ABCD" class="headerlink" title="1.1 业务介绍 - ABCD"></a>1.1 业务介绍 - ABCD</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135253527.png" alt="ABCD"></p><p>BTC.com 是一家区块链技术方案提供者，我们的业务主要分为四个部分，总结来说就是 ABCD：A 是人工智能机器学习，B 是区块链，C 代表云，D 是数据。这些模块不仅相互独立的，也可以互相结合。近几年人工智能、区块链的加速发展与大数据在背后提供的支持息息相关。</p><h3 id="1-2-业务介绍-区块链技术方案提供商"><a href="#1-2-业务介绍-区块链技术方案提供商" class="headerlink" title="1.2 业务介绍 - 区块链技术方案提供商"></a>1.2 业务介绍 - 区块链技术方案提供商</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135345153.png" alt="提供商"></p><p>区块链通俗来讲可以理解为一个不可逆的分布式账本，我们的作用是让大家能更好的浏览账本，挖掘账本背后的信息数据。目前比特币的数据量级大概在几十亿到百亿，数据量大概在数十T，当然我们也有其他的一些业务，如以太坊货币、智能合约分析服务等。</p><p>整体而言我们是一家区块链技术方案的提供商，提供挖矿的服务。与金融行业的银行一样，我们也有很多的 OLAP 需求，比如当黑客攻击交易所或供应链进行资产转移或者洗钱时，需要经过链上的操作，我们可以在链上对其进行分析，以及交易上的跟踪，统计数据等，为警方提供协助。</p><h2 id="二、机遇挑战"><a href="#二、机遇挑战" class="headerlink" title="二、机遇挑战"></a>二、机遇挑战</h2><h3 id="2-1-之前的架构"><a href="#2-1-之前的架构" class="headerlink" title="2.1 之前的架构"></a>2.1 之前的架构</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135500835.png" alt="之前架构"></p><p>大概 2018 年的时候，竞争对手比较少，我们整体的架构如上。底层是区块链的节点，通过 Parser 不断的解析到 MySQL ，再从 MySQL 抽取到 Hive 或者 Presto，从 Spark 跑各种定时任务分析数据，再通过可视化的查询，得到报表或者数据。架构的问题也是显而易见的：</p><ul><li>不能做到实时处理数据</li><li>存在单点问题，比方某一条链路突然挂掉，此时整个环节都会出现问题</li></ul><h3 id="2-2-遇到的需求与挑战"><a href="#2-2-遇到的需求与挑战" class="headerlink" title="2.2 遇到的需求与挑战"></a>2.2 遇到的需求与挑战</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135606502.png" alt="需求和挑战"></p><p>在技术选型的时候我们需要考虑什么呢？首先是缩容，2020年行情不太好，大家都在尽力缩减成本，更好的活下去。在成本有限的情况下，我们如何能做更多的东西，必须提高自身的效率，同时也要保证质量。所以我们需要找到一种平衡，在成本效率还有质量这三者之间进行一定的平衡。</p><h2 id="三、架构演进"><a href="#三、架构演进" class="headerlink" title="三、架构演进"></a>三、架构演进</h2><h3 id="3-1-技术选型"><a href="#3-1-技术选型" class="headerlink" title="3.1 技术选型"></a>3.1 技术选型</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135713512.png" alt="技术选型"></p><p>俗话说，工具选的好，下班下的早，关于是否引入 Flink，我们想了很久，它和 Spark 相比优势在哪里？</p><p>我们实际调研以后，发现 Flink 还是有很多优势，比方说灵活的窗口，精准的语义，低延迟，支持秒级的，实时的数据处理。因为团队本身更熟练 Python ，所以我们当时就选择了 PyFlink ，有专业的开发团队支撑，近几个版本变化比较大，实现了很多功能。在实时 OLAP 方面，数据库我们采用了 ClickHouse 。</p><h3 id="3-2-为什么使用-ClickHouse"><a href="#3-2-为什么使用-ClickHouse" class="headerlink" title="3.2 为什么使用 ClickHouse"></a>3.2 为什么使用 ClickHouse</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135818131.png" alt="clickhouse"></p><p>为什么要使用 ClickHouse ？首先是快，查询的效率高。字节跳动，腾讯，快手等大公司都在用。同时我们也有 C++方面的技术积累，使用起来比较容易，成本不是太高。</p><h3 id="3-3-实时-OLAP-架构"><a href="#3-3-实时-OLAP-架构" class="headerlink" title="3.3 实时 OLAP 架构"></a>3.3 实时 OLAP 架构</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831135854285.png" alt="实时OLAP架构"></p><p>基于以上的技术选型，我们就形成了上图的架构，底层是数据源，包括区块链的节点，通过 Parser 解析到 Kafka，Kafka 负责对接 Flink 和 Spark 任务，然后 Flink 把数据输出到 MySQL 和 ClickHouse，支持报表导出，数据统计，数据同步，OLAP 统计等。</p><p>数据治理方面，我们参考了业界的分层，分成了原始层、明细层、汇总层以及应用层。我们还有机器学习的任务，这些都部署在 K8s 平台之上。</p><h3 id="3-4-架构演进历程"><a href="#3-4-架构演进历程" class="headerlink" title="3.4 架构演进历程"></a>3.4 架构演进历程</h3><p>我们的架构演进过程如下图，从 2018 年的 Spark 和 Hive ，到后来的 Tableau 可视化，今年接触了 Flink ，下半年开始使用 ClickHouse ，后来 Flink 任务比较多了，我们开发了简易的调度平台，开发者只需要上传任务，就会定时或者实时的跑任务。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140015064.png" alt="架构演进过程"></p><h3 id="3-5-架构演进思考"><a href="#3-5-架构演进思考" class="headerlink" title="3.5 架构演进思考"></a>3.5 架构演进思考</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140038200.png" alt="思考"></p><ul><li>为什么演进这么慢，因为区块链的发展还没有达到一定量级，无法像某些大公司有上亿级别或者 PB 级别的数据量。我们的数据量没有那么大，区块链是一个新鲜的事物，没有一定的历史。另外的问题就是资源问题，由于人员不足，人员成本上也有所控制。</li><li>刚才讲的架构，我们总结了它适合怎样的企业。首先是有一定的数据规模，比说某个企业 MySQL 只有几千万的数据，用 MySQL , Redis , MongoDB 都可以，就不适合这套架构。其次是需要一定的成本控制，这一整套成本算下来比 Spark 那一套会低很多。要有技术储备，要开发了解相关的东西。</li><li>区块链数据的特点。数据量比较多，历史数据基本上是不变的，实时数据相对来说是更有价值的，数据和时间存在一定的关联。</li></ul><h3 id="3-6-实时-OLAP-产生的价值"><a href="#3-6-实时-OLAP-产生的价值" class="headerlink" title="3.6 实时 OLAP 产生的价值"></a>3.6 实时 OLAP 产生的价值</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140207316.png" alt="实时OLAP得价值"></p><p>在实时 OLAP 上线后，基本满足了业务需求，同时成本也在可控的范围内。</p><ul><li>适合的是最好的，不要盲目追求新技术，比如数据湖，虽然好，但是我们的数据量级实际上用不到。</li><li>我们不考虑建设技术中台，我们的公司规模是中小型，部门沟通起来比较容易，没有太多的隔阂，没有发展到一定的组织规模，所以我们没有打算发展技术中台，数据中台，不盲目跟风上中台。</li><li>我们达到的效果是缩短了开发的时长，减少作业的运行时间。</li></ul><h2 id="四、架构优化"><a href="#四、架构优化" class="headerlink" title="四、架构优化"></a>四、架构优化</h2><h3 id="4-1-Flink-和-ClickHouse"><a href="#4-1-Flink-和-ClickHouse" class="headerlink" title="4.1 Flink 和 ClickHouse"></a>4.1 Flink 和 ClickHouse</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140326295.png" alt="Flink和Clickhouse"></p><p>Flink 和 ClickHouse 之间有一些联动，我们自定义了三个工作。</p><ul><li>自定义 sink 。</li><li>ClickHouse 要一次性插入很多数据，需要控制好写入的频次，优先写入本地表，耗时比较多。</li><li>我们主要用在智能合约的交易分析，新增的数据比较多，比较频繁，每几秒就有很多数据。数据上关联比较多。</li></ul><h3 id="4-2-ClickHouse-遇到的问题"><a href="#4-2-ClickHouse-遇到的问题" class="headerlink" title="4.2 ClickHouse 遇到的问题"></a>4.2 ClickHouse 遇到的问题</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140434361.png" alt="挑战"></p><ul><li>批量导入时失败和容错。</li><li>Upsert 的优化。</li><li>开发了常用 UDF ，大家知道 ClickHouse 官方是不支持 UDF 的吗？只能通过打补丁，保证 ClickHouse 不会挂。</li></ul><p>我们也在做一些开源方面的跟进，做一些补丁方面的尝试，把我们业务上，技术上常用的 UDF ，集合在一起。</p><h3 id="4-3-批量导入策略"><a href="#4-3-批量导入策略" class="headerlink" title="4.3 批量导入策略"></a>4.3 批量导入策略</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140534356.png" alt="批量导入策略"></p><ul><li>历史数据，可以认为是一种冷数据，相对来说不会经常改变。导入的时候按照大小切分，按照主键排序，类似于 bitcoind ，底层的 Checker 和 Fixer 工作，导入过程中及时进行报警和修复。比如导入某一数据失败了，如何更好的及时发现，之前就只能人肉监控。</li><li>实时数据，我们需要不断解析实时数据，大家可能对重组，51%的概念不太熟悉，这里简单讲一下，上图最长的链也是最重要的链，它上面的一条链是一个重组并且分叉的一条链，当有一个攻击者或者矿工去挖了上面的链，最终的结果会导致这条链被废弃掉，拿不到任何奖励。</li></ul><p>如果超过51%的算力，就会达到这样的效果，成为最长的链，这个是累计难度比较高的，此时我们会认为数据导入失败，同时我们会利用回撤的功能，不断将其回滚和重组，直到满足最完整的链。当然我们也会设置一些记录和 CheckPoint ，这里的 CheckPoint 和 Flink 的 CheckPoint 的概念也有所区别。</p><p>它是区块链方面的 CheckPoint ，区块链有一个币种叫 bch ，会定义 CheckPoint，当满足一定的长度时，它就无法再进行回滚，避免了攻击者的攻击。我们主要是利用 CheckPoint 记录信息，防止回滚，同时还会按照级别/表记录批量插入的失败或者成功，如果失败则会进行重试，以及报警回滚等操作。</p><h3 id="4-4-Upsert-的优化"><a href="#4-4-Upsert-的优化" class="headerlink" title="4.4 Upsert 的优化"></a>4.4 Upsert 的优化</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140739704.png" alt="Upsert的优化"></p><p>ClickHouse 不支持 Upsert ，主要在 SDK 方面做兼容，之前是直接往 MySQL 写数据，目标是通过 SQL 语句修改对应的 SDK 增加临时小表的 join ，通过 join 临时小表，进行 Upsert 的操作。</p><p>举个例子，区块链地址账户余额，就像银行的账户余额，必须非常精确。</p><h3 id="4-5-Kubernetes-方面优化"><a href="#4-5-Kubernetes-方面优化" class="headerlink" title="4.5 Kubernetes 方面优化"></a>4.5 Kubernetes 方面优化</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140856459.png" alt="Kubernetes方面优化"></p><p>Kubernetes 方面的优化。Kubernetes 是一个很完整的平台。</p><ul><li>高可用的存储，在早期的时候，我们就尽可能的将服务部署在 Kubernetes，包括 Flink 集群，基础业务组件，币种节点，ClickHouse 节点，在这方面 ClickHouse 做的比较好，方便兼容，支持高可用操作。</li><li>支持横向扩展。</li><li>服务发现方面，我们做了一些定制。</li></ul><h3 id="4-6-如何保证一致性？"><a href="#4-6-如何保证一致性？" class="headerlink" title="4.6 如何保证一致性？"></a>4.6 如何保证一致性？</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831140935924.png" alt="数据一致性问题"></p><ul><li>采用 Final 进行查询，等待数据合并完成。</li><li>在数据方面的话，实现幂等性，保证唯一性，通过主键排序，整理出来一组数据，再写入。</li><li>写入异常时就及时修复和回填，保证最终一致性。</li></ul><h3 id="4-7-监控"><a href="#4-7-监控" class="headerlink" title="4.7 监控"></a>4.7 监控</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831141040118.png" alt="监控"></p><p>使用 Prometheus 作为监控工具。使用方便，成本较低。</p><h2 id="五、未来展望"><a href="#五、未来展望" class="headerlink" title="五、未来展望"></a>五、未来展望</h2><h3 id="5-1-从-1-到-2"><a href="#5-1-从-1-到-2" class="headerlink" title="5.1 从 1 到 2"></a>5.1 从 1 到 2</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831141110039.png" alt="未来展望"></p><ul><li>扩展更多的业务和数据。之前我们的业务模式比较单一，只有数据方面的统计，之后会挖掘更多信息，包括链上追踪，金融方面的审计。</li><li>赚更多的钱，尽可能的活下去，我们才能去做更多的事情，去探索更多的盈利模式。</li><li>跟进 Flink 和 PyFlink 的生态，积极参与开源的工作，优化相关作业。探索多 sink 方面的工作，原生 Kubernetes 的实践。</li></ul><h3 id="5-2-从-2-到-3"><a href="#5-2-从-2-到-3" class="headerlink" title="5.2 从 2 到 3"></a>5.2 从 2 到 3</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831141240635.png" alt="展望未来"></p><ul><li>数据建模的规范，规定手段，操作。</li><li>Flink 和机器学习相结合。</li><li>争取拿到实时在线训练的业务，Flink 做实时监控，是非常不错的选择。大公司都已经有相关的实践。包括报警等操作。</li></ul><p>总的来说的话，路漫漫其修远兮，使用 Flink 真不错。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的生命周期怎么会用到这些</title>
      <link href="/2021/08/31/Flink%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%80%8E%E4%B9%88%E4%BC%9A%E7%94%A8%E5%88%B0%E8%BF%99%E4%BA%9B/"/>
      <url>/2021/08/31/Flink%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%80%8E%E4%B9%88%E4%BC%9A%E7%94%A8%E5%88%B0%E8%BF%99%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink的生命周期怎么会用到这些"><a href="#Flink的生命周期怎么会用到这些" class="headerlink" title="Flink的生命周期怎么会用到这些"></a>Flink的生命周期怎么会用到这些</h1><p> Flink API提供了开发的接口，此外，为了实现业务逻辑，还必须为开发者提供自定义业务逻辑的能力。。Flink中设计了用户自定义函数体系(User Defined Function,UDF),开发人员实现业务逻辑就是开发UDF。</p><p>一、环境对象</p><p>​    StreamExecutionEnvironment是Flink应用开发时的概念，表示流计算作业的执行环境，是作业开发的入口、数据源接口、生成和转换DataStream的接口、数据Sink的接口、作业配置接口、作业启动执行的入口。</p><p>​    Environment是运行时作业级别的概念，从StreamExecutionEnvironment中的配置信息衍生而来。进入到Flink作业执行的时刻，作业需要的是相关的配置信息，如作业的名称、并行度、作业编号JobID、监控的Metric、容错的配置信息、IO等，用StreamExecutionRuntime对象就不适合了，很多API是不需要的，所以在Flink中抽象出了Environment作为运行时刻的上下文信息。</p><p>​    RuntimeContext是运行时Task实例级别的概念。Environment本身仍然是比较粗粒度作业级别的配置，对于每一个Task而言，其本身有更细节的配置信息,所以Flink又抽象了RuntimeContext,每一个Task实例有自己的RuntimeContext。</p><h2 id="1-1-执行环境"><a href="#1-1-执行环境" class="headerlink" title="1.1 执行环境"></a>1.1 执行环境</h2><p>StreamExecutionEnvironment是Flink流计算应用的执行环境，是Flink作业开发和启动执行的入口，开发者对StreamExecutionEnvironment的实现是无感知的。</p><ul><li>LocalStreamEnvironment</li></ul><p>​    本地执行环境，在单个JVM中使用多线程模拟Flink集群。</p><p>​    其基本的工作流程如下：</p><p>​    \1) 执行Flink作业的Main函数生成Streamgraph,转化为JobGraph。</p><p>​    \2) 设置任务运行的配置信息。</p><p>​    \3) 根据配置信息启动对应的LocalFlinkMiniCluster。</p><p>​    \4) 根据配置信息和miniCluster生成对应的MiniClusterClient。</p><p>​    \5) 通过MiniClusterClient提交JobGraph 到MiniCluster。</p><ul><li>RemoteStreamEnvironment</li></ul><p>​    在大规模数据中心中部署的Flink生产集群的执行环境。</p><p>​    当将作业发布到Flink集群的时候，使用RemoteStreamEnvironment。</p><p>​    其基本的工作流程如下：</p><p>​    \1) 执行Flink作业的Main函数生成Streamgraph，转化为JobGraph。</p><p>​    \2) 设置任务运行的配置信息。</p><p>​    \3) 提交JobGraph到远程的Flink集群。</p><ul><li>StreamContextEnvironment</li></ul><p>​    在Cli命令行或者单元测试时候会被使用，执行步骤同上。</p><ul><li>StreamPlanEnvironment</li></ul><p>​    在Flink Web UI管理界面中可视化展现Job的时候，专门用来生成执行计划(实际上就是StreamGraph)</p><ul><li>ScalaShellStreamEnvironment</li></ul><p>​    这是Scala Shell执行环境，可以在命令行中交互式开发Flink作业。</p><p>​    其基本工作流程如下：</p><p>​    \1) 校验部署模式，目前Scala Shell仅支持attached模式。</p><p>​    \2) 上传每个作业需要的Jar文件。</p><p>​    其余步骤与RemoteStreamEnvironment类似。</p><h2 id="1-2-运行时环境"><a href="#1-2-运行时环境" class="headerlink" title="1.2 运行时环境"></a>1.2 运行时环境</h2><ul><li>RuntimeEnvironment</li></ul><p>​    在Task开始执行时进行初始化，把Task运行相关的信息都封装到该对象中，其中不光包含了配置信息，运行时的各种服务也会被包装到其中。</p><ul><li>SavepointEnvironment</li></ul><p>​    SavepointEnvironment是Environment的最小化实现，在状态处理器的API中使用。</p><h2 id="1-3-运行时上下文"><a href="#1-3-运行时上下文" class="headerlink" title="1.3 运行时上下文"></a>1.3 运行时上下文</h2><p>​    RuntimeContext是Function运行时的上下文，封装了Function运行时可能需要的所有信息，让Function在运行时能够获取到作业级别的信息，如并行度相关信息、Task名称、执行配置信息(ExecutionConfig)、State等。</p><p>​     Function的每个实例都有一个RuntimeContext对象，在RichFunction中通过getRunctionContext()可以访问该对象。</p><p>​    RuntimeContext的类体系图如下:</p><ul><li>StreamingRuntimeContext：在流计算UDF中使用的上下文，用来访问作业信息、状态等。</li><li>DistributedRuntimeUDFContext：由运行时UDF所在的批处理算子创建，在DataSet批处理中使用。</li><li>RuntimeUDFContext：在批处理应用的UDF中使用。</li><li>SavepointRuntimeContext：支持对检查点和保存点进行操作，包括读取、变更、写入等。</li><li>CepRuntimeContext：CEP复杂事件处理中使用的上下文。</li></ul><h1 id="二、数据流元素"><a href="#二、数据流元素" class="headerlink" title="二、数据流元素"></a>二、数据流元素</h1><p>  数据流元素在Flink中叫做StreamElement，有数据记录StreamRecord,延迟标记LatencyMarker、Watermark、流状态标记StreamStatus这四种。在执行层面，4种数据流元素都被序列化成二进制数据，形成混合的数据流，在算子中将混合数据流中的数据流元素反序列化出来。</p><ul><li> StreamRecord</li></ul><p>​    StreamRecord表示数据流中的一条记录(或者叫做一个事件)，也叫数据记录。</p><p>​    包含以下内容：</p><p>​    1）数据的值本身</p><p>​    2）时间戳(可选)</p><ul><li>LatencyMarker</li></ul><p>​    用来近似评估延迟，LatencyMarker在Source中创建，并向下游发送，绕过业务处理逻辑，在Sink节点中使用LatencyMarker估计数据在整个DAG图中的流转花费的时间。</p><p>​     LatencyMarker包含信息如下：</p><p>​    1）周期性的在数据源算子中创造出来的时间戳。</p><p>​    2）算子编号</p><p>​    3）数据源算子所在的Task编号</p><ul><li>Watermark</li></ul><p>​    是一个时间戳，用来告诉算子所有时间早于等于Watermark的事件或记录都已经达到，不会再有比Watermark更早的记录。</p><ul><li>StreamStatus</li></ul><p>​    用来通知Task是否会继续接收到上游的记录或者Watermark。在数据源算子中生成，向下游沿着DataFlow传递。</p><p>​    有两种表示状态:</p><p>​    1）空闲状态(IDLE)</p><p>​    2）活动状态(ACTIVE) </p><h1 id="三、数据转换"><a href="#三、数据转换" class="headerlink" title="三、数据转换"></a>三、数据转换</h1><p>​    数据转换在Flink中叫做Transformation,是衔接DataStream Api和Flink内核的逻辑结构。</p><p>​    Transformation有两大类：</p><ul><li>物理Transformation:会转换成算子，继承了PhysicalTransformation。</li><li>虚拟Transformation: 不会转换成具体算子。</li></ul><p>​    Tranformation包含了Flink的运行时关键参数:</p><p>​    1）name：转换器名称，主要用于可视化。</p><p>​    2）uid：用户指定的uid,该uid的主要目的是在job重启时再次分配跟之前相同的uid,可以持久保存状态。</p><p>​    3）bufferTimeout：buffer超时时间。</p><p>​    4）parallelism：并行度。</p><p>​    5）id：跟属性uid无关，生成方式是基于一个静态累加器。</p><p>​    6）outputType：输出类型，用来进行序列化数据。</p><p>​    7）slotSharingGroup：给当前的Transformation设置Slot共享组。</p><h2 id="3-1-物理Transformation"><a href="#3-1-物理Transformation" class="headerlink" title="3.1 物理Transformation"></a>3.1 物理Transformation</h2><ul><li>SourceTransformation</li></ul><p>​    从数据源读取数据的Transformation，是Flink作业的起点。只有下游Transformation，没有上游输入。</p><ul><li>SinkTransformation</li></ul><p>​    将数据写到外部存储的Transformation,是Flink作业的终点。</p><ul><li>OneInputTransformation</li></ul><p>​    单流输入的Transformation(只接收一个输入流)，跟上面的SinkTransformation构造器类似，同样需要input和operator参数。</p><ul><li>TwoInputTransformation</li></ul><p>​    双输入的Transformation(接收两种流作为输入)，分别叫做第一输入和第二输入。</p><h2 id="3-2-虚拟Transformation"><a href="#3-2-虚拟Transformation" class="headerlink" title="3.2 虚拟Transformation"></a>3.2 虚拟Transformation</h2><ul><li>SideOutputTransformation</li></ul><p>​    在旁路输出中转换而来，表示上游Transformation的一个分流。每个sideoutput通过OutputTag标识。</p><ul><li>SplitTransformation</li></ul><p>​    用来按条件切分数据流，该转换用于将一个流拆分成多个流。</p><ul><li>SelectTransformation</li></ul><p>​    与SplitTransformation配合使用，用来在下游选择SplitTransformation切分的数据流。</p><ul><li>PartitionTransformation</li></ul><p>​    该转换器用于改变输入元素的分区，其名称为Partition。工作时除了提供一个StreamTransformation作为输入外，还需要提供一个StreamPartitionor的实例来进行分区。</p><ul><li>UnionTransformation</li></ul><p>​    合并转换器，该转换器用于将多个输入StreamTransformation进行合并，因此该转换器接收StreamTransformation的集合。Union要求上游输入的数据的结构必须是完全相同的。</p><ul><li>FeedbackTransformation</li></ul><p>​    表示FlinkDAG中的一个反馈点。简单来说，就是把符合条件的数据发回上游Transformation处理，一个反馈点可以连接一个或多个上游的Transformation,这些连接关系叫反馈边。符合反馈条件并交给上游的Transformation的数据流叫做反馈流。</p><p>​    FeedbackTransformation的固定名称为Feedback，有两个重要参数：</p><p>​    1）input：上游输入StreamTransformation</p><p>​    2）waitTime：默认为0，即永远等待，如果设置了等待时间，一旦超过该等待时间，则计算结束并且不再接收数据。</p><p>​    实例化FeedbackTransformation时，会自动创建一个用于存储反馈边的集合feedbackEdges。FeedbackTransformation通过定义一个实力方法addFeedbackEdge来收集，在加入的StreamTransformation的实例有一个要求，当前FeedbackTransformation跟待加入的StreamTransformation并行度一致。</p><ul><li>CoFeedbackTransformation</li></ul><p>​    与FeedbackTransformation类似，也是FlinkDAG中的一个反馈点。不同之处在于，CoFeedbackTransformation反馈给上游的数据流与上游Transformation的输入类型不同，所以要求上游的Transformation必须是TwoInputTransformation。</p><h1 id="四、算子行为"><a href="#四、算子行为" class="headerlink" title="四、算子行为"></a>四、算子行为</h1><h2 id="4-1-生命周期管理"><a href="#4-1-生命周期管理" class="headerlink" title="4.1 生命周期管理"></a>4.1 生命周期管理</h2><p>​    1）setup：初始化环境、时间服务、注册监控等。</p><p>​    2）open：该行为由各个具体的算子负责实现，包含了算子的初始化逻辑。</p><p>​    3）close：所有的数据处理完毕之后关闭算子，此时需要去报将所有的缓存数据向下游发送。</p><p>​    4）dispose：该方法在算子生命周期的最后执行阶段，此时算子已经关闭，停止处理数据，进行资源的释放。</p><p>​    StreamTask作为算子的容器，负责管理算子的生命周期。</p><h2 id="4-2-异步算子"><a href="#4-2-异步算子" class="headerlink" title="4.2 异步算子"></a>4.2 异步算子</h2><p> 异步算子的目的是解决与外部系统交互时网络延迟所导致的系统瓶颈问题。</p><p>​    异步算子的两种输出模式</p><p>​    1）顺序输出</p><p>​        先收到的数据先输出，后续数据元素的异步函数调用无论是否先完成，都需要等待，顺序模式可以保证消息不乱序，但是可能增加延迟，降低算子的吞吐量。</p><p>​    2）无序输出</p><p>​        先处理完的数据元素先输出，不保证消息顺序，相比于顺序模式，无序输出模式算子延迟低、吞吐量高。无序输出模式并不是完全无序的，仍然要保持Watermark不能超越其前面数据元素的原则。等待完成队列将按照Watermakr切分成组，组内可以无序输出，组之间必须严格保证顺序。</p><h1 id="五、处理函数"><a href="#五、处理函数" class="headerlink" title="五、处理函数"></a>五、处理函数</h1><h2 id="5-1-双流Join"><a href="#5-1-双流Join" class="headerlink" title="5.1 双流Join"></a>5.1 双流Join</h2><ul><li>即时Join</li></ul><p>​    逻辑如下:</p><p>​    \1) 创建一个State对象</p><p>​    2）接收到输入流 1事件后更新Sate。</p><p>​    3）接收到输出流 2的事件后遍历State，根据Join条件进行匹配，将匹配结果发送到下游。</p><ul><li>延迟双流Join</li></ul><p>​    在流式数据里，数据可能是乱序的，数据会延迟到达，并且为了提供处理效率，使用小批量模式计算，而不是每个事件触发一次Join计算。</p><p>​    逻辑如下:</p><p>​    1）创建2个state对象，分别缓存输入流1和输入流2的事件。</p><p>​    2）创建一个定时器，等待数据的到达，定时延迟触发Join计算。</p><p>​    3）接收到输入流1事件后更新State。</p><p>​    4）接收到输入流2事件后更新State。</p><p>​    5）定时器遍历State1和State2，根据Join条件进行匹配，将匹配结果发送到下游。</p><h1 id="六、数据分区"><a href="#六、数据分区" class="headerlink" title="六、数据分区"></a>六、数据分区</h1><p> 数据分区在Flink中叫做Partition。本质上说，分布式计算就是把一个作业切分成子任务Task，将不同的数据交给不同的Task计算。StreamParitioner是Flink中的数据流分区抽象接口，决定了在实际运行中的数据流分发模式。</p><ul><li>自定义分区</li></ul><p>​    使用用户自定义分区函数，为每一个元组选择目标分区。</p><ul><li>ForwardParitioner</li></ul><p>​    用于在同一个OperatorChain中上下游算子之间的数据转发， 实际上数据是直接传递给下游的。</p><ul><li>ShufflePartitioner</li></ul><p>​    随机将元素进行分区，可以确保下游的Task能够均匀的获取数据。</p><ul><li>ReblancePartitioner</li></ul><p>​    以Round-robin的方式为每个元素分配分区，确保下游的Task可以均匀的获取数据，以免数据倾斜。</p><ul><li>RescalingPartitioner</li></ul><p>​    根据上下游Task的数据进行分区。使用Round-robin选择下游的一个Task进行数据分区，如上游有2个Source,下游有6个Map,那么每个Source会分配3个固定下游的map,不会向未分配给自己的分区写入数据。</p><ul><li>BroadcastPartitioner</li></ul><p>​    将该记录广播给所有分区，即有N个分区，就把数据复制N份，每个分区1份。</p><ul><li>KeyGroupStreamPartitioner</li></ul><p>​    keyedStream根据KeyGroup索引编号进行分区，该分区器不是提供给用户来用。KeyedStream在构造Transformation的时候默认使用KeyedGroup分区形式，从而在底层上支持作业Rescale功能。</p><h1 id="七、分布式ID"><a href="#七、分布式ID" class="headerlink" title="七、分布式ID"></a>七、分布式ID</h1>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实时数仓数据接口展示</title>
      <link href="/2021/08/31/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3%E5%B1%95%E7%A4%BA/"/>
      <url>/2021/08/31/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3%E5%B1%95%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="实时数仓数据接口展示"><a href="#实时数仓数据接口展示" class="headerlink" title="实时数仓数据接口展示"></a>实时数仓数据接口展示</h1><h2 id="学习总结"><a href="#学习总结" class="headerlink" title="学习总结"></a>学习总结</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">数据接口是我们在对接前端的时候，使用Spring</span> <span class="token attr-value">Boot的方式来对接SQL最终通过接口的方式来进行可视化</span>目前市场上常用的数据可视化的工具有quickBI，tableau，Superset，Sugar但是Sugar是收费的，所以我们要熟练的去写这种数据接口，但是不是重点，但是必须得会，如果以后工作当中的30%的时间都是在进行写数据接口的话，立马离职，因为这个会影响我以后的发展生涯使用sugar的时候必须得利用一个内网穿透得工具，使得局域网里面得IP能够被外界进行访问，我使用得网云穿直接注册实名认证就行还需要在sugar里面配置一些域名得信息来简化开发只需要记住格式来进行暂时就好了</code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210831111147707.png" alt="实时数仓项目"></p><h2 id="数据接口模式规范"><a href="#数据接口模式规范" class="headerlink" title="数据接口模式规范"></a>数据接口模式规范</h2><table><thead><tr><th><strong>分层</strong></th><th><strong>类</strong></th><th><strong>处理内容</strong></th></tr></thead><tbody><tr><td><strong>controller</strong>   <strong>控制层</strong></td><td>SugarController</td><td>查询交易额接口及返回参数处理</td></tr><tr><td><strong>service</strong>    <strong>服务层</strong></td><td>ProductStatsService  ProductStatsServiceImpl</td><td>查询商品统计数据</td></tr><tr><td><strong>mapper</strong>   <strong>数据映射层</strong></td><td>ProductStatsMapper</td><td>编写SQL查询商品统计表</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 企业级数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 企业级数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-10-值班安排</title>
      <link href="/2021/08/30/0002-10-%E5%80%BC%E7%8F%AD%E5%AE%89%E6%8E%92/"/>
      <url>/2021/08/30/0002-10-%E5%80%BC%E7%8F%AD%E5%AE%89%E6%8E%92/</url>
      
        <content type="html"><![CDATA[<h1 id="一-排班信息"><a href="#一-排班信息" class="headerlink" title="一 排班信息"></a>一 排班信息</h1><h1 id="二-值班周期"><a href="#二-值班周期" class="headerlink" title="二 值班周期"></a>二 值班周期</h1><h1 id="三-任务延迟处理方式"><a href="#三-任务延迟处理方式" class="headerlink" title="三 任务延迟处理方式"></a>三 任务延迟处理方式</h1><h1 id="四-任务报错处理方式"><a href="#四-任务报错处理方式" class="headerlink" title="四 任务报错处理方式"></a>四 任务报错处理方式</h1><h1 id="五-告警机制"><a href="#五-告警机制" class="headerlink" title="五 告警机制"></a>五 告警机制</h1><h1 id="六-责任划分"><a href="#六-责任划分" class="headerlink" title="六 责任划分"></a>六 责任划分</h1><h1 id="七-起夜率考核"><a href="#七-起夜率考核" class="headerlink" title="七 起夜率考核"></a>七 起夜率考核</h1><h1 id="八-重要任务最晚产出时间列表"><a href="#八-重要任务最晚产出时间列表" class="headerlink" title="八 重要任务最晚产出时间列表"></a>八 重要任务最晚产出时间列表</h1>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-9-调度系统</title>
      <link href="/2021/08/30/0002-9-%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F/"/>
      <url>/2021/08/30/0002-9-%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-9-调度系统"><a href="#0002-9-调度系统" class="headerlink" title="0002-9-调度系统"></a>0002-9-调度系统</h1><h1 id="一-责任人"><a href="#一-责任人" class="headerlink" title="一 责任人"></a>一 责任人</h1><h1 id="二-输出表名"><a href="#二-输出表名" class="headerlink" title="二 输出表名"></a>二 输出表名</h1><h1 id="三-调度周期"><a href="#三-调度周期" class="headerlink" title="三 调度周期"></a>三 调度周期</h1><h1 id="四-加工方式"><a href="#四-加工方式" class="headerlink" title="四 加工方式"></a>四 加工方式</h1><h1 id="五-任务名称"><a href="#五-任务名称" class="headerlink" title="五 任务名称"></a>五 任务名称</h1><h1 id="六-优先级"><a href="#六-优先级" class="headerlink" title="六 优先级"></a>六 优先级</h1><h1 id="七-依赖属性"><a href="#七-依赖属性" class="headerlink" title="七 依赖属性"></a>七 依赖属性</h1><h1 id="八-版型信息"><a href="#八-版型信息" class="headerlink" title="八 版型信息"></a>八 版型信息</h1>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-8-缓慢变化为解决方案</title>
      <link href="/2021/08/30/0002-8-%E7%BC%93%E6%85%A2%E5%8F%98%E5%8C%96%E4%B8%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2021/08/30/0002-8-%E7%BC%93%E6%85%A2%E5%8F%98%E5%8C%96%E4%B8%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-8-缓慢变化为解决方案"><a href="#0002-8-缓慢变化为解决方案" class="headerlink" title="0002-8-缓慢变化为解决方案"></a>0002-8-缓慢变化为解决方案</h1><h1 id="一-背景"><a href="#一-背景" class="headerlink" title="一 背景"></a>一 背景</h1><p>  众所周知，虽然维度表属性相对稳定，但是并不是一成不变的，尽管相当缓慢，维度值仍会随时间而变化。比如商品类目的改变，医院等级的改变</p><p>在一些情况下，保留历史数据没有什么分析价值，而在另一些情况下,保留历史数据是非常重要的</p><h1 id="二-解决方案"><a href="#二-解决方案" class="headerlink" title="二 解决方案"></a>二 解决方案</h1><h2 id="2-1-重写维度值"><a href="#2-1-重写维度值" class="headerlink" title="2.1 重写维度值"></a>2.1 重写维度值</h2><p>在维度表中，仅需以当前值重写先前存在的值，不需要触碰事实表</p><p>缺点：如果业务需要准确的跟踪历史变化，这种方案是没法实现的，并且在以后想改变是非常困难的</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180059221.png" alt="image-20210830180059221"></p><p>修改后：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180111798.png" alt="image-20210830180111798"></p><h2 id="2-2-插入新的维度行"><a href="#2-2-插入新的维度行" class="headerlink" title="2.2 插入新的维度行"></a>2.2 插入新的维度行</h2><p>插入新的维度行。采用此种方式，保留历史数据，</p><p>维度值变化前的事实和过去的维度值关联，维度值变化后的事实和当前的维度值关联</p><p>缺点：虽然此方案能够区分历史情况，但是该方式不能将变化前后记录的事实归一为变化前的维度或者归一为变化后的维度</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180140766.png" alt="image-20210830180140766"></p><h2 id="2-3-添加维度列"><a href="#2-3-添加维度列" class="headerlink" title="2.3 添加维度列"></a>2.3 添加维度列</h2><p>有些是只保留最新的维度值和最近的维度值，也有的是维度值一有变化就新增一个属性字段。都不是很好的解决方案</p><p>变化前：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180153135.png" alt="image-20210830180153135"></p><p>变化后：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180202319.png" alt="image-20210830180202319"></p><h2 id="2-4-拉链表处理"><a href="#2-4-拉链表处理" class="headerlink" title="2.4 拉链表处理"></a>2.4 拉链表处理</h2><p>这是精确跟踪缓慢变化维度属性的主要技术，因为新维度行能够自动划分事实表的历史，所以这是一项非常好的技术</p><p>变化前：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180215495.png" alt="image-20210830180215495"></p><p>变化后：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180226056.png" alt="image-20210830180226056"></p>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-7-多值维度与多值属性交叉维度</title>
      <link href="/2021/08/30/0002-7-%E5%A4%9A%E5%80%BC%E7%BB%B4%E5%BA%A6%E4%B8%8E%E5%A4%9A%E5%80%BC%E5%B1%9E%E6%80%A7%E4%BA%A4%E5%8F%89%E7%BB%B4%E5%BA%A6/"/>
      <url>/2021/08/30/0002-7-%E5%A4%9A%E5%80%BC%E7%BB%B4%E5%BA%A6%E4%B8%8E%E5%A4%9A%E5%80%BC%E5%B1%9E%E6%80%A7%E4%BA%A4%E5%8F%89%E7%BB%B4%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-7-多值维度与多值属性交叉维度"><a href="#0002-7-多值维度与多值属性交叉维度" class="headerlink" title="0002-7-多值维度与多值属性交叉维度"></a>0002-7-多值维度与多值属性交叉维度</h1><h1 id="一-背景"><a href="#一-背景" class="headerlink" title="一 背景"></a>一 背景</h1><p>正常情况下，维表和事实表之间是一对多的关系，维表中的一行记录会连接事实表中的多行记录，事实表中的一行记录在维度表中只能关联上一条记录，不会发生数据发散的现象</p><p>想法是美好的，但是事实总是不尽人意。因为现实中不但事实表和维度表之间存在多对多的关系，维度表和维度表之间也存在多对多的关系</p><p>这两种情况本质是相同的，但事实表和维度表之间的多对多关系少了唯一描述事实和维度组的中间维度。</p><p>对于这两种情况，一种称为桥接表的中间表就需要派上用场了，并且还可以支持更为复杂的多对多的关系</p><h1 id="二-事实表与维度表多对多-多值维度"><a href="#二-事实表与维度表多对多-多值维度" class="headerlink" title="二 事实表与维度表多对多(多值维度)"></a>二 事实表与维度表多对多(多值维度)</h1><p>比如下单了一套学习课程，但是这套课程并不是某一个用户买的，而是好几个用户合买的，所以为了处理这种情况，需要创建一个桥接表，将这些合买的用户组成一个组</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1626356999496-110166ba-7da4-4997-afcd-f14a0bb044a9.png" alt="img"></p><p>ETL过程需要对每条事实表中的用户组，在桥接表中查找相应的用户主键,上图所示的桥接表有重复计数的风险。如果按用户累加购买金额，对某些分析而言结果是正确的，但对于其他情况仍会有重复计数的问题。要解决这个问题，可以向桥接表中添加权重。    </p><p>权重是一个分数值，所有的用户组的权重累加起来为1。将权重和累加事实相乘，以按照每个用户在分组中的比重分配事实。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1626357010802-a409596e-bc06-4a8b-8c36-4cbc963d75e0.png" alt="img"></p><p>优点：</p><p>灵活简化了生成报表的难度</p><p>借权重避免了多重计算</p><p>缺点：</p><p>桥接表的维护比较复杂，当出现一个新组合时，得先判断桥接表中是否已存在</p><h1 id="三-维表与维表多对多-交叉维度"><a href="#三-维表与维表多对多-交叉维度" class="headerlink" title="三 维表与维表多对多(交叉维度)"></a>三 维表与维表多对多(交叉维度)</h1><p>从分析的角度来看，维度之间的多对多关系是一个很重要的概念，大多数维度都不是完全相互独立的。</p><p>在银行系统中，账户和顾客之间有直接关系，但不是一对一的关系。一个账户可以有一个或多个签名确认的用户，一个用户也可有多个账户</p><p>有2种方案解决</p><p>和多值维度一样，创建账户-用户组桥接表来连接事实表</p><p>还有一种方法是利用账户和用户之间的关系，如下图</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830180003222.png" alt="image-20210830180003222"></p><p>桥接表可以捕获多对多关系，并且由于源系统中的关系是已知的，因此创建桥接表比多值维度手动构建维度表(桥接表)更容易</p><h1 id="四-总结"><a href="#四-总结" class="headerlink" title="四 总结"></a>四 总结</h1><p>处理多值维度最好的办法是降低事实表的粒度。这种处理方式也是维度建模的一个原则，即事实表应该建立在最细粒度上。这样的处理，需要对事实表的事实进行分摊。</p><p>但是有些时候，事实表的粒度是不能降低的，多值维度的出现是无法避免的。如上述交叉维度，事实表与用户维度没有直接的关系，不能将数据粒度进行细分，即使细分的话帐户余额也很难分摊。这时，可以采用桥接表技术进行处理。在帐户维度表和用户维度表之间建立个帐户-用户桥接表。这个桥接表可以解决掉帐户维度和用户维度之间的多对多关系，也解决掉的帐户维度表的多值维度问题。</p><p>总之，多值维度是应该尽量避免的，它给数据处理带来了很大的麻烦。如果多值维度不能避免的话，应该建立桥接表来进行处理。</p>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-6-指标数据一致性</title>
      <link href="/2021/08/30/0002-6-%E6%8C%87%E6%A0%87%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"/>
      <url>/2021/08/30/0002-6-%E6%8C%87%E6%A0%87%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-6-指标数据一致性"><a href="#0002-6-指标数据一致性" class="headerlink" title="0002-6-指标数据一致性"></a>0002-6-指标数据一致性</h1><h1 id="一-事件背景"><a href="#一-事件背景" class="headerlink" title="一 事件背景"></a>一 事件背景</h1><p>业务方反馈：</p><p>1.同样的指标取自2张表，结果不一样</p><p>2.同样的指标，取自同一张表，但是是2个需求，指标口径不统一</p><p>3.同一个指标，命名不一样，导致重复计算</p><p>4.不同的两个指标，命名一样，导致产生误解</p><h1 id="二-目标"><a href="#二-目标" class="headerlink" title="二 目标"></a>二 目标</h1><p>从设计、开发、部署和使用层面，避免重复建设和指标冗余建设，从而保障数据口径的规范和统一，最终实现数据资产全链路关联、提供标准数据输出以及建立统一的数据公共层。</p><h1 id="三-解决方案"><a href="#三-解决方案" class="headerlink" title="三 解决方案"></a>三 解决方案</h1><h2 id="3-1-统一定义"><a href="#3-1-统一定义" class="headerlink" title="3.1 统一定义"></a>3.1 统一定义</h2><ul><li><p>指标定义一致性</p></li><li><p>数据来源一致性</p></li><li><p>统计口径一致性</p></li><li><p>维度一致性</p></li><li><p>维度和指标数据出口唯一</p></li></ul><h2 id="3-2-词根规范"><a href="#3-2-词根规范" class="headerlink" title="3.2 词根规范"></a>3.2 词根规范</h2><p>词根评审</p><p>需要新增一个词根的时候，需要部门评审，看看是否有必要新增，并且如果确定下来需要新增的话如何命名</p><p>如 cnt 代表的意思是count  数量，如果之前词根里面没有的话，理论上来说，新增该词根是没问题的</p><p>词根大全</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175948044.png" alt="image-20210830175948044"></p><h2 id="3-3-指标规范"><a href="#3-3-指标规范" class="headerlink" title="3.3 指标规范"></a>3.3 指标规范</h2><p>指标评审及定义</p><p>每定一个指标都是需要业务方(或其他部门)，运营人员与数据部门一起评审决定的，包括指标是否有必要新增，如何定义等</p><p>现列举一些常用流量指标：</p><ul><li><p>日活跃度=当日启动用户/累计用户*100%</p></li><li><p>周活跃度=周活跃用户/累计用户*100%</p></li><li><p>月活跃度=月活跃用户/累计用户*100%</p></li><li><p>页面访问次数：页面被打开的次数，同一页面的多次访问均会被计数。</p></li><li><p>页面平均停留时长：每一次页面访问的停留时长的平均值</p></li></ul><p>指标命名规范</p><p><strong>基础指标</strong></p><p>主要是指不能再拆解的指标，通常表达业务实体原子量化属性的且不可再分的概念集合，如订单数</p><p>单个基础指标词根+修饰词</p><p><strong>复合指标</strong></p><p>建立在基础指标之上，通过一定运算规则形成的计算指标集合，如人均费用=总费用/人数</p><p><strong>派生指标</strong></p><p>指的是基础指标或复合指标与维度成员，统计属性，管理属性等相结合产生的指标，如最近7天医生接单量=医生在过去7天一共接到的订单</p><p>多个基础指标词根+修饰词 </p><h2 id="3-4-标准建模"><a href="#3-4-标准建模" class="headerlink" title="3.4 标准建模"></a>3.4 标准建模</h2><p>建立数据公共层对模型架构进行标准规范设计和管理</p><h2 id="3-5-规范研发"><a href="#3-5-规范研发" class="headerlink" title="3.5 规范研发"></a>3.5 规范研发</h2><p>整个开发过程需要严格遵守开发规范</p><h2 id="3-6-工具保障"><a href="#3-6-工具保障" class="headerlink" title="3.6 工具保障"></a>3.6 工具保障</h2><h3 id="3-6-1-指标管理平台-指标库"><a href="#3-6-1-指标管理平台-指标库" class="headerlink" title="3.6.1 指标管理平台(指标库)"></a>3.6.1 指标管理平台(指标库)</h3><p>设计原则</p><ul><li><p>指标口径一致性</p></li><li><p>使用便捷性</p></li><li><p>数据处理高性能，以及智能化</p></li><li><p>开发维护高效性</p></li></ul><p>指标体系</p><p>指标分层级</p><p>一级分类：投资类，运营类等</p><p>二级分类：</p><p>三级分类：</p><p>待定</p><p>可以通过自研WEB系统来进行展示</p><p>展示内容可以有：</p><ul><li><p>指标编码</p></li><li><p>指标名称</p></li><li><p>业务口径</p></li><li><p>指标类型</p></li><li><p>存储的表</p></li><li><p>责任人</p></li><li><p>创建时间</p></li><li><p>状态</p></li><li><p>……</p></li></ul><h3 id="3-6-2-程序稽核保障"><a href="#3-6-2-程序稽核保障" class="headerlink" title="3.6.2 程序稽核保障"></a>3.6.2 程序稽核保障</h3><p>开发程序对指标进行监控</p><h1 id="四-数据交付"><a href="#四-数据交付" class="headerlink" title="四 数据交付"></a>四 数据交付</h1><p>数据交付标准化</p>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-5-数据同步策略</title>
      <link href="/2021/08/30/0002-5-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/"/>
      <url>/2021/08/30/0002-5-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-5-数据同步策略"><a href="#0002-5-数据同步策略" class="headerlink" title="0002-5-数据同步策略"></a>0002-5-数据同步策略</h1><h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175504552.png" alt="image-20210830175504552"></h1><h1 id="一-mysql数据准备"><a href="#一-mysql数据准备" class="headerlink" title="一 mysql数据准备"></a>一 mysql数据准备</h1><p>第一天 9月10号数据</p><pre class=" language-sql"><code class="language-sql"><span class="token number">1</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token number">2</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">14</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">14</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token number">3</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">16</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">16</span>:<span class="token number">20</span>:<span class="token number">11</span></code></pre><p>第二天 9月11号数据</p><pre class=" language-sql"><code class="language-sql"><span class="token number">1</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token number">2</span><span class="token punctuation">,</span>已支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">14</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">14</span>:<span class="token number">21</span>:<span class="token number">11</span><span class="token number">3</span><span class="token punctuation">,</span>已支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span> <span class="token number">16</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">16</span>:<span class="token number">21</span>:<span class="token number">11</span><span class="token number">4</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">12</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token number">5</span><span class="token punctuation">,</span>待支付<span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">14</span>:<span class="token number">20</span>:<span class="token number">11</span><span class="token punctuation">,</span><span class="token number">2020</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">11</span> <span class="token number">14</span>:<span class="token number">20</span>:<span class="token number">11</span></code></pre><p>对比mysql第一天和第二天的数据发现，第二天新增了订单id为4和5这两条数据，并且订单id为2和3的状态更新为了已支付</p><h1 id="二-全量表"><a href="#二-全量表" class="headerlink" title="二 全量表"></a>二 全量表</h1><p>每天的所有的最新状态的数据。</p><p>1、全量表，有无变化，都要报</p><p>2、每次上报的数据都是所有的数据（变化的 + 没有变化的）</p><p>9月10号全量抽取到ods层</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> wedw_ods<span class="token punctuation">.</span>order_info_20200910<span class="token punctuation">(</span> order_id     string    <span class="token keyword">COMMENT</span> <span class="token string">'订单id'</span><span class="token punctuation">,</span>order_status string    <span class="token keyword">COMMENT</span> <span class="token string">'订单状态'</span><span class="token punctuation">,</span>create_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>update_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'更新时间'</span><span class="token punctuation">)</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单表'</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175536168.png" alt="image-20210830175536168"></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_df<span class="token punctuation">(</span> order_id     string    <span class="token keyword">COMMENT</span> <span class="token string">'订单id'</span><span class="token punctuation">,</span>order_status string    <span class="token keyword">COMMENT</span> <span class="token string">'订单状态'</span><span class="token punctuation">,</span>create_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>update_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'更新时间'</span><span class="token punctuation">)</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单表'</span>partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>date_id string<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true"># 把wedw_ods.order_info_20200910数据全量插到dwd层2020-09-10分区</span><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_df <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span><span class="token punctuation">)</span><span class="token keyword">select</span>order_id<span class="token punctuation">,</span>order_status<span class="token punctuation">,</span>create_time<span class="token punctuation">,</span>update_time<span class="token keyword">from</span> wedw_ods<span class="token punctuation">.</span>order_info_20200910<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175549857.png" alt="image-20210830175549857"></p><p>9月11号全量抽取到ods层</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> wedw_ods<span class="token punctuation">.</span>order_info_20200911<span class="token punctuation">(</span> order_id     string    <span class="token keyword">COMMENT</span> <span class="token string">'订单id'</span><span class="token punctuation">,</span>order_status string    <span class="token keyword">COMMENT</span> <span class="token string">'订单状态'</span><span class="token punctuation">,</span>create_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>update_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'更新时间'</span><span class="token punctuation">)</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单表'</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p><img src="C:/Users/wmy/AppData/Roaming/Typora/typora-user-images/image-20210830175604658.png" alt="image-20210830175604658"></p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># 把wedw_ods.order_info_20200911数据全量插到dwd层2020-09-11分区</span><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_df <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span><span class="token punctuation">)</span><span class="token keyword">select</span>order_id<span class="token punctuation">,</span>order_status<span class="token punctuation">,</span>create_time<span class="token punctuation">,</span>update_time<span class="token keyword">from</span> wedw_ods<span class="token punctuation">.</span>order_info_20200911<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175627171.png" alt="image-20210830175627171"></p><p>全量抽取，每个分区保留历史全量快照。</p><h1 id="三-增量表"><a href="#三-增量表" class="headerlink" title="三 增量表"></a>三 增量表</h1><p>增量表：新增数据，增量数据是上次导出之后的新数据。</p><p>1、记录每次增加的量，而不是总量；</p><p>2、增量表，只报变化量，无变化不用报</p><p>3、业务库表中需有主键及创建时间，修改时间</p><p>9月10号全量抽取到ods层(全量初始化)</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175659972.png" alt="image-20210830175659972"></p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># 把wedw_ods.order_info_20200910数据全量插到dwd层2020-09-10分区</span><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_di <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span><span class="token punctuation">)</span><span class="token keyword">select</span>order_id<span class="token punctuation">,</span>order_status<span class="token punctuation">,</span>create_time<span class="token punctuation">,</span>update_times<span class="token keyword">from</span> wedw_ods<span class="token punctuation">.</span>order_info_20200910<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175712697.png" alt="image-20210830175712697"></p><p>9月11号抽取更新的数据及当天新增的数据，即订单id为2,3,4,5的数据</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175723229.png" alt="image-20210830175723229"></p><p>wedw_dwd.order_info_di表9月10号的分区数据与wedw_ods.order_info_20200911增量抽取的数据合并，有2种方案</p><p>a.两个表通过主键关联，dwd表存在并且ods表不存在的数据</p><p>union all 一下wedw_ods.order_info_20200911表所有的数据，即全量数据插入到dwd表的9月11号的分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_di <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span><span class="token punctuation">)</span><span class="token keyword">select</span> t1<span class="token punctuation">.</span>order_id<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>order_status<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>create_time<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>update_time<span class="token keyword">from</span>wedw_dwd<span class="token punctuation">.</span>order_info_di t1<span class="token keyword">left</span> <span class="token keyword">join</span>wedw_ods<span class="token punctuation">.</span>order_info_20200911 t2<span class="token keyword">on</span> t1<span class="token punctuation">.</span>order_id <span class="token operator">=</span> t2<span class="token punctuation">.</span>order_id<span class="token keyword">where</span> t1<span class="token punctuation">.</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span><span class="token operator">and</span> t2<span class="token punctuation">.</span>order_id <span class="token operator">is</span> <span class="token boolean">null</span><span class="token keyword">union</span> <span class="token keyword">all</span><span class="token keyword">select</span>  order_id<span class="token punctuation">,</span>order_status<span class="token punctuation">,</span>create_time<span class="token punctuation">,</span>update_time<span class="token keyword">from</span> wedw_ods<span class="token punctuation">.</span>order_info_20200911<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175738461.png" alt="image-20210830175738461"></p><p>b.两个表数据union all一下，再根据order_id去重(根据order分组，更新时间降序，取第一条)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_di <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span><span class="token punctuation">)</span><span class="token keyword">select</span> t2<span class="token punctuation">.</span>order_id<span class="token punctuation">,</span>t2<span class="token punctuation">.</span>order_status<span class="token punctuation">,</span>t2<span class="token punctuation">.</span>create_time<span class="token punctuation">,</span>t2<span class="token punctuation">.</span>update_time <span class="token keyword">from</span><span class="token punctuation">(</span>    <span class="token keyword">select</span>     t1<span class="token punctuation">.</span>order_id    <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>order_status    <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>create_time    <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>update_time    <span class="token punctuation">,</span>row_number<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">over</span><span class="token punctuation">(</span><span class="token keyword">partition</span> <span class="token keyword">by</span> order_id <span class="token keyword">order</span> <span class="token keyword">by</span> update_time <span class="token keyword">desc</span><span class="token punctuation">)</span> <span class="token keyword">as</span> rn    <span class="token keyword">from</span>    <span class="token punctuation">(</span>        <span class="token keyword">select</span>         order_id        <span class="token punctuation">,</span>order_status        <span class="token punctuation">,</span>create_time        <span class="token punctuation">,</span>update_time        <span class="token keyword">from</span>        wedw_dwd<span class="token punctuation">.</span>order_info_di         <span class="token keyword">where</span> date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span>         <span class="token keyword">union</span> <span class="token keyword">all</span>         <span class="token keyword">select</span>          order_id        <span class="token punctuation">,</span>order_status        <span class="token punctuation">,</span>create_time        <span class="token punctuation">,</span>update_time        <span class="token keyword">from</span>         wedw_ods<span class="token punctuation">.</span>order_info_20200911    <span class="token punctuation">)</span> t1<span class="token punctuation">)</span> t2<span class="token keyword">where</span> t2<span class="token punctuation">.</span>rn <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175756494.png" alt="image-20210830175756494"></p><p>特殊增量表：da表，每天的分区就是当天的数据，其数据特点就是数据产生后就不会发生变化，如日志表。</p><h1 id="四-拉链表"><a href="#四-拉链表" class="headerlink" title="四 拉链表"></a>四 拉链表</h1><p>维护历史状态，以及最新状态数据</p><p>适用情况：</p><p>1.数据量比较大</p><p>2.表中的部分字段会被更新</p><p>3.需要查看某一个时间点或者时间段的历史快照信息</p><p>查看某一个订单在历史某一个时间点的状态</p><p>某一个用户在过去某一段时间，下单次数</p><p>4.更新的比例和频率不是很大</p><p>如果表中信息变化不是很大，每天都保留一份全量，那么每次全量中会保存很多不变的信息，对存储是极大的浪费</p><p>优点</p><p>1、满足反应数据的历史状态</p><p>2、最大程度节省存储</p><p>9月10号全量抽取到ods层</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> wedw_ods<span class="token punctuation">.</span>order_info_20200910<span class="token punctuation">(</span> order_id     string    <span class="token keyword">COMMENT</span> <span class="token string">'订单id'</span><span class="token punctuation">,</span>order_status string    <span class="token keyword">COMMENT</span> <span class="token string">'订单状态'</span><span class="token punctuation">,</span>create_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>update_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'更新时间'</span><span class="token punctuation">)</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单表'</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175813111.png" alt="image-20210830175813111"></p><p>建立dwd层拉链表</p><p>增加两个字段：<br>start_dt(表示该条记录的生命周期开始时间——周期快照时的状态)<br>end_dt(该条记录的生命周期结束时间)</p><p>end_dt= ‘9999-12-31’ 表示该条记录目前处于有效状态</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz<span class="token punctuation">(</span> order_id     string    <span class="token keyword">COMMENT</span> <span class="token string">'订单id'</span><span class="token punctuation">,</span>order_status string    <span class="token keyword">COMMENT</span> <span class="token string">'订单状态'</span><span class="token punctuation">,</span>create_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>update_time  <span class="token keyword">timestamp</span> <span class="token keyword">COMMENT</span> <span class="token string">'更新时间'</span><span class="token punctuation">,</span>start_dt     <span class="token keyword">date</span>      <span class="token keyword">COMMENT</span> <span class="token string">'开始生效日期'</span><span class="token punctuation">,</span>end_dt       <span class="token keyword">date</span>      <span class="token keyword">COMMENT</span> <span class="token string">'结束生效日期'</span><span class="token punctuation">)</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单表'</span>partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>date_id string<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p>注：第一次加工的时候需要初始化所有数据，start_time设置为数据日期2020-09-10 ，end_time设置为9999-12-31</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span><span class="token punctuation">)</span><span class="token keyword">select</span> order_id    <span class="token punctuation">,</span>order_status<span class="token punctuation">,</span>create_time <span class="token punctuation">,</span>update_time <span class="token punctuation">,</span>to_date<span class="token punctuation">(</span>update_time<span class="token punctuation">)</span> <span class="token keyword">as</span> start_dt   <span class="token punctuation">,</span><span class="token string">'9999-12-31'</span> <span class="token keyword">as</span> end_dt  <span class="token keyword">from</span>wedw_ods<span class="token punctuation">.</span>order_info_20200910<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175837695.png" alt="image-20210830175837695"></p><p>9月11号抽取更新的数据及当天新增的数据到ods层，即订单id为2,3,4,5的数据</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175847495.png" alt="image-20210830175847495"></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz <span class="token keyword">partition</span><span class="token punctuation">(</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span><span class="token punctuation">)</span><span class="token keyword">select</span> t1<span class="token punctuation">.</span>order_id    <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>order_status<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>create_time <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>update_time<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>start_dt<span class="token punctuation">,</span><span class="token keyword">case</span> <span class="token keyword">when</span> t1<span class="token punctuation">.</span>end_dt <span class="token operator">=</span> <span class="token string">'9999-12-31'</span> <span class="token operator">and</span> t2<span class="token punctuation">.</span>order_id <span class="token operator">is</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">then</span> t1<span class="token punctuation">.</span>date_id <span class="token keyword">else</span> t1<span class="token punctuation">.</span>end_dt <span class="token keyword">end</span> <span class="token keyword">as</span> end_dt<span class="token keyword">from</span>wedw_dwd<span class="token punctuation">.</span>order_info_dz t1<span class="token keyword">left</span> <span class="token keyword">join</span> wedw_ods<span class="token punctuation">.</span>order_info_20200911 t2<span class="token keyword">on</span> t1<span class="token punctuation">.</span>order_id <span class="token operator">=</span> t2<span class="token punctuation">.</span>order_id<span class="token keyword">where</span> t1<span class="token punctuation">.</span>date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span><span class="token keyword">union</span> <span class="token keyword">all</span><span class="token keyword">SELECT</span> t1<span class="token punctuation">.</span>order_id    <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>order_status<span class="token punctuation">,</span>t1<span class="token punctuation">.</span>create_time <span class="token punctuation">,</span>t1<span class="token punctuation">.</span>update_time<span class="token punctuation">,</span>to_date<span class="token punctuation">(</span>update_time<span class="token punctuation">)</span> <span class="token keyword">as</span> start_dt<span class="token punctuation">,</span><span class="token string">'9999-12-31'</span> <span class="token keyword">as</span> end_dt<span class="token keyword">FROM</span> wedw_ods<span class="token punctuation">.</span>order_info_20200911 t1<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1626355774395-33091aff-f966-48c2-a154-939624f0f4c4.png" alt="img"></p><p>查询当前的所有有效记录：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz <span class="token keyword">where</span> date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span><span class="token operator">and</span> end_dt <span class="token operator">=</span><span class="token string">'9999-12-31'</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175901363.png" alt="image-20210830175901363"></p><p>查询9月10号历史快照:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz <span class="token keyword">where</span> date_id <span class="token operator">=</span> <span class="token string">'2020-09-10'</span> <span class="token operator">and</span> start_dt <span class="token operator">&lt;=</span> <span class="token string">'2020-09-10'</span> <span class="token operator">and</span> end_dt <span class="token operator">>=</span><span class="token string">'2020-09-10'</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175915527.png" alt="image-20210830175915527"></p><p>查询9月11号历史快照:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> wedw_dwd<span class="token punctuation">.</span>order_info_dz <span class="token keyword">where</span> date_id <span class="token operator">=</span> <span class="token string">'2020-09-11'</span> <span class="token operator">and</span> start_dt <span class="token operator">&lt;=</span> <span class="token string">'2020-09-11'</span> <span class="token operator">and</span> end_dt <span class="token operator">>=</span><span class="token string">'2020-09-11'</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175932702.png" alt="image-20210830175932702"></p><h1 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h1><p>在工作中，其实上述3种表都是很有可能会用到的，那么我们应该怎么选择呢？</p><ul><li><p>如果数据量不是很大(不超过20W)且预估后续增长的非常慢，可以考虑全量表抽取，这是最简便的方法</p></li><li><p>如果数据量目前来说不是很大，但是业务发展很快，数据量一段时间后就会上来，建议增量抽取哦</p></li><li><p>目前数据量本身就非常大，肯定是需要增量抽取的，比如现在有10亿数据，如果你每天全量抽取一遍，相信我，你会抽哭的</p></li><li><p>对于历史状态需要保存的，这个时候就需要使用拉链表了，实际工作中，使用拉链表的场景并不会太多，比如订单表，保存订单历史状态，维表(缓慢变化维的处理)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-4-数据模型建设</title>
      <link href="/2021/08/30/0002-4-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%BB%BA%E8%AE%BE/"/>
      <url>/2021/08/30/0002-4-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%BB%BA%E8%AE%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-4-数据模型建设"><a href="#0002-4-数据模型建设" class="headerlink" title="0002-4-数据模型建设"></a>0002-4-数据模型建设</h1><h1 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h1><h2 id="1-1-什么是数据建模"><a href="#1-1-什么是数据建模" class="headerlink" title="1.1 什么是数据建模"></a>1.1 什么是数据建模</h2><p>数据建模简单来说就是基于对业务的理解，将各种数据进行整合和关联，并最终使得这些数据可用性，可读性增强，让使用方能快速的获取到自己关心的有价值的信息并且及时的作出响应，为公司带来效益。</p><h2 id="1-2-为什么要数据建模"><a href="#1-2-为什么要数据建模" class="headerlink" title="1.2 为什么要数据建模"></a>1.2 为什么要数据建模</h2><p>数据建模是一套方法论，主要是对数据的整合和存储做一些指导，强调从各个角度合理的存储数据。</p><p>· 进行全面的业务梳理，改进业务流程。</p><p>在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。</p><p>· 建立全方位的数据视角，消灭信息孤岛和数据差异。</p><p>通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。</p><p>· 解决业务的变动和数据仓库的灵活性。</p><p>通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。</p><p>· 帮助数据仓库系统本身的建设。</p><p>通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。</p><p>有了合适的数据模型，是会带来很多好处的：</p><ul><li><p>查询使用性能提升</p></li><li><p>用户效率提高，改善用户体验</p></li><li><p>数据质量提升</p></li><li><p>降低企业成本</p></li><li><p>……</p></li></ul><p>所以大数据系统需要数据模型方法来更好的组织和存储，以便在性能,成本，效率和质量之间取的平衡。</p><h1 id="二-建模工具"><a href="#二-建模工具" class="headerlink" title="二 建模工具"></a>二 建模工具</h1><p>PowerDesigner：</p><p>Power Designer 是Sybase公司的CASE工具集，使用它可以方便地对管理信息系统进行分析设计，他几乎包括了数据库模型设计的全过程。利用Power Designer可以制作数据流程图、概念数据模型、物理数据模型，还可以为数据仓库制作结构模型，也能对团队设计模型进行控制。他可以与许多流行的软件开发工具，例如PowerBuilder、Delphi、VB等相配合使开发时间缩短和使系统设计更优化。</p><p>power designer是能进行数据库设计的强大的软件，是一款开发人员常用的数据库建模工具。使用它可以分别从概念数据模型(Conceptual Data Model)和物理数据模型(Physical Data Model)两个层次对数据库进行设计。在这里，概念数据模型描述的是独立于数据库管理系统(DBMS)的实体定义和实体关系定义；物理数据模型是在概念数据模型的基础上针对目标数据库管理系统的具体化。</p><h1 id="三-kimball与inmon架构"><a href="#三-kimball与inmon架构" class="headerlink" title="三 kimball与inmon架构"></a>三 kimball与inmon架构</h1><h2 id="3-1-inmon架构"><a href="#3-1-inmon架构" class="headerlink" title="3.1 inmon架构"></a>3.1 inmon架构</h2><p>辐射状企业信息工厂(CIF) 方法由Bill Inmon及业界人士倡导的。在这个环境下，数据从操作性数据源中获取，在ETL系统中处理，将这一过程称为数据获取，从这一过程中获得的原子数据保存在满足第三范式的数据库中，这种规范化的原子数据的仓库被称为CIF架构下的企业级数据仓库(EDW)</p><p>与Kimball方法相似，CIF提倡企业数据协调与集成，但CIF认为要利用规范化的EDW承担这一角色，而Kimball架构强调具有一致性维度的企业总线的重要作用</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175051008.png" alt="image-20210830175051008"></p><p>Inmon企业级数据仓库的分析数据库通常以部门为中心（而不是围绕业务过程来组织），而且包含汇总数据，并不是原子级别数据，如果ETL过程中数据所应用的业务规则超越了基本概要，如部门改名了或者其他的类似计算，要将分析数据库与EDW原子数据联系起来将变得很困难</p><h2 id="3-2-kimball架构"><a href="#3-2-kimball架构" class="headerlink" title="3.2 kimball架构"></a>3.2 kimball架构</h2><p>Kimball架构利用了CIF中处于中心地位的EDW，但是此次的EDW完全与分析与报表用户隔离，仅作为数据来源，其中数据是维度的，原子的，以过程为中心的，与企业级数据仓库总线结构保持一致。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175105496.png" alt="image-20210830175105496"></p><h2 id="3-3-架构对比"><a href="#3-3-架构对比" class="headerlink" title="3.3 架构对比"></a>3.3 架构对比</h2><h3 id="3-3-1-流程"><a href="#3-3-1-流程" class="headerlink" title="3.3.1 流程"></a>3.3.1 流程</h3><p>Inmon架构是自顶向下，即从数据抽取–&gt;数据仓库–&gt;数据集市，以数据源为导向，是一种瀑布流开发方法，模型偏向于3NF，</p><p>Kimball:架构是自下向上，即从数据集市(主题划分)–&gt;数据仓库–&gt; 数据抽取，是以需求为导向的，一般使用星型模型  </p><h3 id="3-3-2-事实表与维表"><a href="#3-3-2-事实表与维表" class="headerlink" title="3.3.2 事实表与维表"></a>3.3.2 事实表与维表</h3><p>Inmon架构下，不强调事实表和维表的概念，因为数据源变化可能会比较大，更加强调的是数据清洗的工作  </p><p>kimball架构强调模型由事实表和维表组成，注重事实表与维表的设计</p><h3 id="3-3-3-数据集市"><a href="#3-3-3-数据集市" class="headerlink" title="3.3.3 数据集市"></a>3.3.3 数据集市</h3><p>Inmon架构中，数据集市有自己的物理存储，是真实存在的。</p><p>Kimball数据仓库架构中，数据集市是一个逻辑概念，只是多维数据仓库中的主题域划分，并没有自己的物理存储，也可以说是虚拟的数据集市。是数据仓库的一个访问层，是按主题域组织的数据集合，用于支持部门级的决策。</p><h3 id="3-3-4-中心"><a href="#3-3-4-中心" class="headerlink" title="3.3.4 中心"></a>3.3.4 中心</h3><p>Inmon架构是以部门为中心，而Kimball架构是以业务过程为中心 </p><h3 id="3-3-5-EDW访问"><a href="#3-3-5-EDW访问" class="headerlink" title="3.3.5 EDW访问"></a>3.3.5 EDW访问</h3><p>Inmon架构中用户可以直接访问企业数据仓库(EDW)</p><p>Kimball架构中用户不可以直接访问企业数据仓库(EDW)，只能访问展现区数据</p><p>企业开发中一般选择Kimball维度建模</p><h1 id="四-数仓建模阶段划分"><a href="#四-数仓建模阶段划分" class="headerlink" title="四 数仓建模阶段划分"></a>四 数仓建模阶段划分</h1><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175117296.png" alt="image-20210830175117296"></p><h2 id="4-1-业务模型"><a href="#4-1-业务模型" class="headerlink" title="4.1 业务模型"></a>4.1 业务模型</h2><p>生成业务模型，主要解决业务层面的分解和程序化</p><ul><li><p>划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。</p></li><li><p>深入了解各个业务部门的内具体业务流程并将其程序化。</p></li><li><p>提出修改和改进业务部门工作流程的方法并程序化。</p></li><li><p>数据建模的范围界定，整个数据仓库项目的目标和阶段划分。</p></li></ul><h2 id="4-2-领域模型"><a href="#4-2-领域模型" class="headerlink" title="4.2 领域模型"></a>4.2 领域模型</h2><ul><li><p>生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。</p></li><li><p>抽取关键业务概念，并将之抽象化。</p></li><li><p>将业务概念分组，按照业务主线聚合类似的分组概念。</p></li><li><p>细化分组概念，理清分组概念内的业务流程并抽象化。</p></li><li><p>理清分组概念之间的关联，形成完整的领域概念模型。</p></li></ul><h2 id="4-3-逻辑模型"><a href="#4-3-逻辑模型" class="headerlink" title="4.3 逻辑模型"></a>4.3 逻辑模型</h2><p>生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。</p><ul><li><p>业务概念实体化，并考虑其具体的属性</p></li><li><p>事件实体化，并考虑其属性内容</p></li><li><p>说明实体化，并考虑其属性内容</p></li></ul><h2 id="4-4-物理模型"><a href="#4-4-物理模型" class="headerlink" title="4.4 物理模型"></a>4.4 物理模型</h2><p>生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。</p><ul><li><p>针对特定物理化平台，做出相应的技术调整</p></li><li><p>针对模型的性能考虑，对特定平台作出相应的调整</p></li><li><p>针对管理的需要，结合特定的平台，做出相应的调整</p></li></ul><p>生成最后的执行脚本，并完善之。</p><h1 id="五-模型建设方法"><a href="#五-模型建设方法" class="headerlink" title="五 模型建设方法"></a>五 模型建设方法</h1><h2 id="5-1-ER模型"><a href="#5-1-ER模型" class="headerlink" title="5.1 ER模型"></a>5.1 ER模型</h2><p>ER模型是属于三范式的，是企业级的主题抽象而不是单独描述某个业务</p><h3 id="5-1-1-什么是范式"><a href="#5-1-1-什么是范式" class="headerlink" title="5.1.1 什么是范式"></a>5.1.1 什么是范式</h3><p>当分类不可再分时，这种关系是规范化的，一个低级范式分解转换为更高级的范式时，就叫做规范化</p><p>数据表可以分为1-5NF，第一范式是最低要求，第五范式则是最高要求</p><p>最常用的范式有第一范式（1NF）、第二范式（2NF）、第三范式（3NF）</p><h3 id="5-1-2-第一范式"><a href="#5-1-2-第一范式" class="headerlink" title="5.1.2 第一范式"></a>5.1.2 第一范式</h3><p>表中的每一列都是不可拆分的原子项</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175129153.png" alt="image-20210830175129153"></p><p>由上图可知，phone字段里面存了2个值，具有可分割性，不符合1NF，可以改成：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175144217.png" alt="image-20210830175144217"></p><h3 id="5-1-3-第二范式"><a href="#5-1-3-第二范式" class="headerlink" title="5.1.3 第二范式"></a>5.1.3 第二范式</h3><p>第二范式要同时满足下面两个条件：</p><ul><li>满足第一范式</li><li>没有部分依赖</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175154496.png" alt="image-20210830175154496"></p><p>上图可以看出，如果一个用户下了很多订单，则用户名，收获地址和手机号有重复出现的情况造成数据冗余，很明显不太符合第二范式，可以改成：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175207361.png" alt="image-20210830175207361"></p><h3 id="5-1-4-第三范式"><a href="#5-1-4-第三范式" class="headerlink" title="5.1.4 第三范式"></a>5.1.4 第三范式</h3><p>第三范式要同时满足下面两个条件：</p><ul><li>满足第二范式</li><li>没有传递依赖</li></ul><p>简单点说，关系重复，能互相推导出来</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175217121.png" alt="image-20210830175217121"></p><p>如上图所示，如果知道了zip邮编，其实是能推出来省市区的，相反，知道了省市区，也是可以推出邮编的，有传递依赖，造成了冗余，不符合第三范式，需要改造:</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175228370.png" alt="image-20210830175228370"></p><h3 id="5-1-5-小结"><a href="#5-1-5-小结" class="headerlink" title="5.1.5 小结"></a>5.1.5 小结</h3><p>在关系数据模型设计中，一般需要满足第三范式的要求。如果一个表有良好的主外键设计，就应该是满足3NF的表。</p><p>规范化带来的好处是通过减少数据冗余提高更新数据的效率，同时保证数据完整性。然而，我们在实际应用中也要防止过度规范化的问题。规范化程度越高，划分的表就越多，在查询数据时越有可能使用表连接操作。</p><p>而如果连接的表过多，会影响查询的性能。关键的问题是要依据业务需求，仔细权衡数据查询和数据更新的关系，制定最适合的规范化程度。还有一点需要注意的是，不要为了遵循严格的规范化规则而修改业务需求。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175239653.png" alt="image-20210830175239653"></p><h2 id="5-2-维度建模"><a href="#5-2-维度建模" class="headerlink" title="5.2 维度建模"></a>5.2 维度建模</h2><p>维度建模是一种将大量数据结构化的逻辑设计手段，包含维度和指标，它不像ER模型目的是消除冗余数据，维度建模是面向分析，最终目的是提高查询性能，所以会增加数据冗余，并且违反三范式。</p><p>维度建模也是重点关注让用户快速完成需求分析且对于复杂查询及时响应，维度建模一般可以分为三种：</p><ul><li><p>星型模型</p></li><li><p>雪花模型</p></li><li><p>星座模型</p></li></ul><p>其中最常用的其实是星型模型</p><h3 id="5-2-1-背景"><a href="#5-2-1-背景" class="headerlink" title="5.2.1 背景"></a>5.2.1 背景</h3><p>在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型，雪花型模型及星座模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型，雪花型模型还是星座模型进行组织。</p><h3 id="5-2-2-事实表和维度表"><a href="#5-2-2-事实表和维度表" class="headerlink" title="5.2.2 事实表和维度表"></a>5.2.2 事实表和维度表</h3><h5 id="事实表（Fact-Table）"><a href="#事实表（Fact-Table）" class="headerlink" title="事实表（Fact Table）"></a><strong>事实表（Fact Table）</strong></h5><p>指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。</p><p>事实表作为数据仓库建模的核心，需要根据业务过程来设计，包含了引用的维度和业务过程有关的度量。</p><p>作为度量业务过程的事实，一般为整型或浮点型的十进制数值，有可加性，半可加性和不可加性三种类型</p><h5 id="可加"><a href="#可加" class="headerlink" title="可加"></a><strong>可加</strong></h5><p>最灵活最有用的事实是完全可加，可加性度量可以按照与事实表关联的任意维度汇总。比如订单总金额</p><h5 id="半可加"><a href="#半可加" class="headerlink" title="半可加"></a><strong>半可加</strong></h5><p>半可加度量可以对某些维度汇总，但不能对所有维度汇总。差额是常见的半可加事实，除了时间维度外，他们可以跨所有维度进行操作。(比如每天的余额加起来毫无意义)</p><h5 id="不可加"><a href="#不可加" class="headerlink" title="不可加"></a><strong>不可加</strong></h5><p>一些度量是完全不可加的，例如：比率。对非可加事实，一种好的方法是，分解为可加的组件来实现聚集</p><h5 id="维度表"><a href="#维度表" class="headerlink" title="维度表"></a><strong>维度表</strong></h5><p>维度表（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等的属性）、地点表（包含国家、省／州、城市等属性）等。维度是维度建模的基础和灵魂。</p><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a><strong>优点</strong></h5><ul><li><p>缩小了事实表的大小。</p></li><li><p>便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。</p></li><li><p>维度表可以为多个事实表重用，以减少重复工作。</p></li></ul><h5 id="下钻"><a href="#下钻" class="headerlink" title="下钻"></a><strong>下钻</strong></h5><p>下钻是商业用户分析数据的最基本的方法。下钻仅需要在查询上增加一个行头指针，新行的头指针是一个维度属性，附加了sql语言的group by表达式，属性可以来自任何与查询使用的事实表关联的维度，下钻不需要预先存在层次的定义，或者是下钻路径。</p><h5 id="退化维度"><a href="#退化维度" class="headerlink" title="退化维度"></a><strong>退化维度</strong></h5><p>有时，维度除了主键外没有其他内容，例如，当某一发票包含多个数据项时，数据项事实行继承了发票的所有描述性维度外键，发票除了外键无其他项，但发票数量仍然是在此数据项级别的合法维度键。这种退化维度被放入事实表中，清楚的表明没有关联的维度表，退化维度常见于交易和累计快照事实表中。</p><h3 id="5-2-3-事实表与维表的关系"><a href="#5-2-3-事实表与维表的关系" class="headerlink" title="5.2.3 事实表与维表的关系"></a>5.2.3 事实表与维表的关系</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175255437.png" alt="image-20210830175255437"></p><h3 id="5-2-4-星型模型"><a href="#5-2-4-星型模型" class="headerlink" title="5.2.4 星型模型"></a>5.2.4 星型模型</h3><p>星形模型中有一张事实表，以及零个或多个维度表，事实表与维度表通过主键外键相关联，维度表之间没有关联，当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。星形模型是最简单，也是最常用的模型。由于星形模型只有一张大表，因此它相比于其他模型更适合于大数据处理。其他模型可以通过一定的转换，变为星形模型。</p><p>星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175308911.png" alt="image-20210830175308911"></p><h3 id="5-2-5-雪花模型"><a href="#5-2-5-雪花模型" class="headerlink" title="5.2.5 雪花模型"></a>5.2.5 雪花模型</h3><p>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的维度表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图，将地域维表又分解为国家，省份，城市等维表。它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175321494.png" alt="image-20210830175321494"></p><h3 id="5-2-6-星座模型"><a href="#5-2-6-星座模型" class="headerlink" title="5.2.6 星座模型"></a>5.2.6 星座模型</h3><p>星座模型是由星型模型延伸而来，星型模型是基于一张事实表而星座模式是基于多张事实表，并且共享维度表信息，这种模型往往应用于数据关系比星型模型和雪花模型更复杂的场合。星座模型需要多个事实表共享维度表，因而可以视为星形模型的集合，故亦被称为星系模型</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175340906.png" alt="image-20210830175340906"></p><h3 id="5-2-7-对比"><a href="#5-2-7-对比" class="headerlink" title="5.2.7 对比"></a>5.2.7 对比</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1626349903114-ee0ca66a-12e2-4d3c-a503-7a850b254bd4.png" alt="img"></p><ul><li><p>星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。</p></li><li><p>星型结构不用考虑很多正规化的因素，设计与实现都比较简单。</p></li><li><p>雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率比较低。</p></li><li><p>正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。</p></li></ul><h3 id="5-2-8-小结"><a href="#5-2-8-小结" class="headerlink" title="5.2.8 小结"></a>5.2.8 小结</h3><p>通过对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来减少表查询的次数从而提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型和星座模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。</p><h3 id="5-2-9-建模原则"><a href="#5-2-9-建模原则" class="headerlink" title="5.2.9 建模原则"></a>5.2.9 建模原则</h3><ul><li>高内聚和低辑合</li></ul><p>将业务相近或者相关、粒度相同的数据设计为一个逻辑或者物理模型：将高概率同 时访问的数据放一起 ，将低概率同时访问的数据分开存储。</p><ul><li>核心模型与扩展模型分离</li></ul><p>建立核心模型与扩展模型体系，核心模型包括的宇段支持常用的核心业务，扩展模型包括的字段支持个性化或少量应用的需要 ，不能让扩展模型的宇段过度侵人核心模型，以免破坏核心模型的架构简洁性与可维护性。</p><ul><li>公共处理逻辑下沉及单一</li></ul><p>越是底层公用的处理逻辑越应该在数据调度依赖的底层进行封装与实现，不要让公用的处理逻辑暴露给应用层实现，不要让公共逻辑多处同时存在。</p><ul><li>成本与性能平衡</li></ul><p>适当的数据冗余可换取查询和刷新性能，不宜过度冗余与数据复制。</p><ul><li>数据可回滚</li></ul><p>不改变处理逻辑，不修改代码的情况下重跑任务结果不变</p><ul><li>一致性</li></ul><p>字段命名及定义必须一致</p><ul><li>命名清晰、可理解</li></ul><p>表命名需清晰、一致，表名需易于使用方理解</p><h3 id="5-2-10-星型模型设计步骤"><a href="#5-2-10-星型模型设计步骤" class="headerlink" title="5.2.10 星型模型设计步骤"></a>5.2.10 星型模型设计步骤</h3><p>\1. 选择需要进行分析决策的业务过程  比如下单</p><p>\2. 选择粒度。在事件分析中，我们要预判所有分析需要细分的程度，从而决定选择的粒度。比如订单粒度，粒度是维度的一个组合。</p><p>\3. 识别维表。选择好粒度之后，就需要基于此粒度设计维表，包括维度属性，用于分析时进行分组和筛选。</p><p>\4. 选择事实。确定分析需要衡量的指标  </p><h2 id="5-3-Data-vault模型"><a href="#5-3-Data-vault模型" class="headerlink" title="5.3 Data vault模型"></a>5.3 Data vault模型</h2><p>Data Vault Dan Linstedt 发起创建的一种模型，它是模型的衍生，其设计的出发点也是为了实现数据的整合，但不能直接用于数据分析决策。它强调建立一个可审计的基础数据层，也就是强调数据的历史性、可追溯性和原子性，而不要求对数据进行过度的一致性处理和整合；</p><p>同时它基于主题概念将企业数据进行结构化组织，并引入了更进一步的范式处理来优化模型，以应对源系统变更的扩展性。Data Vault 型由以下几部分组成。</p><ul><li><p>Hub ：是企业的核心业务实体，由 实体 key 、数据仓库序列代理键、装载时间、数据来源组成。</p></li><li><p>Link ：代表 Hub 之间的关系。这里与 模型最大的区别是将关系作为一个独立的单元抽象，可以提升模型的扩展性。它可以直接描述 1:1 1:n n:n 的关系，而不需要做任何变更。它由 Hub 的代理键、装载时间、数据来源组成。</p></li><li><p>Satellite ：是 Hub 的详细描述内容， 一个 Hub 可以有多个 Satellite它由 Hub 的代理键、装载时间、来源类型、详细的 Hub 描述信息组成。</p></li></ul><p>Data Vault 模型比 ER 模型更容易设计和产出，它的 ETL 加工可实现配置化。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175353352.png" alt="image-20210830175353352"></p><h2 id="5-4-Anchor模型"><a href="#5-4-Anchor模型" class="headerlink" title="5.4 Anchor模型"></a>5.4 Anchor模型</h2><p>进一步规范化处理，其核心思想是所有的扩展只添加而不是修改，因此将模型规范到6NF，基本编程了K-V结构化模型。</p><p>那么总的来说，分为三个阶段：</p><p>1、将数据以源表结构相同的方式同步到Oracle，数据工程师基于ODS数据进行统计。</p><p>2、通过一些模型技术改变烟囱式的开发模型，消除一些冗余，提升数据的一致性。（经验）在不太成熟、快速变化的业务面前，构建ER模型的风险非常大，不太适合去构建ER模型。</p><p>3、选择了以Kimball的维度建模为核心理念的模型方法论，同时对其进行了一定的升级和扩展，构建了公共层模型数据架构体系。</p>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-3-数据研发流程</title>
      <link href="/2021/08/30/0002-3-%E6%95%B0%E6%8D%AE%E7%A0%94%E5%8F%91%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/08/30/0002-3-%E6%95%B0%E6%8D%AE%E7%A0%94%E5%8F%91%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-3-数据研发流程"><a href="#0002-3-数据研发流程" class="headerlink" title="0002-3-数据研发流程"></a>0002-3-数据研发流程</h1><ol><li><p>需求分析调研(数据调研，需求调研，业务调研)：明确口径，评估排期，需求正规流程提交</p></li><li><p>指标管理：完善指标命名规范，指标同名同义，指标与业务强相关，明确指标构成要素</p></li><li><p>模型设计：完善开发流程规范，标准化业务调研，知识库文档集中管理，建立模型评审机制</p></li><li><p>ETL开发：ODS-&gt;DWD-&gt;DW-&gt;DWS-&gt;ADS </p></li><li><p>数据验证：制定数据测试标准</p></li><li><p>任务调度：规范化调度参数配置</p></li><li><p>上线管理</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-2-数据调研探查</title>
      <link href="/2021/08/30/0002-2-%E6%95%B0%E6%8D%AE%E8%B0%83%E7%A0%94%E6%8E%A2%E6%9F%A5/"/>
      <url>/2021/08/30/0002-2-%E6%95%B0%E6%8D%AE%E8%B0%83%E7%A0%94%E6%8E%A2%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-2-数据调研探查"><a href="#0002-2-数据调研探查" class="headerlink" title="0002-2-数据调研探查"></a>0002-2-数据调研探查</h1><h3 id="一-业务调研"><a href="#一-业务调研" class="headerlink" title="一 业务调研"></a><strong>一 业务调研</strong></h3><p>数据仓库是要涵盖所有业务领域，还是各个业务领域独自建设，业务领域内的业务线也同样面临着这个问题。所以要构建大数据数据仓库，就需要了解各个业务领域、业务线的业务有什么共同点和不同点，以及各个业务线可以细分为哪几个业务模块，每个业务模块具体的业务流程又是怎样的。业务调研是否充分，将会直接决定数据仓库建设是否成功。</p><h3 id="二-需求调研"><a href="#二-需求调研" class="headerlink" title="二 需求调研"></a><strong>二 需求调研</strong></h3><p>了解业务系统的业务后不等于说就可以实施数仓建设了，还需要收集数据使用者的需求，及找分析师、运营人员、产品人员等了解他们对数据的诉求。通常需求调研分下面两种途径：</p><p>\1. 根据与分析师、运营人员、产品人员的沟通获取需求。</p><p>\2. 对现有报表、数据进行研究分析获取数据建设需求。</p><h3 id="三-数据调研"><a href="#三-数据调研" class="headerlink" title="三 数据调研"></a><strong>三 数据调研</strong></h3><p>前期需要做好数据探查工作，需要了解数据库类型，数据来源，全量数据情况及数据每年增长情况，更新机制；还需要了解数据是否结构化，是否清洗，是接口调用还是直接访问库，有哪些类型的数据，数据结构之怎样的。</p><ul><li>数据开发，模型建设之前，先了解数据结构，数据内容，数据特性，对数据有一个整体把控</li><li>探查一下本次需求能不能实现，怎么实现，有没有隐藏bug，数据质量如何</li></ul>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-1-数据仓库简介</title>
      <link href="/2021/08/30/0002-1-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E4%BB%8B/"/>
      <url>/2021/08/30/0002-1-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-1-数据仓库简介"><a href="#0002-1-数据仓库简介" class="headerlink" title="0002-1-数据仓库简介"></a>0002-1-数据仓库简介</h1><h1 id="一什么是数据仓库"><a href="#一什么是数据仓库" class="headerlink" title="一什么是数据仓库"></a><strong>一什么是数据仓库</strong></h1><h2 id="1-1-数据仓库概念"><a href="#1-1-数据仓库概念" class="headerlink" title="1.1 数据仓库概念"></a><strong>1.1</strong> <strong>数据仓库概念</strong></h2><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。</p><h2 id="1-2-数据仓库特点"><a href="#1-2-数据仓库特点" class="headerlink" title="1.2 数据仓库特点"></a><strong>1.2</strong> <strong>数据仓库特点</strong></h2><h3 id="1-2-1-面向主题"><a href="#1-2-1-面向主题" class="headerlink" title="1.2.1****面向主题"></a><strong>1.2.1****面向主题</strong></h3><p>普通的操作型数据库主要面向事务性处理，而数据仓库中的所有数据一般按照主题进行划分。主题是对业务数据的一种抽象，是从较高层次上对信息系统中的数据进行归纳和整理。</p><p>面向主题的数据可以划分成两部分—-根据原系统业务数据的特点进行主题的抽取和确定每个主题所包含的数据内容。例如客户主题、产品主题、财务主题等；而客户主题包括客户基本信息、客户信用信息、客户资源信息等内容。分析数据仓库主题的时候，一般方法是先确定几个基本的主题，然后再将范围扩大，最后再逐步求精</p><h3 id="1-2-2-集成性"><a href="#1-2-2-集成性" class="headerlink" title="1.2.2****集成性"></a><strong>1.2.2****集成性</strong></h3><p>面向操作型的数据库通常是异构的、并且相互独立，所以无法对信息进行概括和反映信</p><p>息的本质。而数据仓库中的数据是经过数据的抽取、清洗、切换、加载得到的，所以为了保证数据不存在二义性，必须对数据进行编码统一和必要的汇总，以保证数据仓库内数据的一致性。数据仓库在经历数据集成阶段后，使数据仓库中的数据都遵守统一的编码规则，并且消除许多冗余数据。 </p><h3 id="1-2-3-稳定性"><a href="#1-2-3-稳定性" class="headerlink" title="1.2.3****稳定性"></a><strong>1.2.3****稳定性</strong></h3><p>数据仓库中的数据反映的都是一段历史时期的数据内容，它的主要操作是查询、分析而</p><p>不进行一般意义上的更新（数据集成前的操作型数据库主要完成数据的增加、修改、删除、查询），一旦某个数据进入到数据仓库后，一般情况下数据会被长期保留，当超过规定的期限才会被删除。通常数据仓库需要做的工作就是加载、查询和分析，一般不进行任何修改操作，是为了企业高层人员决策分析之用。</p><h3 id="1-2-4-反映历史变化"><a href="#1-2-4-反映历史变化" class="headerlink" title="1.2.4****反映历史变化"></a><strong>1.2.4****反映历史变化</strong></h3><p>数据仓库不断从操作型数据库或其他数据源获取变化的数据，从而分析和预测需</p><p>要的历史数据，所以一般数据仓库中数据表的键码（维度）都含有时间键，以表明数据的历史时期信息，然后不断增加新的数据内容。通过这些历史信息可以对企业的发展历程和趋势做出分析和预测。数据仓库的建设需要大量的业务数据作为积累，并将这些宝贵的历史信息经过加工、整理，最后提供给决策分析人员，这是数据仓库建设的根本目的。</p><h2 id="1-3-数据仓库发展历程"><a href="#1-3-数据仓库发展历程" class="headerlink" title="1.3 数据仓库发展历程"></a><strong>1.3</strong> <strong>数据仓库发展历程</strong></h2><p>数据仓库的发展大致经历了这样的三个过程：</p><ul><li><p>简单报表阶段：这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。</p></li><li><p>数据集市阶段：这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。</p></li><li><p>数据仓库阶段：这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。</p></li></ul><p>通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。</p><h2 id="1-4-数据仓库意义"><a href="#1-4-数据仓库意义" class="headerlink" title="1.4 数据仓库意义"></a><strong>1.4 数据仓库意义</strong></h2><ul><li><p>建立公司统一数据中心</p></li><li><p>为数据BP。运营人员提供数据支持</p></li><li><p>为领导提供决策支持</p></li></ul><h2 id="1-5-数据库和数据仓库的区别"><a href="#1-5-数据库和数据仓库的区别" class="headerlink" title="1.5 数据库和数据仓库的区别"></a><strong>1.5</strong> <strong>数据库和数据仓库的区别</strong></h2><h3 id="1-5-1-数据库"><a href="#1-5-1-数据库" class="headerlink" title="1.5.1****数据库"></a><strong>1.5.1****数据库</strong></h3><p> 是一种逻辑概念，用来存放数据的仓库，通过数据库软件来实现，数据库由许多表组成，表是二维的，一张表里面可以有很多字段，数据库的表，在与能够用二维表现多维关系。</p><h3 id="1-5-2-数据仓库"><a href="#1-5-2-数据仓库" class="headerlink" title="1.5.2****数据仓库"></a><strong>1.5.2****数据仓库</strong></h3><pre><code>  是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策。</code></pre><p>数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。</p><h3 id="1-5-3-对比"><a href="#1-5-3-对比" class="headerlink" title="1.5.3 对比"></a><strong>1.5.3</strong> <strong>对比</strong></h3><p>操作型处理，叫联机事务处理OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p><p>分析型处理，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830175022821.png" alt="image-20210830175022821"></p>]]></content>
      
      
      <categories>
          
          <category> 离线数仓建设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓建设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-5-建模模型评审规范</title>
      <link href="/2021/08/30/0001-5-%E5%BB%BA%E6%A8%A1%E6%A8%A1%E5%9E%8B%E8%AF%84%E5%AE%A1%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/30/0001-5-%E5%BB%BA%E6%A8%A1%E6%A8%A1%E5%9E%8B%E8%AF%84%E5%AE%A1%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-5-建模模型评审规范"><a href="#0001-5-建模模型评审规范" class="headerlink" title="0001-5-建模模型评审规范"></a>0001-5-建模模型评审规范</h1><h1 id="1、指导理论"><a href="#1、指导理论" class="headerlink" title="1、指导理论"></a><strong>1、指导理论</strong></h1><p>数据模型的维度设计主要以维度建模理论为基础，基于维度数据模型总线架构，构建一致性维度和事实。参考阿里onedata建模 </p><h1 id="2、模型层次"><a href="#2、模型层次" class="headerlink" title="2、模型层次"></a><strong>2、模型层次</strong></h1><p>ods: </p><p> 抽取缓冲层，表结构跟业务系统一样；是数仓跟外围数据统一的接口层 </p><p>dwd: </p><p> 数仓模型层，按一定规则组织，有利于统计分析； </p><p> 其中包括，明细表，快照表，累计表 </p><p>dws: </p><p> 公共汇总层 </p><p>ads: </p><p> 存放个性化指标加工，面向应用层 </p><h1 id="3、基本原则"><a href="#3、基本原则" class="headerlink" title="3、基本原则"></a><strong>3、基本原则</strong></h1><p>高内聚，低耦合：相近业务，相关，粒度相同数据设计为一个模型，高访问，低访问分别存储 </p><p>核心模型和扩展模型分离 </p><p>公共处理逻辑下沉单一 </p><p>成本与性能平衡：宽表处理涉及也多，分步骤，分任务处理，最后合并，提高并发性 </p><p>数据可回滚 </p><p>一致性：同一个名词标识同一个意思，同一个意思同一个命名，规范 </p><p>命名清晰可以理解 </p><h1 id="4、实施步骤"><a href="#4、实施步骤" class="headerlink" title="4、实施步骤"></a><strong>4、实施步骤</strong></h1><ul><li><p>选择已经确定业务进行梳理，了解业务流程（参考产品数据文档） </p></li><li><p>ER关系：输出ER关系图，了解实体关系，验证数据是否一致 PDM </p></li><li><p>现有数据流梳理：输出数据流程图 PDM </p></li><li><p>结合提数模型，指标，产品需求文档等，优化目前模型：基于上一步PDM输出优化模型，记录优化点；（数据流跟数据模型结合） </p></li><li><p>基于以上文档进行评审，不通过进行迭代 </p></li><li><p>评审通过后进行开发，上线：开发在目前系统上实施，优先级降低，任务时间戳开 </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-8-字典</title>
      <link href="/2021/08/30/0001-8-%E5%AD%97%E5%85%B8/"/>
      <url>/2021/08/30/0001-8-%E5%AD%97%E5%85%B8/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-7-命名规范</title>
      <link href="/2021/08/30/0001-7-%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/30/0001-7-%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-7-命名规范"><a href="#0001-7-命名规范" class="headerlink" title="0001-7-命名规范"></a>0001-7-命名规范</h1><h1 id="一-表命名"><a href="#一-表命名" class="headerlink" title="一 表命名"></a>一 表命名</h1><h2 id="1-1-ods层表命名"><a href="#1-1-ods层表命名" class="headerlink" title="1.1 ods层表命名"></a>1.1 ods层表命名</h2><p>1.不能使用中文 </p><p>2.时间分区字段统一使用ds </p><h2 id="1-2-cdm层命名"><a href="#1-2-cdm层命名" class="headerlink" title="1.2 cdm层命名"></a>1.2 cdm层命名</h2><h2 id="1-3-ads层命名"><a href="#1-3-ads层命名" class="headerlink" title="1.3 ads层命名"></a>1.3 ads层命名</h2><h2 id="1-4-临时表命名"><a href="#1-4-临时表命名" class="headerlink" title="1.4 临时表命名"></a>1.4 临时表命名</h2><h2 id="1-5-静态表命名"><a href="#1-5-静态表命名" class="headerlink" title="1.5 静态表命名"></a>1.5 静态表命名</h2><h1 id="二-视图命名"><a href="#二-视图命名" class="headerlink" title="二 视图命名"></a>二 视图命名</h1><h1 id="三-字段命名"><a href="#三-字段命名" class="headerlink" title="三 字段命名"></a>三 字段命名</h1><h1 id="四-任务命名"><a href="#四-任务命名" class="headerlink" title="四 任务命名"></a>四 任务命名</h1><h1 id="五-指标命名"><a href="#五-指标命名" class="headerlink" title="五 指标命名"></a>五 指标命名</h1>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-6-开发规范</title>
      <link href="/2021/08/30/0001-6-%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/30/0001-6-%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-6-开发规范"><a href="#0001-6-开发规范" class="headerlink" title="0001-6-开发规范"></a>0001-6-开发规范</h1><h1 id="一-任务开发原则"><a href="#一-任务开发原则" class="headerlink" title="一 任务开发原则"></a>一 任务开发原则</h1><h2 id="1-1-支持重跑"><a href="#1-1-支持重跑" class="headerlink" title="1.1 支持重跑"></a>1.1 支持重跑</h2><h2 id="1-2-数据生命周期合理"><a href="#1-2-数据生命周期合理" class="headerlink" title="1.2 数据生命周期合理"></a>1.2 数据生命周期合理</h2><h2 id="1-3-任务迭代不会严重影响任务产出时间"><a href="#1-3-任务迭代不会严重影响任务产出时间" class="headerlink" title="1.3 任务迭代不会严重影响任务产出时间"></a>1.3 任务迭代不会严重影响任务产出时间</h2><h1 id="二-规范"><a href="#二-规范" class="headerlink" title="二 规范"></a>二 规范</h1><h2 id="2-1-DDL语句规范"><a href="#2-1-DDL语句规范" class="headerlink" title="2.1 DDL语句规范"></a>2.1 DDL语句规范</h2><h3 id="2-1-1-建表规范"><a href="#2-1-1-建表规范" class="headerlink" title="2.1.1 建表规范"></a>2.1.1 建表规范</h3><h3 id="2-1-2-新增单个字段规范"><a href="#2-1-2-新增单个字段规范" class="headerlink" title="2.1.2 新增单个字段规范"></a>2.1.2 新增单个字段规范</h3><h3 id="2-1-3-新增多个字段规范"><a href="#2-1-3-新增多个字段规范" class="headerlink" title="2.1.3 新增多个字段规范"></a>2.1.3 新增多个字段规范</h3><h3 id="2-1-4-修改表注释规范"><a href="#2-1-4-修改表注释规范" class="headerlink" title="2.1.4 修改表注释规范"></a>2.1.4 修改表注释规范</h3><h3 id="2-1-5-修改字段注释规范"><a href="#2-1-5-修改字段注释规范" class="headerlink" title="2.1.5 修改字段注释规范"></a>2.1.5 修改字段注释规范</h3><h3 id="2-1-6-删除表规范"><a href="#2-1-6-删除表规范" class="headerlink" title="2.1.6 删除表规范"></a>2.1.6 删除表规范</h3><h3 id="2-1-7-DDL语句维护规范"><a href="#2-1-7-DDL语句维护规范" class="headerlink" title="2.1.7 DDL语句维护规范"></a>2.1.7 DDL语句维护规范</h3><p>目前的方式1（直接在任务上面写）： </p><p>又或者目前的方式2（任务上面不写，先创建好表，再开发任务，需要查看建表语句的话，需要 show create table 查看）: </p><p>这2种方式都是有弊端的： </p><ul><li><p>没做到统一管理统一维护 </p></li><li><p>查询建表语句比较费时间 </p></li><li><p>无法做到表结构变更全流程监控 </p></li></ul><p>所以需要调整一下目前的方式 </p><p>在每个项目每个分层下面新建一个DDL目录 </p><p>DDL目录下面，每个任务新建一个文件，命名方式 ddl_表名，如： ddl_tablename</p><h2 id="2-2-临时表命名规范（待定）"><a href="#2-2-临时表命名规范（待定）" class="headerlink" title="2.2 临时表命名规范（待定）"></a>2.2 临时表命名规范（待定）</h2><h2 id="2-3-sql编写规范"><a href="#2-3-sql编写规范" class="headerlink" title="2.3 sql编写规范"></a>2.3 sql编写规范</h2><h3 id="2-3-1-原则定义"><a href="#2-3-1-原则定义" class="headerlink" title="2.3.1 原则定义"></a>2.3.1 原则定义</h3><ul><li><p>要求代码行清晰、整齐，具有一定的可观赏性 </p></li><li><p>代码编写要充分考虑执行速度最优的原则 </p></li><li><p>代码行整体层次分明、结构化强 </p></li><li><p>代码中应有必要的注释以增强代码的可读性 </p></li><li><p>规范要求非强制性约束代码开发人员的代码编写行为，在实际应用中在不违反常规要求的前提下允许存在可理解的偏差 </p></li></ul><h3 id="2-3-2-大小写规则"><a href="#2-3-2-大小写规则" class="headerlink" title="2.3.2 大小写规则"></a>2.3.2 大小写规则</h3><ul><li><p>所有的SQL语句中的保留字：全部小写 </p></li><li><p>表名、视图名、宏和存储过程名：全部小写 </p></li><li><p>字段名：全部小写 </p></li></ul><h3 id="2-3-3-缩进与换行"><a href="#2-3-3-缩进与换行" class="headerlink" title="2.3.3 缩进与换行"></a>2.3.3 缩进与换行</h3><ul><li><p>整个的SQL语句最好按照子句进行分行编写，SELECT、FROM、WHERE、UPDATE、INSERT 等每个关键字都要另起一行 </p></li><li><p>同一级别的子句间要对齐 </p></li><li><p>逗号放在每行的开头 </p></li><li><p>分号放在SQL语句的最后，单独占一行 </p></li><li><p>每行宽度不超过80字符（每个字符为8个点阵宽），超过行宽的代码可折行与上行对齐编排 </p></li><li><p>在所有需要缩进的地方，每次缩进4格；在以下情况下需要缩进： </p></li></ul><p> 不同层次的SQL语句之间 </p><p> SELECT INSERT等关键字之后的字段列表和关键字之间 </p><h3 id="2-3-4-运算符前后间隔要求"><a href="#2-3-4-运算符前后间隔要求" class="headerlink" title="2.3.4 运算符前后间隔要求"></a>2.3.4 运算符前后间隔要求</h3><ul><li>算术运算符、逻辑运算符的前后保留一个空格 </li></ul><h3 id="2-3-5-注释"><a href="#2-3-5-注释" class="headerlink" title="2.3.5 注释"></a>2.3.5 注释</h3><ul><li><p>每个字段后面使用字段中文名作为注释 </p></li><li><p>针对复杂的SQL语句，请尽量增加相应的注释说明，以便自己和其它同事事后可以比较容易的读懂和修改。 </p></li><li><p>无效脚本可采用单行/多行注释。（– 或 /* */ 方式） </p></li><li><p>脚本注释中应包含以下内容： </p></li></ul><p> a.编写人/编写日期 </p><p> b.修改人/修改日期 </p><p> c.该脚本的编写目的与主要内容 </p><p> d.如果有特殊处理、特别的技巧等内容，一定要在注释中详细说明 </p><p> e.每一段SQL的前面必须要有注释，重点说明该SQL的技术含义和应用 </p><p> 含义、选择理由。技术含义指这段SQL在技术上这么写的原因、好处 </p><p> ，应用含义指这段SQL从应用的角度将，是为了达到一个什么样的目的 </p><p> 。如果有可能，还需要说明为什么这么选择，而不选择别的方式的理 </p><p> 由。如果发现了不足，还可记录不足的原因和建议的解决办法等 </p><ul><li>建表注释规则 表中文名+“ |create <a href="http://byzhutao/" target="_blank" rel="noopener">by:</a>”+建表人账户 +“ at ”+创建时间’ </li></ul><h3 id="2-3-6-连接的使用"><a href="#2-3-6-连接的使用" class="headerlink" title="2.3.6 连接的使用"></a>2.3.6 连接的使用</h3><ul><li><p>表中的字段若是从其它表引用的，要确保该字段在被引用的表中存在 </p></li><li><p>要求所有的连接都写成join的形式，写完整 比如 inner join </p></li><li><p>连接符or、in、and、以及＝、=等前后加上一个空格。 </p></li><li><p>多表连接时，使用表的别名来引用列。如 t1.user_id t2.user_name</p></li></ul><h3 id="2-3-7-其他"><a href="#2-3-7-其他" class="headerlink" title="2.3.7 其他"></a>2.3.7 其他</h3><ul><li><p>整表数据删除时使用TRUNCATE效率高于DELETE </p></li><li><p>尽量避免使用NOT IN语句，而改用LEFT JOIN方式进行判断筛选，除非是特殊情况 </p></li><li><p>JOIN时对于关联字段值可能为NULL时，应将NULL值转换为随机值后（rand）再进行关联 </p></li><li><p>表名前面需要加上项目名，一个是比较清晰，另一个是如果需要把该任务整段代码移到其他项目去跑的时候（比如分析查询项目），需要手动加上项目名，否则会报错</p></li><li><p>不要出现select * 这样的操作，需要哪些字段就写哪些字段，主要有2个原因 </p></li></ul><p>提升查询效率 </p><p> 降低维护成本（如果使用select * ，无论是insert into 还会union all操作，上游新增字段都会直接导致 该任务报错） </p>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-4-数据清洗规范</title>
      <link href="/2021/08/30/0001-4-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/30/0001-4-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-4-数据清洗规范"><a href="#0001-4-数据清洗规范" class="headerlink" title="0001-4-数据清洗规范"></a>0001-4-数据清洗规范</h1><h1 id="1-单位统一"><a href="#1-单位统一" class="headerlink" title="1.单位统一"></a>1.单位统一</h1><ul><li>比如金额单位统一为 元/美元/分/美分 </li></ul><h1 id="2-字段类型统一"><a href="#2-字段类型统一" class="headerlink" title="2.字段类型统一"></a>2.字段类型统一</h1><p>如 平台id，统一为int或者string，避免出现2个不同的表定义不同的类型 </p><p> product_id,统一定义为 string，不要出现a表定义为string，b表定义为bigint </p><h1 id="3-注释补全"><a href="#3-注释补全" class="headerlink" title="3.注释补全"></a>3.注释补全</h1><ul><li>对没有注释的字段，需要及时补全 </li><li>新建表的时候，一定要带上注释 </li></ul><h1 id="4-时间格式统一"><a href="#4-时间格式统一" class="headerlink" title="4.时间格式统一"></a>4.时间格式统一</h1><p>如2020-10-16，2020/10/16，20201016统一格式为20201016 </p><h1 id="5-枚举值统一"><a href="#5-枚举值统一" class="headerlink" title="5.枚举值统一"></a>5.枚举值统一</h1><p>A表 1:男 2:女 </p><p>B表 0:男 1女 </p><p>统一为：1:男 2:女 </p><h1 id="6-数据脱敏"><a href="#6-数据脱敏" class="headerlink" title="6. 数据脱敏"></a>6. 数据脱敏</h1><p>比如身份证号，手机号，邮箱等需要用加密函数进行脱敏 </p><h1 id="7-空值替换"><a href="#7-空值替换" class="headerlink" title="7.空值替换"></a>7.空值替换</h1><p>使用默认值或者中位数填充 </p><table><thead><tr><th>字段类型</th><th>缺省值</th><th>备注</th></tr></thead><tbody><tr><td>string</td><td>‘-99’ (待定)</td><td>id,文字描述，人名等</td></tr><tr><td>int</td><td>0</td><td>数量/次数</td></tr><tr><td>bigint</td><td>0</td><td>数量/次数</td></tr><tr><td>smallint/tinyint</td><td>0</td><td>是否</td></tr><tr><td>double</td><td>0</td><td>金额等</td></tr><tr><td>decimal(16,4)</td><td>0</td><td>金额等</td></tr><tr><td>date</td><td>1700-01-01 (待定)</td><td>日期(年月日)</td></tr><tr><td>timestamp</td><td>1700-01-01 00:00:00 (待定)</td><td>时间(年月日时分秒)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-3-数据域</title>
      <link href="/2021/08/30/0001-3-%E6%95%B0%E6%8D%AE%E5%9F%9F/"/>
      <url>/2021/08/30/0001-3-%E6%95%B0%E6%8D%AE%E5%9F%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-3-数据域"><a href="#0001-3-数据域" class="headerlink" title="0001-3-数据域"></a>0001-3-数据域</h1><table><thead><tr><th>数据域</th><th>英文缩写</th><th>业务过程举例</th><th>备注</th></tr></thead><tbody><tr><td>会员域</td><td>vip</td><td>下单，退款，续费</td><td></td></tr><tr><td>日志域</td><td>log</td><td>曝光，浏览，点击</td><td></td></tr><tr><td>交易域</td><td>trd</td><td></td><td></td></tr><tr><td>。。。。。。</td><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-2-词根整理</title>
      <link href="/2021/08/30/0001-2-%E8%AF%8D%E6%A0%B9%E6%95%B4%E7%90%86/"/>
      <url>/2021/08/30/0001-2-%E8%AF%8D%E6%A0%B9%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-2-词根整理"><a href="#0001-2-词根整理" class="headerlink" title="0001-2-词根整理"></a>0001-2-词根整理</h1><ul><li>新词根插入前，首先检查下是否已存在，如果已存在则不需要插入</li><li>如果不存在则需要将新词根追加至后面，追加之前最后进行评审</li></ul><table><thead><tr><th>id</th><th>英文词根</th><th>中文词根</th><th>使用频率</th><th>评审日期</th><th>负责人</th></tr></thead><tbody><tr><td>1</td><td>1d</td><td>最近1天</td><td>高</td><td></td><td></td></tr><tr><td>2</td><td>1h</td><td>最近1小时</td><td>高</td><td></td><td></td></tr><tr><td>3</td><td>1m</td><td>最近1月</td><td>高</td><td></td><td></td></tr><tr><td>4</td><td>1w</td><td>最近1周</td><td>高</td><td></td><td></td></tr><tr><td>5</td><td>3d</td><td>最近3天</td><td>高</td><td></td><td></td></tr><tr><td>6</td><td>7d</td><td>最近7天</td><td>高</td><td></td><td></td></tr><tr><td>7</td><td>7ad</td><td>最近7天日均</td><td>高</td><td></td><td></td></tr><tr><td>8</td><td>30d</td><td>最近30天</td><td>高</td><td></td><td></td></tr><tr><td>9</td><td>age</td><td>年龄</td><td>高</td><td></td><td></td></tr><tr><td>10</td><td>amt</td><td>金额</td><td>高</td><td></td><td></td></tr><tr><td>11</td><td>cnt</td><td>数量</td><td>高</td><td></td><td></td></tr><tr><td>12</td><td>abnorm</td><td>异常</td><td>高</td><td></td><td></td></tr><tr><td>13</td><td>account</td><td>账号</td><td>高</td><td></td><td></td></tr><tr><td>14</td><td>activate</td><td>激活</td><td>高</td><td></td><td></td></tr><tr><td>15</td><td>active</td><td>活跃</td><td>高</td><td></td><td></td></tr><tr><td>16</td><td>activity</td><td>活动</td><td>高</td><td></td><td></td></tr><tr><td>17</td><td>actual</td><td>实际</td><td>高</td><td></td><td></td></tr><tr><td>18</td><td>addr</td><td>地址</td><td>高</td><td></td><td></td></tr><tr><td>19</td><td>admin</td><td>管理</td><td>高</td><td></td><td></td></tr><tr><td>20</td><td>alias</td><td>别名</td><td>高</td><td></td><td></td></tr><tr><td>21</td><td>alipay</td><td>支付宝</td><td>高</td><td></td><td></td></tr><tr><td>22</td><td>app</td><td>应用</td><td>高</td><td></td><td></td></tr><tr><td>23</td><td>apply</td><td>申请</td><td>高</td><td></td><td></td></tr><tr><td>24</td><td>approval</td><td>批准</td><td>高</td><td></td><td></td></tr><tr><td>25</td><td>area</td><td>地区、区域</td><td>高</td><td></td><td></td></tr><tr><td>27</td><td>attitude</td><td>态度</td><td>高</td><td></td><td></td></tr><tr><td>28</td><td>audit</td><td>审核</td><td>高</td><td></td><td></td></tr><tr><td>29</td><td>avg</td><td>平均</td><td>高</td><td></td><td></td></tr><tr><td>30</td><td>basic</td><td>基本</td><td>高</td><td></td><td></td></tr><tr><td>31</td><td>bill</td><td>账单</td><td>高</td><td></td><td></td></tr><tr><td>32</td><td>bind</td><td>绑定</td><td>高</td><td></td><td></td></tr><tr><td>33</td><td>birth</td><td>出生</td><td>高</td><td></td><td></td></tr><tr><td>34</td><td>birthday</td><td>生日</td><td>高</td><td></td><td></td></tr><tr><td>35</td><td>biz</td><td>业务</td><td>高</td><td></td><td></td></tr><tr><td>36</td><td>browser</td><td>浏览器</td><td>高</td><td></td><td></td></tr><tr><td>37</td><td>call</td><td>呼叫</td><td>高</td><td></td><td></td></tr><tr><td>38</td><td>cancel</td><td>取消</td><td>高</td><td></td><td></td></tr><tr><td>39</td><td>cash</td><td>现金</td><td>高</td><td></td><td></td></tr><tr><td>40</td><td>category</td><td>类别</td><td>高</td><td></td><td></td></tr><tr><td>41</td><td>census</td><td>户籍</td><td>高</td><td></td><td></td></tr><tr><td>42</td><td>cert</td><td>凭证、认证、身份证</td><td>高</td><td></td><td></td></tr><tr><td>43</td><td>channel</td><td>渠道</td><td>高</td><td></td><td></td></tr><tr><td>44</td><td>charge</td><td>收费</td><td>高</td><td></td><td></td></tr><tr><td>45</td><td>chat</td><td>聊天</td><td>高</td><td></td><td></td></tr><tr><td>46</td><td>check</td><td>检查、稽核</td><td>高</td><td></td><td></td></tr><tr><td>47</td><td>city</td><td>城市</td><td>高</td><td></td><td></td></tr><tr><td>48</td><td>classify</td><td>分类</td><td>高</td><td></td><td></td></tr><tr><td>49</td><td>click</td><td>点击</td><td>高</td><td></td><td></td></tr><tr><td>50</td><td>client</td><td>客户端</td><td>高</td><td></td><td></td></tr><tr><td>51</td><td>close</td><td>关闭</td><td>高</td><td></td><td></td></tr><tr><td>52</td><td>cloud</td><td>云</td><td>高</td><td></td><td></td></tr><tr><td>53</td><td>cn</td><td>中文</td><td>高</td><td></td><td></td></tr><tr><td>54</td><td>cnt</td><td>总数、数量</td><td>高</td><td></td><td></td></tr><tr><td>55</td><td>code</td><td>代码</td><td>高</td><td></td><td></td></tr><tr><td>56</td><td>collect</td><td>收集</td><td>高</td><td></td><td></td></tr><tr><td>57</td><td>comment</td><td>评价</td><td>高</td><td></td><td></td></tr><tr><td>58</td><td>common</td><td>常见</td><td>高</td><td></td><td></td></tr><tr><td>59</td><td>commu</td><td>沟通</td><td>高</td><td></td><td></td></tr><tr><td>60</td><td>company</td><td>公司、单位</td><td>高</td><td></td><td></td></tr><tr><td>61</td><td>complain</td><td>投诉</td><td>高</td><td></td><td></td></tr><tr><td>62</td><td>config</td><td>配置</td><td>高</td><td></td><td></td></tr><tr><td>63</td><td>confirm</td><td>确认</td><td>高</td><td></td><td></td></tr><tr><td>64</td><td>conflict</td><td>冲突</td><td>高</td><td></td><td></td></tr><tr><td>65</td><td>connected</td><td>连接、接通</td><td>高</td><td></td><td></td></tr><tr><td>66</td><td>consult</td><td>咨询</td><td>高</td><td></td><td></td></tr><tr><td>67</td><td>contact</td><td>联系</td><td>高</td><td></td><td></td></tr><tr><td>68</td><td>content</td><td>内容</td><td>高</td><td></td><td></td></tr><tr><td>69</td><td>cost</td><td>成本</td><td>高</td><td></td><td></td></tr><tr><td>70</td><td>country</td><td>国家</td><td>高</td><td></td><td></td></tr><tr><td>71</td><td>county</td><td>区县</td><td>高</td><td></td><td></td></tr><tr><td>72</td><td>coupon</td><td>优惠券</td><td>高</td><td></td><td></td></tr><tr><td>73</td><td>create</td><td>创建</td><td>高</td><td></td><td></td></tr><tr><td>74</td><td>creator</td><td>创建者</td><td>高</td><td></td><td></td></tr><tr><td>75</td><td>cube</td><td>cube</td><td>高</td><td></td><td></td></tr><tr><td>76</td><td>cur</td><td>当前</td><td>高</td><td></td><td></td></tr><tr><td>77</td><td>customer</td><td>客服</td><td>高</td><td></td><td></td></tr><tr><td>78</td><td>dau</td><td>日活</td><td>高</td><td></td><td></td></tr><tr><td>79</td><td>daily</td><td>每日</td><td>高</td><td></td><td></td></tr><tr><td>80</td><td>data</td><td>数据</td><td>高</td><td></td><td></td></tr><tr><td>81</td><td>date</td><td>日期</td><td>高</td><td></td><td></td></tr><tr><td>82</td><td>day</td><td>天</td><td>高</td><td></td><td></td></tr><tr><td>83</td><td>deal</td><td>处理</td><td>高</td><td></td><td></td></tr><tr><td>84</td><td>dec</td><td>解密</td><td>高</td><td></td><td></td></tr><tr><td>85</td><td>deleted</td><td>删除</td><td>高</td><td></td><td></td></tr><tr><td>87</td><td>desc</td><td>描述</td><td>高</td><td></td><td></td></tr><tr><td>88</td><td>detail</td><td>明细</td><td>高</td><td></td><td></td></tr><tr><td>89</td><td>device</td><td>设备</td><td>高</td><td></td><td></td></tr><tr><td>90</td><td>discount</td><td>优惠、折扣</td><td>高</td><td></td><td></td></tr><tr><td>91</td><td>disp</td><td>显示</td><td>高</td><td></td><td></td></tr><tr><td>92</td><td>doc</td><td>文档</td><td>高</td><td></td><td></td></tr><tr><td>93</td><td>done</td><td>完成</td><td>高</td><td></td><td></td></tr><tr><td>94</td><td>dur</td><td>期间、时长</td><td>高</td><td></td><td></td></tr><tr><td>95</td><td>eff</td><td>生效</td><td>高</td><td></td><td></td></tr><tr><td>97</td><td>en</td><td>英文</td><td>高</td><td></td><td></td></tr><tr><td>98</td><td>enc</td><td>加密</td><td>高</td><td></td><td></td></tr><tr><td>99</td><td>end</td><td>结束</td><td>高</td><td></td><td></td></tr><tr><td>100</td><td>entrance</td><td>入口</td><td>高</td><td></td><td></td></tr><tr><td>101</td><td>enum</td><td>枚举</td><td>高</td><td></td><td></td></tr><tr><td>102</td><td>env</td><td>环境</td><td>高</td><td></td><td></td></tr><tr><td>103</td><td>event</td><td>事件</td><td>高</td><td></td><td></td></tr><tr><td>104</td><td>exceed</td><td>过期</td><td>高</td><td></td><td></td></tr><tr><td>105</td><td>exe</td><td>执行</td><td>高</td><td></td><td></td></tr><tr><td>106</td><td>exp</td><td>失效</td><td>高</td><td></td><td></td></tr><tr><td>108</td><td>ext</td><td>扩展、外部</td><td>高</td><td></td><td></td></tr><tr><td>109</td><td>extend</td><td>内部</td><td>高</td><td></td><td></td></tr><tr><td>110</td><td>external</td><td>外部</td><td>高</td><td></td><td></td></tr><tr><td>111</td><td>f4w</td><td>未来4周</td><td>高</td><td></td><td></td></tr><tr><td>112</td><td>f7d</td><td>未来7天</td><td>高</td><td></td><td></td></tr><tr><td>114</td><td>fee</td><td>费用</td><td>高</td><td></td><td></td></tr><tr><td>115</td><td>field</td><td>字段</td><td>高</td><td></td><td></td></tr><tr><td>116</td><td>file</td><td>文件</td><td>高</td><td></td><td></td></tr><tr><td>117</td><td>filter</td><td>筛选</td><td>高</td><td></td><td></td></tr><tr><td>118</td><td>fst</td><td>首次</td><td>高</td><td></td><td></td></tr><tr><td>119</td><td>first</td><td>第一</td><td>高</td><td></td><td></td></tr><tr><td>120</td><td>flag</td><td>标识</td><td>高</td><td></td><td></td></tr><tr><td>121</td><td>follow</td><td>关注</td><td>高</td><td></td><td></td></tr><tr><td>122</td><td>frequency</td><td>频率</td><td>高</td><td></td><td></td></tr><tr><td>123</td><td>get</td><td>获取</td><td>高</td><td></td><td></td></tr><tr><td>124</td><td>goods</td><td>商品</td><td>高</td><td></td><td></td></tr><tr><td>125</td><td>graded</td><td>职务</td><td>高</td><td></td><td></td></tr><tr><td>126</td><td>group</td><td>组</td><td>高</td><td></td><td></td></tr><tr><td>127</td><td>history</td><td>历史</td><td>高</td><td></td><td></td></tr><tr><td>128</td><td>hour</td><td>小时</td><td>高</td><td></td><td></td></tr><tr><td>129</td><td>code</td><td>编码</td><td>高</td><td></td><td></td></tr><tr><td>130</td><td>idcard</td><td>身份证</td><td>高</td><td></td><td></td></tr><tr><td>131</td><td>identify</td><td>证件</td><td>高</td><td></td><td></td></tr><tr><td>132</td><td>image</td><td>图片</td><td>高</td><td></td><td></td></tr><tr><td>133</td><td>import</td><td>导入</td><td>高</td><td></td><td></td></tr><tr><td>134</td><td>income</td><td>收入</td><td>高</td><td></td><td></td></tr><tr><td>135</td><td>incr</td><td>新增</td><td>高</td><td></td><td></td></tr><tr><td>136</td><td>index</td><td>指数、指标</td><td>高</td><td></td><td></td></tr><tr><td>137</td><td>info</td><td>信息</td><td>高</td><td></td><td></td></tr><tr><td>138</td><td>inspect</td><td>检测</td><td>高</td><td></td><td></td></tr><tr><td>139</td><td>interval</td><td>间隔</td><td>高</td><td></td><td></td></tr><tr><td>140</td><td>intro</td><td>介绍</td><td>高</td><td></td><td></td></tr><tr><td>141</td><td>invite</td><td>邀请</td><td>高</td><td></td><td></td></tr><tr><td>142</td><td>invitee</td><td>被邀请人</td><td>高</td><td></td><td></td></tr><tr><td>143</td><td>is</td><td>是否</td><td>高</td><td></td><td></td></tr><tr><td>144</td><td>item</td><td>项</td><td>高</td><td></td><td></td></tr><tr><td>145</td><td>job</td><td>职业</td><td>高</td><td></td><td></td></tr><tr><td>146</td><td>lst</td><td>最近</td><td>高</td><td></td><td></td></tr><tr><td>147</td><td>latitude</td><td>纬度</td><td>高</td><td></td><td></td></tr><tr><td>148</td><td>leadin</td><td>引入</td><td>高</td><td></td><td></td></tr><tr><td>149</td><td>lev2</td><td>二级</td><td>高</td><td></td><td></td></tr><tr><td>150</td><td>level</td><td>等级</td><td>高</td><td></td><td></td></tr><tr><td>151</td><td>limit</td><td>限制</td><td>高</td><td></td><td></td></tr><tr><td>152</td><td>link</td><td>关联</td><td>高</td><td></td><td></td></tr><tr><td>153</td><td>log</td><td>日志</td><td>高</td><td></td><td></td></tr><tr><td>154</td><td>login</td><td>登录</td><td>高</td><td></td><td></td></tr><tr><td>155</td><td>longitude</td><td>经度</td><td>高</td><td></td><td></td></tr><tr><td>156</td><td>manual</td><td>手工维护</td><td>高</td><td></td><td></td></tr><tr><td>157</td><td>mask</td><td>掩码</td><td>高</td><td></td><td></td></tr><tr><td>158</td><td>match</td><td>匹配</td><td>高</td><td></td><td></td></tr><tr><td>159</td><td>max</td><td>最大值</td><td>高</td><td></td><td></td></tr><tr><td>160</td><td>vip</td><td>会员</td><td>高</td><td></td><td></td></tr><tr><td>161</td><td>memo</td><td>备忘、说明</td><td>高</td><td></td><td></td></tr><tr><td>162</td><td>method</td><td>方法</td><td>高</td><td></td><td></td></tr><tr><td>163</td><td>min</td><td>最小</td><td>高</td><td></td><td></td></tr><tr><td>164</td><td>minute</td><td>分钟</td><td>高</td><td></td><td></td></tr><tr><td>165</td><td>mode</td><td>模式</td><td>高</td><td></td><td></td></tr><tr><td>166</td><td>modify</td><td>修改</td><td>高</td><td></td><td></td></tr><tr><td>167</td><td>month</td><td>月份</td><td>高</td><td></td><td></td></tr><tr><td>168</td><td>msg</td><td>消息</td><td>高</td><td></td><td></td></tr><tr><td>169</td><td>name</td><td>名称、姓名</td><td>高</td><td></td><td></td></tr><tr><td>170</td><td>need</td><td>需要</td><td>高</td><td></td><td></td></tr><tr><td>171</td><td>neg_comment</td><td>差评</td><td>高</td><td></td><td></td></tr><tr><td>172</td><td>next</td><td>下（月）</td><td>高</td><td></td><td></td></tr><tr><td>173</td><td>nick</td><td>昵称</td><td>高</td><td></td><td></td></tr><tr><td>174</td><td>nm</td><td>自然月</td><td>高</td><td></td><td></td></tr><tr><td>175</td><td>no</td><td>序号</td><td>高</td><td></td><td></td></tr><tr><td>176</td><td>nq</td><td>自然季度</td><td>高</td><td></td><td></td></tr><tr><td>177</td><td>number</td><td>号码</td><td>高</td><td></td><td></td></tr><tr><td>178</td><td>nw</td><td>自然周</td><td>高</td><td></td><td></td></tr><tr><td>179</td><td>online</td><td>在线</td><td>高</td><td></td><td></td></tr><tr><td>180</td><td>open</td><td>开通</td><td>高</td><td></td><td></td></tr><tr><td>181</td><td>operate</td><td>操作</td><td>高</td><td></td><td></td></tr><tr><td>182</td><td>operator</td><td>操作人、执行人</td><td>高</td><td></td><td></td></tr><tr><td>183</td><td>opt</td><td>操作</td><td>高</td><td></td><td></td></tr><tr><td>184</td><td>order</td><td>订单</td><td>高</td><td></td><td></td></tr><tr><td>186</td><td>origin</td><td>原始</td><td>高</td><td></td><td></td></tr><tr><td>187</td><td>other</td><td>其他</td><td>高</td><td></td><td></td></tr><tr><td>188</td><td>outtime</td><td>超时</td><td>高</td><td></td><td></td></tr><tr><td>189</td><td>package</td><td>包</td><td>高</td><td></td><td></td></tr><tr><td>190</td><td>page</td><td>页面</td><td>高</td><td></td><td></td></tr><tr><td>191</td><td>parent</td><td>父</td><td>高</td><td></td><td></td></tr><tr><td>192</td><td>path</td><td>路径</td><td>高</td><td></td><td></td></tr><tr><td>193</td><td>pay</td><td>支付</td><td>高</td><td></td><td></td></tr><tr><td>194</td><td>payable</td><td>应付</td><td>高</td><td></td><td></td></tr><tr><td>195</td><td>payment</td><td>支付单</td><td>高</td><td></td><td></td></tr><tr><td>196</td><td>people</td><td>人口</td><td>高</td><td></td><td></td></tr><tr><td>197</td><td>person</td><td>人</td><td>高</td><td></td><td></td></tr><tr><td>198</td><td>phone</td><td>手机</td><td>高</td><td></td><td></td></tr><tr><td>199</td><td>pkid</td><td>主键ID</td><td>高</td><td></td><td></td></tr><tr><td>200</td><td>place</td><td>地点</td><td>高</td><td></td><td></td></tr><tr><td>201</td><td>planning</td><td>计划</td><td>高</td><td></td><td></td></tr><tr><td>202</td><td>plf</td><td>平台</td><td>高</td><td></td><td></td></tr><tr><td>204</td><td>pos_comment</td><td>好评</td><td>高</td><td></td><td></td></tr><tr><td>205</td><td>post</td><td>发布</td><td>高</td><td></td><td></td></tr><tr><td>206</td><td>poster</td><td>发帖人</td><td>高</td><td></td><td></td></tr><tr><td>207</td><td>pre</td><td>上（月）</td><td>高</td><td></td><td></td></tr><tr><td>208</td><td>price</td><td>价格</td><td>高</td><td></td><td></td></tr><tr><td>209</td><td>priority</td><td>优先级</td><td>高</td><td></td><td></td></tr><tr><td>210</td><td>prod</td><td>生产</td><td>高</td><td></td><td></td></tr><tr><td>211</td><td>product</td><td>产品</td><td>高</td><td></td><td></td></tr><tr><td>212</td><td>project</td><td>项目</td><td>高</td><td></td><td></td></tr><tr><td>213</td><td>provider</td><td>提供者</td><td>高</td><td></td><td></td></tr><tr><td>214</td><td>province</td><td>省份</td><td>高</td><td></td><td></td></tr><tr><td>215</td><td>proxy</td><td>代理</td><td>高</td><td></td><td></td></tr><tr><td>216</td><td>pv</td><td>访问量</td><td>高</td><td></td><td></td></tr><tr><td>217</td><td>qrcode</td><td>二维码</td><td>高</td><td></td><td></td></tr><tr><td>218</td><td>quantity</td><td>数量</td><td>高</td><td></td><td></td></tr><tr><td>219</td><td>query</td><td>查询</td><td>高</td><td></td><td></td></tr><tr><td>220</td><td>range</td><td>范围</td><td>高</td><td></td><td></td></tr><tr><td>221</td><td>rank</td><td>排行</td><td>高</td><td></td><td></td></tr><tr><td>222</td><td>rate</td><td>百分比</td><td>高</td><td></td><td></td></tr><tr><td>223</td><td>read</td><td>读取</td><td>高</td><td></td><td></td></tr><tr><td>224</td><td>reason</td><td>原因</td><td>高</td><td></td><td></td></tr><tr><td>225</td><td>receive</td><td>接收</td><td>高</td><td></td><td></td></tr><tr><td>226</td><td>record</td><td>记录</td><td>高</td><td></td><td></td></tr><tr><td>227</td><td>refund</td><td>退款</td><td>高</td><td></td><td></td></tr><tr><td>228</td><td>reg</td><td>注册</td><td>高</td><td></td><td></td></tr><tr><td>229</td><td>region</td><td>片区</td><td>高</td><td></td><td></td></tr><tr><td>230</td><td>remain</td><td>剩余的</td><td>高</td><td></td><td></td></tr><tr><td>231</td><td>remark</td><td>备注</td><td>高</td><td></td><td></td></tr><tr><td>232</td><td>renew</td><td>续费</td><td>高</td><td></td><td></td></tr><tr><td>233</td><td>reply</td><td>回复、回帖</td><td>高</td><td></td><td></td></tr><tr><td>234</td><td>report</td><td>报告</td><td>高</td><td></td><td></td></tr><tr><td>235</td><td>request</td><td>请求</td><td>高</td><td></td><td></td></tr><tr><td>236</td><td>reserve</td><td>预定、预约</td><td>高</td><td></td><td></td></tr><tr><td>237</td><td>resource</td><td>资源</td><td>高</td><td></td><td></td></tr><tr><td>238</td><td>response</td><td>响应</td><td>高</td><td></td><td></td></tr><tr><td>239</td><td>result</td><td>结果</td><td>高</td><td></td><td></td></tr><tr><td>240</td><td>return</td><td>返回</td><td>高</td><td></td><td></td></tr><tr><td>241</td><td>review</td><td>复审</td><td>高</td><td></td><td></td></tr><tr><td>242</td><td>role</td><td>角色</td><td>高</td><td></td><td></td></tr><tr><td>243</td><td>room</td><td>房间</td><td>高</td><td></td><td></td></tr><tr><td>244</td><td>root</td><td>根</td><td>高</td><td></td><td></td></tr><tr><td>245</td><td>rule</td><td>规则</td><td>高</td><td></td><td></td></tr><tr><td>246</td><td>scan</td><td>扫码</td><td>高</td><td></td><td></td></tr><tr><td>247</td><td>score</td><td>分数、评分</td><td>高</td><td></td><td></td></tr><tr><td>248</td><td>screen</td><td>屏幕</td><td>高</td><td></td><td></td></tr><tr><td>249</td><td>search</td><td>搜索</td><td>高</td><td></td><td></td></tr><tr><td>250</td><td>send</td><td>发送</td><td>高</td><td></td><td></td></tr><tr><td>251</td><td>seq</td><td>序列号</td><td>高</td><td></td><td></td></tr><tr><td>252</td><td>serial</td><td>流水</td><td>高</td><td></td><td></td></tr><tr><td>253</td><td>serv</td><td>服务</td><td>高</td><td></td><td></td></tr><tr><td>254</td><td>session</td><td>会话</td><td>高</td><td></td><td></td></tr><tr><td>255</td><td>sex</td><td>性别</td><td>高</td><td></td><td></td></tr><tr><td>256</td><td>share</td><td>分享</td><td>高</td><td></td><td></td></tr><tr><td>257</td><td>shield</td><td>屏蔽</td><td>高</td><td></td><td></td></tr><tr><td>258</td><td>show</td><td>展示</td><td>高</td><td></td><td></td></tr><tr><td>259</td><td>sign</td><td>签约</td><td>高</td><td></td><td></td></tr><tr><td>260</td><td>skip</td><td>评估</td><td>高</td><td></td><td></td></tr><tr><td>261</td><td>sort</td><td>排序</td><td>高</td><td></td><td></td></tr><tr><td>262</td><td>source</td><td>来源</td><td>高</td><td></td><td></td></tr><tr><td>263</td><td>special</td><td>擅长</td><td>高</td><td></td><td></td></tr><tr><td>264</td><td>src</td><td>来源</td><td>高</td><td></td><td></td></tr><tr><td>265</td><td>staff</td><td>人员、员工</td><td>高</td><td></td><td></td></tr><tr><td>266</td><td>start</td><td>开始</td><td>高</td><td></td><td></td></tr><tr><td>267</td><td>stat</td><td>统计</td><td>高</td><td></td><td></td></tr><tr><td>268</td><td>status</td><td>状态</td><td>高</td><td></td><td></td></tr><tr><td>269</td><td>std</td><td>标准</td><td>高</td><td></td><td></td></tr><tr><td>270</td><td>store</td><td>库存</td><td>高</td><td></td><td></td></tr><tr><td>271</td><td>sub</td><td>子</td><td>高</td><td></td><td></td></tr><tr><td>272</td><td>subject</td><td>科目</td><td>高</td><td></td><td></td></tr><tr><td>273</td><td>success</td><td>成功</td><td>高</td><td></td><td></td></tr><tr><td>274</td><td>sum</td><td>汇总</td><td>高</td><td></td><td></td></tr><tr><td>275</td><td>summary</td><td>小结</td><td>高</td><td></td><td></td></tr><tr><td>276</td><td>supplier</td><td>供应商</td><td>高</td><td></td><td></td></tr><tr><td>277</td><td>system</td><td>系统</td><td>高</td><td></td><td></td></tr><tr><td>278</td><td>tag</td><td>标签</td><td>高</td><td></td><td></td></tr><tr><td>279</td><td>target</td><td>目标</td><td>高</td><td></td><td></td></tr><tr><td>280</td><td>task</td><td>任务</td><td>高</td><td></td><td></td></tr><tr><td>281</td><td>td</td><td>截止当日</td><td>高</td><td></td><td></td></tr><tr><td>282</td><td>team</td><td>团队</td><td>高</td><td></td><td></td></tr><tr><td>283</td><td>tel</td><td>电话</td><td>高</td><td></td><td></td></tr><tr><td>285</td><td>template</td><td>模板</td><td>高</td><td></td><td></td></tr><tr><td>286</td><td>test</td><td>测试</td><td>高</td><td></td><td></td></tr><tr><td>287</td><td>text</td><td>文本</td><td>高</td><td></td><td></td></tr><tr><td>288</td><td>ticket</td><td>工单</td><td>高</td><td></td><td></td></tr><tr><td>289</td><td>time</td><td>时间</td><td>高</td><td></td><td></td></tr><tr><td>290</td><td>time_section</td><td>时间段</td><td>高</td><td></td><td></td></tr><tr><td>291</td><td>times</td><td>次数</td><td>高</td><td></td><td></td></tr><tr><td>292</td><td>title</td><td>标题、职称</td><td>高</td><td></td><td></td></tr><tr><td>293</td><td>topic</td><td>主题</td><td>高</td><td></td><td></td></tr><tr><td>294</td><td>total</td><td>总共</td><td>高</td><td></td><td></td></tr><tr><td>295</td><td>town</td><td>乡镇</td><td>高</td><td></td><td></td></tr><tr><td>296</td><td>trade</td><td>交易</td><td>高</td><td></td><td></td></tr><tr><td>297</td><td>ts</td><td>准实时</td><td>高</td><td></td><td></td></tr><tr><td>298</td><td>tt</td><td>零点截止当前</td><td>高</td><td></td><td></td></tr><tr><td>299</td><td>type</td><td>类型</td><td>高</td><td></td><td></td></tr><tr><td>301</td><td>unit</td><td>单位</td><td>高</td><td></td><td></td></tr><tr><td>302</td><td>update</td><td>更新</td><td>高</td><td></td><td></td></tr><tr><td>303</td><td>upload</td><td>上传</td><td>高</td><td></td><td></td></tr><tr><td>304</td><td>used</td><td>使用</td><td>高</td><td></td><td></td></tr><tr><td>305</td><td>user</td><td>用户</td><td>高</td><td></td><td></td></tr><tr><td>306</td><td>uuid</td><td>uuid</td><td>高</td><td></td><td></td></tr><tr><td>307</td><td>uv</td><td>访问用户数</td><td>高</td><td></td><td></td></tr><tr><td>308</td><td>valid</td><td>有效</td><td>高</td><td></td><td></td></tr><tr><td>309</td><td>value</td><td>值</td><td>高</td><td></td><td></td></tr><tr><td>310</td><td>ver</td><td>版本</td><td>高</td><td></td><td></td></tr><tr><td>311</td><td>video</td><td>视频</td><td>高</td><td></td><td></td></tr><tr><td>312</td><td>village</td><td>村</td><td>高</td><td></td><td></td></tr><tr><td>313</td><td>visit</td><td>访问</td><td>高</td><td></td><td></td></tr><tr><td>314</td><td>way</td><td>方式</td><td>高</td><td></td><td></td></tr><tr><td>315</td><td>wechat</td><td>微信</td><td>高</td><td></td><td></td></tr><tr><td>316</td><td>week</td><td>周</td><td>高</td><td></td><td></td></tr><tr><td>317</td><td>weight</td><td>权重</td><td>高</td><td></td><td></td></tr><tr><td>318</td><td>whole</td><td>全部</td><td>高</td><td></td><td></td></tr><tr><td>319</td><td>word</td><td>词</td><td>高</td><td></td><td></td></tr><tr><td>320</td><td>year</td><td>年</td><td>高</td><td></td><td></td></tr><tr><td>321</td><td>ytd</td><td>年初截止当日</td><td>高</td><td></td><td></td></tr><tr><td>322</td><td>zone</td><td>战区</td><td>高</td><td></td><td></td></tr><tr><td>323</td><td>access</td><td>接入、访问</td><td>高</td><td></td><td></td></tr><tr><td>324</td><td>repeat</td><td>重复</td><td>高</td><td></td><td></td></tr><tr><td>325</td><td>division</td><td>分时</td><td>高</td><td></td><td></td></tr><tr><td>326</td><td>owner</td><td>责任人、负责人</td><td>高</td><td></td><td></td></tr><tr><td>327</td><td>evaluate</td><td>评价、评论</td><td>高</td><td></td><td></td></tr><tr><td>330</td><td>relate</td><td>关联</td><td>高</td><td></td><td></td></tr><tr><td>331</td><td>exclusive</td><td>独有的、独占的</td><td>高</td><td></td><td></td></tr><tr><td>332</td><td>deposit</td><td>押金、订金</td><td>高</td><td></td><td></td></tr><tr><td>333</td><td>notify</td><td>通知、通告</td><td>高</td><td></td><td></td></tr><tr><td>334</td><td>receipt</td><td>收据、收条</td><td>高</td><td></td><td></td></tr><tr><td>335</td><td>install</td><td>安装</td><td>高</td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-1-分层规范</title>
      <link href="/2021/08/30/0001-1-%E5%88%86%E5%B1%82%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/30/0001-1-%E5%88%86%E5%B1%82%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-1-分层规范"><a href="#0001-1-分层规范" class="headerlink" title="0001-1-分层规范"></a>0001-1-分层规范</h1><h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>数据仓库一般分为三层，自上而下分别为数据贴源层（ODS，Operation Data Store）、数据公共层（CDM，Common Data Model）和数据应用层（ADS，Application Data Service）。</p><h2 id="二-ODS层"><a href="#二-ODS层" class="headerlink" title="二 ODS层"></a>二 ODS层</h2><p>贴源层，与业务库保持一致，不做任何处理</p><h2 id="三-CDM层"><a href="#三-CDM层" class="headerlink" title="三 CDM层"></a>三 CDM层</h2><p>数据公共层CDM（Common Data Model，又称通用数据模型层），包括DIM维度表、DWD,DW和DWS，由ODS层数据加工而成。主要完成数据加工与整合，建立一致性的维度，构建可复用的面向分析和统计的明细事实表，以及汇总公共粒度的指标</p><p>公共维度层（DIM）：基于维度建模理念思想，建立企业一致性维度。降低数据计算口径和算法不统一风险。     公共维度层的表通常也被称为逻辑维度表，维度和维度逻辑表通常一一对应。</p><p>明细粒度事实层（DWD）：对数据进行规范化编码转换，清洗，统一格式，脱敏等，不做横向整合</p><p>主题宽表层(DW) 对dwd各种信息进行整合，输出主题宽表(面向业务过 程，不同业务过程的信息不冗余建设，采用外键形式)</p><p>公共汇总粒度事实层（DWS）：以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标事实表，以宽表化手段物理化模型。构建命名规范、口径一致的统计指标，为上层提供公共指标，建立汇总宽表、明细事实表。</p><p>公共汇总粒度事实层的表通常也被称为汇总逻辑表，用于存放派生指标数据。</p><h2 id="四-ADS层"><a href="#四-ADS层" class="headerlink" title="四 ADS层"></a>四 ADS层</h2><p>数据应用层ADS（Application Data Service）：面向业务需求定制开发，存放数据产品个性化的统计指标数据。</p><h2 id="五-逻辑分层架构"><a href="#五-逻辑分层架构" class="headerlink" title="五 逻辑分层架构"></a>五 逻辑分层架构</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/8407327/1626350203232-7ff97992-930a-4064-8a68-eac0cffbaa94.png" alt="img1"></p><h2 id="六-分层的好处"><a href="#六-分层的好处" class="headerlink" title="六 分层的好处"></a>六 分层的好处</h2><ul><li><p>清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</p></li><li><p>数据血缘追踪：简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</p></li><li><p>减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</p></li><li><p>把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</p></li></ul><h2 id="七-数据流向"><a href="#七-数据流向" class="headerlink" title="七 数据流向"></a>七 数据流向</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/8407327/1626350246414-50040f21-c68a-4711-b40a-75a46f5c8677.png" alt="img2"></p><ul><li><p>正常流向：ODS-&gt;DWD-&gt;DW-&gt;DWS-&gt;ADS，当出现ODS-&gt;DWD-&gt;DWS-&gt;ADS这种关系时，说明主题域未覆盖全。应将DWD数据落到DW中，对于使用频度非常低的表允许DWD-&gt;DWS。</p></li><li><p>尽量避免出现DWS宽表中使用DWD又使用（该DWD所归属主题域）DW的表。</p></li><li><p>同一主题域内对于DW生成DW的表，原则上要尽量避免，否则会影响ETL的效率。</p></li><li><p>DW、DWS和ADS中禁止直接使用ODS的表， ODS的表只能被DWD引用。</p></li><li><p>禁止出现反向依赖，例如DW的表依赖DWS的表。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据研发规范文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据研发规范文档 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-2-Flink典型ETL场景</title>
      <link href="/2021/08/30/0001-2-Flink%E5%85%B8%E5%9E%8BETL%E5%9C%BA%E6%99%AF/"/>
      <url>/2021/08/30/0001-2-Flink%E5%85%B8%E5%9E%8BETL%E5%9C%BA%E6%99%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-2-Flink典型ETL场景"><a href="#0001-2-Flink典型ETL场景" class="headerlink" title="0001-2-Flink典型ETL场景"></a>0001-2-Flink典型ETL场景</h1><h2 id="1-关联维表"><a href="#1-关联维表" class="headerlink" title="1 关联维表"></a>1 关联维表</h2><h2 id="1-1-预加载维表"><a href="#1-1-预加载维表" class="headerlink" title="1.1 预加载维表"></a>1.1 预加载维表</h2><p>实现RichMapFunction，在open方法中读取数据库中的维度数据全量加载到内存中</p><p>优点：简单</p><p>缺点：适用于数据量小的维表</p><h2 id="1-2-热存储维表"><a href="#1-2-热存储维表" class="headerlink" title="1.2 热存储维表"></a>1.2 热存储维表</h2><p>将维度数据存储待hbase或者redis中，通过异步IO查询热存储，利用cache机制将维度数据缓存在内存</p><p>优点：支持较多的维度数据</p><p>缺点：维度更新有延迟</p><h2 id="1-3-广播维表"><a href="#1-3-广播维表" class="headerlink" title="1.3 广播维表"></a>1.3 广播维表</h2><p>利用broadcast state将维度数据流广播出去</p><p>优点：维度变更可及时更新结果</p><p>缺点：数据保存在内存中，支持的数据量比较小</p><h2 id="2-双流join"><a href="#2-双流join" class="headerlink" title="2 双流join"></a>2 双流join</h2><h2 id="2-1-window-join"><a href="#2-1-window-join" class="headerlink" title="2.1 window join"></a>2.1 window join</h2><h3 id="2-1-1-Tumbling-Window-Join"><a href="#2-1-1-Tumbling-Window-Join" class="headerlink" title="2.1.1  Tumbling Window Join"></a>2.1.1  Tumbling Window Join</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830174800809.png" alt="image-20210830174800809"></p><pre class=" language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeySelector<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>assigners<span class="token punctuation">.</span>TumblingEventTimeWindows<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">;</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> orangeStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> greenStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>orangeStream<span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span>greenStream<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">equalTo</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>TumblingEventTimeWindows<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">apply</span> <span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">JoinFunction</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> String<span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> String <span class="token function">join</span><span class="token punctuation">(</span>Integer first<span class="token punctuation">,</span> Integer second<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> first <span class="token operator">+</span> <span class="token string">","</span> <span class="token operator">+</span> second<span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="2-1-2-Sliding-Window-Join"><a href="#2-1-2-Sliding-Window-Join" class="headerlink" title="2.1.2 Sliding Window Join"></a>2.1.2 Sliding Window Join</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830174817065.png" alt="image-20210830174817065"></p><pre class=" language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeySelector<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>assigners<span class="token punctuation">.</span>SlidingEventTimeWindows<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">;</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> orangeStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> greenStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>orangeStream<span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span>greenStream<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">equalTo</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>SlidingEventTimeWindows<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">/* size */</span><span class="token punctuation">,</span> Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">/* slide */</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">apply</span> <span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">JoinFunction</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> String<span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> String <span class="token function">join</span><span class="token punctuation">(</span>Integer first<span class="token punctuation">,</span> Integer second<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> first <span class="token operator">+</span> <span class="token string">","</span> <span class="token operator">+</span> second<span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="2-1-3-Session-Window-Join"><a href="#2-1-3-Session-Window-Join" class="headerlink" title="2.1.3 Session Window Join"></a>2.1.3 Session Window Join</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830174831849.png" alt="image-20210830174831849"></p><pre class=" language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeySelector<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>assigners<span class="token punctuation">.</span>EventTimeSessionWindows<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">;</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> orangeStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> greenStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>orangeStream<span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span>greenStream<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">equalTo</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>EventTimeSessionWindows<span class="token punctuation">.</span><span class="token function">withGap</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">apply</span> <span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">JoinFunction</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> String<span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> String <span class="token function">join</span><span class="token punctuation">(</span>Integer first<span class="token punctuation">,</span> Integer second<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> first <span class="token operator">+</span> <span class="token string">","</span> <span class="token operator">+</span> second<span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h2 id="2-2-Interval-join"><a href="#2-2-Interval-join" class="headerlink" title="2.2 Interval join"></a>2.2 Interval join</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1627222274995-ad3f89bd-cdcd-44a3-a300-2dd5c97334c6.png" alt="img"></p><pre class=" language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeySelector<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>co<span class="token punctuation">.</span>ProcessJoinFunction<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>windowing<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">;</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> orangeStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>DataStream<span class="token operator">&lt;</span>Integer<span class="token operator">></span> greenStream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>orangeStream    <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">intervalJoin</span><span class="token punctuation">(</span>greenStream<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>KeySelector<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">between</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Time<span class="token punctuation">.</span><span class="token function">milliseconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">process</span> <span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProcessJoinFunction</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> <span class="token function">String</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">processElement</span><span class="token punctuation">(</span>Integer left<span class="token punctuation">,</span> Integer right<span class="token punctuation">,</span> Context ctx<span class="token punctuation">,</span> Collector<span class="token operator">&lt;</span>String<span class="token operator">></span> out<span class="token punctuation">)</span> <span class="token punctuation">{</span>            out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>first <span class="token operator">+</span> <span class="token string">","</span> <span class="token operator">+</span> second<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实时数仓架构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时数仓架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-1-当前主流架构</title>
      <link href="/2021/08/30/0001-1-%E5%BD%93%E5%89%8D%E4%B8%BB%E6%B5%81%E6%9E%B6%E6%9E%84/"/>
      <url>/2021/08/30/0001-1-%E5%BD%93%E5%89%8D%E4%B8%BB%E6%B5%81%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-1-当前主流架构"><a href="#0001-1-当前主流架构" class="headerlink" title="0001-1-当前主流架构"></a>0001-1-当前主流架构</h1><h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1627219562253-f6a9fa90-cec9-448f-8396-8b534b9d124d.png" alt="img"></h1><h2 id="1-实时数仓架构特点"><a href="#1-实时数仓架构特点" class="headerlink" title="1.实时数仓架构特点"></a>1.实时数仓架构特点</h2><h3 id="1-1-数仓分层明显少于离线数仓"><a href="#1-1-数仓分层明显少于离线数仓" class="headerlink" title="1.1 数仓分层明显少于离线数仓"></a>1.1 数仓分层明显少于离线数仓</h3><p>一般实时数仓主要是公共层的模型层，缩短数据处理时间，保证数据及时性</p><h3 id="1-2-数据存储的多样化"><a href="#1-2-数据存储的多样化" class="headerlink" title="1.2 数据存储的多样化"></a>1.2 数据存储的多样化</h3><p>离线数仓的数据一般存储于hdfs，但是对于实时数仓的数据，一般使用kafka存储ods贴源层，dwd明细数据，dim维度数据更多的存储在HBase中，也可能存储在redis中，</p><h3 id="1-3-技术难度远高于离线数仓"><a href="#1-3-技术难度远高于离线数仓" class="headerlink" title="1.3 技术难度远高于离线数仓"></a>1.3 技术难度远高于离线数仓</h3><p>目前实时数仓技术栈主要是canal+kafka+flink+hbase+clickhouse，相对于写hivesql来说，难度提升了不少，而且实时数仓对数据的准确性及实时性要求比较高。</p><h2 id="2-实时数仓应用场景"><a href="#2-实时数仓应用场景" class="headerlink" title="2 实时数仓应用场景"></a>2 实时数仓应用场景</h2><p>实时监控，实时推荐</p><h2 id="3-实时数仓架构"><a href="#3-实时数仓架构" class="headerlink" title="3 实时数仓架构"></a>3 实时数仓架构</h2><h3 id="3-1-lamdba架构"><a href="#3-1-lamdba架构" class="headerlink" title="3.1 lamdba架构"></a>3.1 lamdba架构</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1627219562253-f6a9fa90-cec9-448f-8396-8b534b9d124d.png" alt="img"></p><h3 id="3-2-kappa架构"><a href="#3-2-kappa架构" class="headerlink" title="3.2 kappa架构"></a>3.2 kappa架构</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/1627219562253-f6a9fa90-cec9-448f-8396-8b534b9d124d.png" alt="img"></p><p>与 lamdba架构不同点在于，kappa架构全部移除离线部分，用实时生产替代</p><h3 id="3-3-架构对比"><a href="#3-3-架构对比" class="headerlink" title="3.3 架构对比"></a>3.3 架构对比</h3><table><thead><tr><th></th><th>lamdba架构</th><th>kappa架构</th></tr></thead><tbody><tr><td>计算引擎</td><td>批流2套引擎</td><td>流计算引擎</td></tr><tr><td>开发成本</td><td>成本高，维护2套代码</td><td>成本低，维护一套代码</td></tr><tr><td>口径变更</td><td>批处理重新计算</td><td>重新消费kafka数据</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 实时数仓架构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时数仓架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink状态编程-订单超时告警</title>
      <link href="/2021/08/30/Flink%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B-%E8%AE%A2%E5%8D%95%E8%B6%85%E6%97%B6%E5%91%8A%E8%AD%A6/"/>
      <url>/2021/08/30/Flink%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B-%E8%AE%A2%E5%8D%95%E8%B6%85%E6%97%B6%E5%91%8A%E8%AD%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink状态编程-订单超时告警"><a href="#Flink状态编程-订单超时告警" class="headerlink" title="Flink状态编程-订单超时告警"></a>Flink状态编程-订单超时告警</h1><h2 id="一、基础概念"><a href="#一、基础概念" class="headerlink" title="一、基础概念"></a>一、基础概念</h2><p>在Flink架构体系中，有状态计算可以说是Flink非常重要的特性之一。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830150518192.png" alt="Flink优势"></p><p>有状态计算是指:</p><p>在程序计算过程中，在Flink程序内部存储计算产生的中间结果，并提供给后续Function或算子计算结果使用。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830150627260.png" alt="状态计算"></p><p>无状态计算实现的复杂度相对较低，实现起来较容易，但是无法完成提到的比较复杂的业务场景:</p><ul><li>CEP（复杂事件处理）:获取符合某一特定事件规则的事件，状态计算就可以将接入的事件进行存储，然后等待符合规则的事件触发</li><li>最大值、均值等聚合指标（如pv,uv）: 需要利用状态来维护当前计算过程中产生的结果，例如事件的总数、总和以及最大，最小值等</li><li>机器学习场景，维护当前版本模型使用的参数</li><li>其他需要使用历史数据的计算</li></ul><h2 id="二、Flink状态编程"><a href="#二、Flink状态编程" class="headerlink" title="二、Flink状态编程"></a>二、Flink状态编程</h2><h3 id="1、支持的状态类型"><a href="#1、支持的状态类型" class="headerlink" title="1、支持的状态类型"></a>1、支持的状态类型</h3><p>Flink根据数据集是否根据Key进行分区，将状态分为Keyed State和Operator State（Non-keyed State）两种类型。</p><p>其中Keyed State是Operator State的特例，可以通过Key Groups进行管理，主要用于当算子并行度发生变化时，自动重新分布Keyed Sate数据</p><p>同时在Flink中Keyed State和Operator State均具有两种形式:</p><ul><li>一种为托管状态（ManagedState）形式，由Flink Runtime中控制和管理状态数据，并将状态数据转换成为内存Hashtables或RocksDB的对象存储，然后将这些状态数据通过内部的接口持久化到Checkpoints中，任务异常时可以通过这些状态数据恢复任务。</li><li>另外一种是原生状态（Raw State）形式，由算子自己管理数据结构，当触发Checkpoint过程中，Flink并不知道状态数据内部的数据结构，只是将数据转换成bytes数据存储在Checkpoints中，当从Checkpoints恢复任务时，算子自己再反序列化出状态的数据结构。</li></ul><p>在Flink中推荐用户使用Managed State管理状态数据，主要原因是Managed State能够更好地支持状态数据的重平衡以及更加完善的内存管理。</p><h3 id="2、Managed-Keyed-State"><a href="#2、Managed-Keyed-State" class="headerlink" title="2、Managed Keyed State"></a>2、Managed Keyed State</h3><p>六种类型：Managed Keyed State 又分为如下六种类型</p><p>FoldingState已经被标注为deprecated</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830151705846.png" alt="六种状态管理"></p><h3 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h3><p>在Flink中需要通过创建StateDescriptor来获取相应State的操作类。如下方代码，构建一个ValueState</p><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>dataStream<span class="token punctuation">.</span>timer<span class="token punctuation">;</span><span class="token keyword">import</span> lombok<span class="token punctuation">.</span>AllArgsConstructor<span class="token punctuation">;</span><span class="token keyword">import</span> lombok<span class="token punctuation">.</span>Data<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>eventtime<span class="token punctuation">.</span>SerializableTimestampAssigner<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>eventtime<span class="token punctuation">.</span>WatermarkStrategy<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>state<span class="token punctuation">.</span>ValueState<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>state<span class="token punctuation">.</span>ValueStateDescriptor<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>TimeCharacteristic<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>DataStreamSource<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>KeyedStream<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>SingleOutputStreamOperator<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>environment<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeyedProcessFunction<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collector<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>OutputTag<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Duration<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: flinkDemo * @package_name: com.wmy.flink.dataStream.timer * @Author: wmy * @Date: 2021/8/30 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: 订单 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">OrderTimeout</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> OutputTag<span class="token operator">&lt;</span>OrderResult<span class="token operator">></span> orderTimeoutOutputTag <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">OutputTag</span><span class="token operator">&lt;</span>OrderResult<span class="token operator">></span><span class="token punctuation">(</span><span class="token string">"orderTimeout"</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 创建执行环境</span>        StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span>        env<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        KeyedStream<span class="token operator">&lt;</span>OrderEvent<span class="token punctuation">,</span> Long<span class="token operator">></span> orderEventStream <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"flink04"</span><span class="token punctuation">,</span> <span class="token number">9999</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>data <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>                    String<span class="token punctuation">[</span><span class="token punctuation">]</span> fields <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">OrderEvent</span><span class="token punctuation">(</span>Long<span class="token punctuation">.</span><span class="token function">parseLong</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fields<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> fields<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Long<span class="token punctuation">.</span><span class="token function">parseLong</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">assignTimestampsAndWatermarks</span><span class="token punctuation">(</span>                        WatermarkStrategy<span class="token punctuation">.</span>&lt;OrderEvent<span class="token operator">></span><span class="token function">forBoundedOutOfOrderness</span><span class="token punctuation">(</span>Duration<span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                                <span class="token punctuation">.</span><span class="token function">withTimestampAssigner</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">SerializableTimestampAssigner</span><span class="token operator">&lt;</span>OrderEvent<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                                    <span class="token annotation punctuation">@Override</span>                                    <span class="token keyword">public</span> <span class="token keyword">long</span> <span class="token function">extractTimestamp</span><span class="token punctuation">(</span>OrderEvent element<span class="token punctuation">,</span> <span class="token keyword">long</span> recordTimestamp<span class="token punctuation">)</span> <span class="token punctuation">{</span>                                        <span class="token keyword">return</span> element<span class="token punctuation">.</span>eventTime <span class="token operator">*</span> 1000L<span class="token punctuation">;</span>                                    <span class="token punctuation">}</span>                                <span class="token punctuation">}</span><span class="token punctuation">)</span>                <span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>OrderEvent<span class="token operator">:</span><span class="token operator">:</span>getOrderId<span class="token punctuation">)</span><span class="token punctuation">;</span>        SingleOutputStreamOperator<span class="token operator">&lt;</span>OrderResult<span class="token operator">></span> orderResultStream <span class="token operator">=</span> orderEventStream<span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OrderPayMatch</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        orderResultStream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"payed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        orderResultStream<span class="token punctuation">.</span><span class="token function">getSideOutput</span><span class="token punctuation">(</span>orderTimeoutOutputTag<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"timeoutorder"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        env<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"version01"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">OrderPayMatch</span> <span class="token keyword">extends</span> <span class="token class-name">KeyedProcessFunction</span><span class="token operator">&lt;</span>Long<span class="token punctuation">,</span> OrderEvent<span class="token punctuation">,</span> OrderResult<span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token keyword">private</span> ValueState<span class="token operator">&lt;</span>Boolean<span class="token operator">></span> isPayedState<span class="token punctuation">;</span>        <span class="token keyword">private</span> ValueState<span class="token operator">&lt;</span>Long<span class="token operator">></span> timerState<span class="token punctuation">;</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">open</span><span class="token punctuation">(</span>Configuration parameters<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            isPayedState <span class="token operator">=</span> <span class="token function">getIterationRuntimeContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ValueStateDescriptor</span><span class="token operator">&lt;</span>Boolean<span class="token operator">></span><span class="token punctuation">(</span><span class="token string">"state"</span><span class="token punctuation">,</span> Boolean<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            timerState <span class="token operator">=</span> <span class="token function">getIterationRuntimeContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ValueStateDescriptor</span><span class="token operator">&lt;</span>Long<span class="token operator">></span><span class="token punctuation">(</span><span class="token string">"timer"</span><span class="token punctuation">,</span> Long<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">processElement</span><span class="token punctuation">(</span>OrderEvent value<span class="token punctuation">,</span> KeyedProcessFunction<span class="token operator">&lt;</span>Long<span class="token punctuation">,</span> OrderEvent<span class="token punctuation">,</span> OrderResult<span class="token operator">></span><span class="token punctuation">.</span>Context ctx<span class="token punctuation">,</span> Collector<span class="token operator">&lt;</span>OrderResult<span class="token operator">></span> out<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            Boolean isPayed <span class="token operator">=</span> isPayedState<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            Long timerTs <span class="token operator">=</span> timerState<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>value<span class="token punctuation">.</span>eventType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"create"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">// 乱序行为，先得到pay，再得到create</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>isPayed<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OrderResult</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span><span class="token function">getOrderId</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"payed successfully"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">deleteEventTimeTimer</span><span class="token punctuation">(</span>timerTs<span class="token punctuation">)</span><span class="token punctuation">;</span>                    isPayedState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    timerState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                    <span class="token comment" spellcheck="true">// 已经创建订单未支付，设置定时器</span>                    <span class="token keyword">long</span> ts <span class="token operator">=</span> value<span class="token punctuation">.</span>eventTime <span class="token operator">*</span> 1000L <span class="token operator">+</span> <span class="token number">15</span> <span class="token operator">*</span> <span class="token number">60</span> <span class="token operator">*</span> 1000L<span class="token punctuation">;</span>                    ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">registerEventTimeTimer</span><span class="token punctuation">(</span>ts<span class="token punctuation">)</span><span class="token punctuation">;</span>                    timerState<span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span>ts<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>value<span class="token punctuation">.</span>eventType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"pay"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">//假如有定时器，说明create过</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>timerTs <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token comment" spellcheck="true">// timerTs 是 认为超时后的时间戳</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>timerTs <span class="token operator">></span> value<span class="token punctuation">.</span>eventTime <span class="token operator">*</span> 1000L<span class="token punctuation">)</span> <span class="token punctuation">{</span>                        out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OrderResult</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>orderId<span class="token punctuation">,</span> <span class="token string">"payed successfully"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                        ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>orderTimeoutOutputTag<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">OrderResult</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>orderId<span class="token punctuation">,</span> <span class="token string">"this order is timeout"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                    ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">deleteEventTimeTimer</span><span class="token punctuation">(</span>timerTs<span class="token punctuation">)</span><span class="token punctuation">;</span>                    isPayedState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    timerState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                    <span class="token comment" spellcheck="true">// 先来pay</span>                    isPayedState<span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">// 等待watermark时间</span>                    ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">registerEventTimeTimer</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>eventTime <span class="token operator">*</span> 1000L<span class="token punctuation">)</span><span class="token punctuation">;</span>                    timerState<span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>eventTime <span class="token operator">*</span> 1000L<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onTimer</span><span class="token punctuation">(</span><span class="token keyword">long</span> timestamp<span class="token punctuation">,</span> KeyedProcessFunction<span class="token operator">&lt;</span>Long<span class="token punctuation">,</span> OrderEvent<span class="token punctuation">,</span> OrderResult<span class="token operator">></span><span class="token punctuation">.</span>OnTimerContext ctx<span class="token punctuation">,</span> Collector<span class="token operator">&lt;</span>OrderResult<span class="token operator">></span> out<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            Boolean isPayed <span class="token operator">=</span> isPayedState<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>isPayed<span class="token punctuation">)</span> <span class="token punctuation">{</span>                ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>orderTimeoutOutputTag<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">OrderResult</span><span class="token punctuation">(</span>ctx<span class="token punctuation">.</span><span class="token function">getCurrentKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"payed but no create ..."</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">// 典型案例，只有create，没有pay</span>                ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>orderTimeoutOutputTag<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">OrderResult</span><span class="token punctuation">(</span>ctx<span class="token punctuation">.</span><span class="token function">getCurrentKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> <span class="token string">"order timeout ... "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 清楚定时器</span>            isPayedState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            timerState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Data</span>    <span class="token annotation punctuation">@AllArgsConstructor</span>    <span class="token comment" spellcheck="true">// 定义输入订单事件的样例类</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">OrderEvent</span> <span class="token punctuation">{</span>        <span class="token keyword">private</span> Long orderId<span class="token punctuation">;</span>        <span class="token keyword">private</span> String eventType<span class="token punctuation">;</span>        <span class="token keyword">private</span> String txId<span class="token punctuation">;</span>        <span class="token keyword">private</span> Long eventTime<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 定义输出结果样例类</span>    <span class="token annotation punctuation">@Data</span>    <span class="token annotation punctuation">@AllArgsConstructor</span>    <span class="token keyword">public</span> <span class="token keyword">static</span>  <span class="token keyword">class</span> <span class="token class-name">OrderResult</span> <span class="token punctuation">{</span>        <span class="token keyword">private</span> Long orderId<span class="token punctuation">;</span>        <span class="token keyword">private</span> String resultMsg<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这个是基本得超时任务</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Phoenix二级索引-讲原理</title>
      <link href="/2021/08/30/Phoenix%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95-%E8%AE%B2%E5%8E%9F%E7%90%86/"/>
      <url>/2021/08/30/Phoenix%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95-%E8%AE%B2%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Rowkey如何设计</title>
      <link href="/2021/08/30/Rowkey%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1/"/>
      <url>/2021/08/30/Rowkey%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RowKey设计原则</title>
      <link href="/2021/08/30/RowKey%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"/>
      <url>/2021/08/30/RowKey%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HBase存储结构</title>
      <link href="/2021/08/30/HBase%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/"/>
      <url>/2021/08/30/HBase%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="HBASE存储结构"><a href="#HBASE存储结构" class="headerlink" title="HBASE存储结构"></a>HBASE存储结构</h1><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830111340876.png" alt="hbase存储结构"></p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase架构</title>
      <link href="/2021/08/30/hbase%E6%9E%B6%E6%9E%84/"/>
      <url>/2021/08/30/hbase%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase合并-切分-删除标记</title>
      <link href="/2021/08/30/hbase%E5%90%88%E5%B9%B6-%E5%88%87%E5%88%86-%E5%88%A0%E9%99%A4%E6%A0%87%E8%AE%B0/"/>
      <url>/2021/08/30/hbase%E5%90%88%E5%B9%B6-%E5%88%87%E5%88%86-%E5%88%A0%E9%99%A4%E6%A0%87%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase刷写</title>
      <link href="/2021/08/30/hbase%E5%88%B7%E5%86%99/"/>
      <url>/2021/08/30/hbase%E5%88%B7%E5%86%99/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase读写测试</title>
      <link href="/2021/08/30/hbase%E8%AF%BB%E5%86%99%E6%B5%8B%E8%AF%95/"/>
      <url>/2021/08/30/hbase%E8%AF%BB%E5%86%99%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase数据读流程</title>
      <link href="/2021/08/30/hbase%E6%95%B0%E6%8D%AE%E8%AF%BB%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/08/30/hbase%E6%95%B0%E6%8D%AE%E8%AF%BB%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase数据写流程</title>
      <link href="/2021/08/30/hbase%E6%95%B0%E6%8D%AE%E5%86%99%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/08/30/hbase%E6%95%B0%E6%8D%AE%E5%86%99%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="hbase数据写流程"><a href="#hbase数据写流程" class="headerlink" title="hbase数据写流程"></a>hbase数据写流程</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">众所周知，HBase默认适用于写多读少的应用，正是依赖于它相当出色的写入性能：一个100台RS的集群可以轻松地支撑每天10T</span> <span class="token attr-value">的写入量。</span>当然，为了支持更高吞吐量的写入，HBase还在不断地进行优化和修正，这篇文章结合0.98版本的源码全面地分析HBase的写入流程，<span class="token attr-name">全文分为三个部分</span><span class="token punctuation">:</span>    第一部分介绍客户端的写入流程    第二部分介绍服务器端的写入流程，    最后再重点分析WAL的工作原理（注：从服务器端的角度理解，HBase写入分为两个阶段，    第一阶段数据会被写入memstore，并且会执行WAL的写入；    第二阶段会将memstore的中的数据集中flush到磁盘，本文主要集中分析第一阶段的相关细节）。</code></pre><h2 id="客户端流程解析"><a href="#客户端流程解析" class="headerlink" title="客户端流程解析"></a>客户端流程解析</h2><p>（1）用户提交put请求后，HBase客户端会将put请求添加到本地buffer中，符合一定条件就会通过AsyncProcess异步批量提交。HBase默认设置autoflush=true，表示put请求直接会提交给服务器进行处理；用户可以设置autoflush=false，这样的话put请求会首先放到本地buffer，等到本地buffer大小超过一定阈值（默认为2M，可以通过配置文件配置）之后才会提交。很显然，后者采用group commit机制提交请求，可以极大地提升写入性能，但是因为没有保护机制，如果客户端崩溃的话会导致提交的请求丢失。</p><p>（2）在提交之前，HBase会在元数据表.meta.中根据rowkey找到它们归属的region server，这个定位的过程是通过HConnection 的locateRegion方法获得的。如果是批量请求的话还会把这些rowkey按照HRegionLocation分组，每个分组可以对应一次RPC请求。</p><p> 3 ）HBase 会为每个HRegionLocation 构造一个远程RPC 请求MultiServerCallable<row> ， 然后通过rpcCallerFactory.<multiresponse> newCaller()执行调用，忽略掉失败重新提交和错误处理，客户端的提交操作到此结束。****服务器端流程解析****服务器端RegionServer接收到客户端的写入请求后，首先会反序列化为Put对象，然后执行各种检查操作，比如检查region是否是只读、memstore大小是否超过blockingMemstoreSize等。检查完成之后，就会执行如下核心操作：</multiresponse></row></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830112300412.png" alt="客户端解析流程"></p><p>（1） 获取行锁、Region更新共享锁： HBase中使用行锁保证对同一行数据的更新都是互斥操作，用以保证更新的原子性，要么更新成功，要么失败。</p><p>（2） 开始写事务：获取write number，用于实现MVCC，实现数据的非锁定读，在保证读写一致性的前提下提高读取性能。</p><p>（3） 写缓存memstore：HBase中每个列族都会对应一个store，用来存储该列族数据。每个store都会有个写缓存memstore，用于缓存写入数据。HBase并不会直接将数据落盘，而是先写入缓存，等缓存满足一定大小之后再一起落盘。</p><p>（4） Append HLog：HBase使用WAL机制保证数据可靠性，即首先写日志再写缓存，即使发生宕机，也可以通过恢复HLog还原出原始数据。该步骤就是将数据构造为WALEdit对象，然后顺序写入HLog中，此时不需要执行sync操作。0.98版本采用了新的写线程模式实现HLog日志的写入，可以使得整个数据更新性能得到极大提升，具体原理见下一个章节。</p><p>（5）释放行锁以及共享锁</p><p>（6）Sync HLog真正sync到HDFS，在释放行锁之后执行sync操作是为了尽量减少持锁时间，提升写性能。如果Sync失败，执行回滚操作将memstore中已经写入的数据移除。</p><p> （7） 结束写事务：此时该线程的更新操作才会对其他读请求可见，更新才实际生效。具体分析见上一篇文章《HBase - 并发控制深度解析》</p><p>（8） flush memstore：当写缓存满64M之后，会启动flush线程将数据刷新到硬盘。刷新操作涉及到HFile相关结构，后面会详细对此进行介绍。</p><h2 id="WAL机制解析"><a href="#WAL机制解析" class="headerlink" title="WAL机制解析"></a>WAL机制解析</h2><p> WAL(Write-Ahead Logging)是一种高效的日志算法，几乎是所有非内存数据库提升写性能的不二法门，基本原理是在数据写入之前首先顺序写入日志，然后再写入缓存，等到缓存写满之后统一落盘。之所以能够提升写性能，是因为WAL将一次随机写转化为了一次顺序写加一次内存写。提升写性能的同时，WAL可以保证数据的可靠性，即在任何情况下数据不丢失。假如一次写入完成之后发生了宕机，即使所有缓存中的数据丢失，也可以通过恢复日志还原出丢失的数据。</p><h3 id="WAL-持-久-化-等-级"><a href="#WAL-持-久-化-等-级" class="headerlink" title="WAL 持 久 化 等 级"></a>WAL 持 久 化 等 级</h3><p>HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。WAL的持久化等级分为如下四个等级：</p><ol><li><p>SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。</p></li><li><p>ASYNC_WAL：异步将数据写入HLog日志中。</p></li><li><p>SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘。</p></li><li><p>FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。</p></li><li><p>USER_DEFAULT：默认如果用户没有指定持久化等级，HBase使用SYNC_WAL等级持久化数据。</p></li></ol><p>用户可以通过客户端设置WAL持久化等级，代码：put.setDurability(Durability. SYNC_WAL ); </p><h3 id="HLog数据结构"><a href="#HLog数据结构" class="headerlink" title="HLog数据结构"></a>HLog数据结构</h3><p>HBase中，WAL的实现类为HLog，每个Region Server拥有一个HLog日志，所有region的写入都是写到同一个HLog。下图表示同一个Region Server中的3个 region 共享一个HLog。当数据写入时，是将数据对&lt;HLogKey,WALEdit&gt;按照顺序追加到HLog 中，以获取最好的写入性能。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830113150156.png" alt="HLog数据结构"></p><p>上图中HLogKey主要存储了log sequence number，更新时间 write time，region name，表名table name以及cluster ids。其中log sequncece number作为HFile中一个重要的元数据，和HLog的生命周期息息相关，后续章节会详细介绍；region name和table name分别表征该段日志属于哪个region以及哪张表；cluster ids用于将日志复制到集群中其他机器上。</p><p>WALEdit用来表示一个事务中的更新集合，在之前的版本，如果一个事务中对一行row R中三列c1，c2，c3分别做了修改，那么hlog中会有3个对应的日志片段如下所示：</p><p><logseq3-for-edit3>:<keyvalue-for-edit-c3></keyvalue-for-edit-c3></logseq3-for-edit3></p><p> 然而，这种日志结构无法保证行级事务的原子性，假如刚好更新到c2之后发生宕机，那么就会产生只有部分日志写入成功的现象。为此，hbase将所有对同一行的更新操作都表示为一个记录，如下：</p><p> &lt;logseq#-for-entire-txn&gt;:<waledit-for-entire-txn></waledit-for-entire-txn></p><p>其中WALEdit会被序列化为格式&lt;-1, # of edits, <keyvalue>, <keyvalue>, <keyvalue>&gt;，比如&lt;-1, 3, <keyvalue-for-edit- c1="">, <keyvalue-for-edit-c2>, <keyvalue-for-edit-c3>&gt;，其中-1作为标示符表征这种新的日志结构。</keyvalue-for-edit-c3></keyvalue-for-edit-c2></keyvalue-for-edit-></keyvalue></keyvalue></keyvalue></p><h3 id="WAL写入模型"><a href="#WAL写入模型" class="headerlink" title="WAL写入模型"></a>WAL写入模型</h3><p>了解了HLog 的结构之后， 我们就开始研究HLog 的写入模型。 HLog 的写入可以分为三个阶段， 首先将数据对&lt;HLogKey,WALEdit&gt;写入本地缓存，然后再将本地缓存写入文件系统，最后执行sync操作同步到磁盘。在以前老的写入模型中， 上述三步都由工作线程独自完成，如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830113927667.png" alt="WAL写入模型"></p><p>上图中，本地缓存写入文件系统那个步骤工作线程需要持有updateLock执行，不同工作线程之间必然会恶性竞争；不仅如此，在Sync HDFS这步中，工作线程之间需要抢占flushLock，因为Sync操作是一个耗时操作，抢占这个锁会导致写入性能大幅降低。</p><p>所幸的是，来自中国（准确的来说，是来自小米，鼓掌）的3位工程师意识到了这个问题，进而提出了一种新的写入模型并被官方 采纳。根据官方测试，新写入模型的吞吐量比之前提升3倍多，单台RS写入吞吐量介于12150～31520，5台RS组成的集群写入吞吐量介于22000～70000（见HBASE-8755）。下图是小米官方给出来的对比测试结果：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830114032215.png" alt="测试结果"></p><p>在新写入模型中，本地缓存写入文件系统以及Sync HDFS都交给了新的独立线程完成，并引入一个Notify线程通知工作线程是否已经Sync成功，采用这种机制消除上述锁竞争，具体如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210830114052041.png" alt="消除竞争锁"></p><ol><li><p>上文中提到工作线程在写入WALEdit 之后并没有进行Sync ， 而是等到释放行锁阻塞在syncedTillHere 变量上， 等待AsyncNotifier线程唤醒。</p></li><li><p>工作线程将WALEdit写入本地Buffer之后，会生成一个自增变量txid，携带此txid唤醒AsyncWriter线程</p></li><li><p>AsyncWriter 线程会取出本地Buffer 中的所有WALEdit ， 写入HDFS 。注意该线程会比较传入的txid 和已经写入的最大txid（writtenTxid），如果传入的txid小于writteTxid，表示该txid对应的WALEdit已经写入，直接跳过</p></li><li><p>AsyncWriter线程将所有WALEdit写入HDFS之后携带maxTxid唤醒AsyncFlusher线程 </p></li><li><p>AsyncFlusher线程将所有写入文件系统的WALEdit统一Sync刷新到磁盘</p></li><li><p>数据全部落盘之后调用setFlushedTxid方法唤醒AyncNotifier线程</p></li><li><p>AyncNotifier线程会唤醒所有阻塞在变量syncedTillHere的工作线程，工作线程被唤醒之后表示WAL写入完成，后面再执行MVCC结束写事务，推进全局读取点，本次更新才会对用户可见</p></li></ol><p>通过上述过程的梳理可以知道，新写入模型采取了多线程模式独立完成写文件系统、sync磁盘操作，避免了之前多工作线程恶性抢占锁的问题。同时，工作线程在将WALEdit写入本地Buffer之后并没有马上阻塞，而是释放行锁之后阻塞等待WALEdit落盘，这样可以尽可能地避免行锁竞争，提高写入性能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>文章刚开始就提到HBase写入分为两个阶段，本文主要集中分析第一阶段的相关细节，首先介绍了HBase的写入memstore的流程，之后重点分析了WAL的写入模型以及相关优化。</p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStream的双流join</title>
      <link href="/2021/08/30/SparkStream%E7%9A%84%E5%8F%8C%E6%B5%81join/"/>
      <url>/2021/08/30/SparkStream%E7%9A%84%E5%8F%8C%E6%B5%81join/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink实时数仓ODS-DWD-DWM-DWS项目总结</title>
      <link href="/2021/08/30/Flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93ODS-DWD-DWM-DWS%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"/>
      <url>/2021/08/30/Flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93ODS-DWD-DWM-DWS%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink实时数仓ODS-DWD-DWM-DWS项目总结"><a href="#Flink实时数仓ODS-DWD-DWM-DWS项目总结" class="headerlink" title="Flink实时数仓ODS-DWD-DWM-DWS项目总结"></a>Flink实时数仓ODS-DWD-DWM-DWS项目总结</h1><h2 id="项目亮点"><a href="#项目亮点" class="headerlink" title="项目亮点"></a>项目亮点</h2><pre class=" language-properties"><code class="language-properties"><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> DWD 层<span class="token attr-name">1、Flink</span> <span class="token attr-value">CDC ，flink1.11之后 ---> ODS</span>2、行为数据，解耦和时效性,springboot到kafka,flume到kafka3、dwd行为数据，使用侧输出流，新老用户校验<span class="token attr-name">4、dwd业务数据：广播流进行分流(hbase,kafka)，flink</span> <span class="token attr-value">cdc读取配置信息</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> DWM 层1、四张表：UV(状态编程),Uj(CEP),OderWide,PaymentWide（双流join）DWS不同主题会用到不同的数据，不是直接来自于DWD，提取出来，减少程序和数据的复用率，建模的思想，根本的思想得说明白订单和订单明细join，六张维度，地区，用户，SPU，SKU，品牌，品类（3个分类）离线数仓中做降维和维度退化，这个得话我们也可以这样来进行做维护sku表，实时数仓当中，在来一个sku表进行更新操作就好了，对于商品表来说，这个是缓慢渐变维，做维度退化，之间得点那种效果会更好一点勒，类似于3范式和维度建模不一致，我们追求效率，hbase中有维度表，数据量不是很大<span class="token attr-name">双流join</span> <span class="token attr-value">intervalJoin就可以了，考虑到延迟得话就行</span>SparkStreaming中得，因为可能公司Spark和Flink都得进行使用，FlinK只是作为要给人才来进行储备的，可能干的就是Spark反射和命名规范Redis来进行缓存，第三方进行交互，性能瓶颈，IO开销，官网当中提出了使用异步IO来提高它的效率10个线程，核心数够的话，可以进行同时的查询，压缩，串行，读过的数据，9，2，每条数据，中间的延迟很高，异步IO，有重叠时间，数据量很大做到了一个极大的优化，但是生产过程当中的数据量是非常的大背压，导致消费者不接受数据，kafka积压的过程，kafka不消费的话，数据不会丢掉，挤压带来的直接的影响就是延迟性会越来越高，异步IO的效率很高一定要分享出来，这个是面试过程当中非常重要的，实时数仓和离线数仓都是重点，把离线拿出来好好的进行复习</code></pre><pre class=" language-properties"><code class="language-properties"><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> DWS 层4张clickhouse表访客商品地区关键字用到的东西都是零零散散来进行拆开讲的，所以这个也是非常重要的一点难得东西进行拆开就显得稍微简单点亮点：访客主题：union，五个指标，pv,uv,uj,sv,duration_time三个流，page_log（uv,uj,sv），使用fulljoin，使用connect关联来进行做商品有7个流，connect只能使用两个流来进行做，使用了union(DataStreamAPI)<span class="token attr-name">Flink</span> <span class="token attr-value">SQL使用fullJoin来进行改写这个需求</span><span class="token attr-name">地区表和关键词都是使用Flink</span> <span class="token attr-value">SQL来进行做得，不过这些都是使用得是单表来进行做得，这个是很简单得</span>如果是事件事件，使用timestamp(3)来进行做得</code></pre>]]></content>
      
      
      <categories>
          
          <category> 企业级数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 企业级数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink-IKSegmenter-学习使用</title>
      <link href="/2021/08/30/Flink-IKSegmenter-%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/08/30/Flink-IKSegmenter-%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive统计硅谷影音视频网站</title>
      <link href="/2021/08/29/hive%E7%BB%9F%E8%AE%A1%E7%A1%85%E8%B0%B7%E5%BD%B1%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BD%91%E7%AB%99/"/>
      <url>/2021/08/29/hive%E7%BB%9F%E8%AE%A1%E7%A1%85%E8%B0%B7%E5%BD%B1%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BD%91%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h1 id="hive统计硅谷影音视频网站"><a href="#hive统计硅谷影音视频网站" class="headerlink" title="hive统计硅谷影音视频网站"></a>hive统计硅谷影音视频网站</h1><p>[TOC]</p><h2 id="1、需求描述"><a href="#1、需求描述" class="headerlink" title="1、需求描述"></a>1、需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种 TopN 指标： </p><ul><li><input disabled="" type="checkbox"> 1、统计视频观看数 Top10 </li><li><input disabled="" type="checkbox"> 2、统计视频类别热度 Top10 </li><li><input disabled="" type="checkbox"> 3、统计出视频观看数最高的 20 个视频的所属类别以及类别包含 Top20 视频的个数 </li><li><input disabled="" type="checkbox"> 4、统计视频观看数 Top50 所关联视频的所属类别排序 </li><li><input disabled="" type="checkbox"> 5、统计每个类别中的视频热度 Top10,以 Music 为例 </li><li><input disabled="" type="checkbox"> 6、统计每个类别视频观看数 Top10 </li><li><input disabled="" type="checkbox"> 7、 统计上传视频最多的用户 Top10 以及他们上传的视频观看次数在前 20 的视频</li></ul><h2 id="2、数据结构"><a href="#2、数据结构" class="headerlink" title="2、数据结构"></a>2、数据结构</h2><h3 id="1）视频表"><a href="#1）视频表" class="headerlink" title="1）视频表"></a>1）视频表</h3><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>videoId</td><td>视频唯一id（String）</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者（String）</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄（int）</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别（Array<string>）</string></td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度（Int）</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数（Int）</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分（Double）</td><td>满分5分</td></tr><tr><td>Ratings</td><td>流量（Int）</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数（Int）</td><td>一个视频的整数评论数</td></tr><tr><td>relatedId</td><td>相关视频id（Array<string>）</string></td><td>相关视频的id，最多20个</td></tr></tbody></table><h3 id="2）用户表"><a href="#2）用户表" class="headerlink" title="2）用户表"></a>2）用户表</h3><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table><h2 id="3、准备工作"><a href="#3、准备工作" class="headerlink" title="3、准备工作"></a>3、准备工作</h2><h3 id="3-1-准备表"><a href="#3-1-准备表" class="headerlink" title="3.1 准备表"></a>3.1 准备表</h3><h4 id="1）需要准备的表"><a href="#1）需要准备的表" class="headerlink" title="1）需要准备的表"></a>1）需要准备的表</h4><p>创建原始数据表：gulivideo_ori，gulivideo_user_ori， 创建最终表：gulivideo_orc，gulivideo_user_orc</p><h4 id="2）创建原始数据表"><a href="#2）创建原始数据表" class="headerlink" title="2）创建原始数据表"></a>2）创建原始数据表</h4><h5 id="（1）gulivideo-ori"><a href="#（1）gulivideo-ori" class="headerlink" title="（1）gulivideo_ori"></a>（1）gulivideo_ori</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> gulivideo_ori<span class="token punctuation">(</span> videoId string<span class="token punctuation">,</span> uploader string<span class="token punctuation">,</span> age <span class="token keyword">int</span><span class="token punctuation">,</span> category array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">,</span> length <span class="token keyword">int</span><span class="token punctuation">,</span> views <span class="token keyword">int</span><span class="token punctuation">,</span> rate <span class="token keyword">float</span><span class="token punctuation">,</span> ratings <span class="token keyword">int</span><span class="token punctuation">,</span> comments <span class="token keyword">int</span><span class="token punctuation">,</span> relatedId array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">"\t"</span>collection items <span class="token keyword">terminated by</span> <span class="token string">"&amp;"</span>stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span></code></pre><h5 id="（2）创建原始数据表-gulivideo-user-ori"><a href="#（2）创建原始数据表-gulivideo-user-ori" class="headerlink" title="（2）创建原始数据表: gulivideo_user_ori"></a>（2）创建原始数据表: gulivideo_user_ori</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> gulivideo_user_ori<span class="token punctuation">(</span> uploader string<span class="token punctuation">,</span> videos <span class="token keyword">int</span><span class="token punctuation">,</span> friends <span class="token keyword">int</span><span class="token punctuation">)</span><span class="token keyword">row</span> format delimited<span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">"\t"</span>stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span></code></pre><h4 id="3）创建-orc-存储格式带-snappy-压缩的表"><a href="#3）创建-orc-存储格式带-snappy-压缩的表" class="headerlink" title="3）创建 orc 存储格式带 snappy 压缩的表"></a>3）创建 orc 存储格式带 snappy 压缩的表</h4><h5 id="（1）gulivideo-orc"><a href="#（1）gulivideo-orc" class="headerlink" title="（1）gulivideo_orc"></a>（1）gulivideo_orc</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> gulivideo_orc<span class="token punctuation">(</span> videoId string<span class="token punctuation">,</span> uploader string<span class="token punctuation">,</span> age <span class="token keyword">int</span><span class="token punctuation">,</span> category array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">,</span> length <span class="token keyword">int</span><span class="token punctuation">,</span> views <span class="token keyword">int</span><span class="token punctuation">,</span> rate <span class="token keyword">float</span><span class="token punctuation">,</span> ratings <span class="token keyword">int</span><span class="token punctuation">,</span> comments <span class="token keyword">int</span><span class="token punctuation">,</span> relatedId array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">)</span>stored <span class="token keyword">as</span> orctblproperties<span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"SNAPPY"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h5 id="（2）gulivideo-user-orc"><a href="#（2）gulivideo-user-orc" class="headerlink" title="（2）gulivideo_user_orc"></a>（2）gulivideo_user_orc</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> gulivideo_user_orc<span class="token punctuation">(</span> uploader string<span class="token punctuation">,</span> videos <span class="token keyword">int</span><span class="token punctuation">,</span> friends <span class="token keyword">int</span><span class="token punctuation">)</span><span class="token keyword">row</span> format delimited<span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">"\t"</span>stored <span class="token keyword">as</span> orctblproperties<span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"SNAPPY"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h5 id="（3）向-ori-表插入数据"><a href="#（3）向-ori-表插入数据" class="headerlink" title="（3）向 ori 表插入数据"></a>（3）向 ori 表插入数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">"/opt/flink/warehouse/gulivideo/video"</span> <span class="token keyword">into</span> <span class="token keyword">table</span> gulivideo_ori<span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">"/opt/flink/warehouse/gulivideo/user"</span> <span class="token keyword">into</span> <span class="token keyword">table</span> gulivideo_user_ori<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829133218817.png" alt="数据来源"></p><p><em><strong>视频数据</strong></em></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829132804111.png" alt="gulivideo_user_ori"></p><p><em><strong>用户数据</strong></em></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829132906400.png" alt="gulivideo_ori"></p><h5 id="（4）向-orc-表插入数据"><a href="#（4）向-orc-表插入数据" class="headerlink" title="（4）向 orc 表插入数据"></a>（4）向 orc 表插入数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> gulivideo_orc <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> gulivideo_ori<span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> gulivideo_user_orc <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> gulivideo_user_ori<span class="token punctuation">;</span></code></pre><h2 id="4、业务分析"><a href="#4、业务分析" class="headerlink" title="4、业务分析"></a>4、业务分析</h2><h3 id="4-1-统计视频观看数-Top10"><a href="#4-1-统计视频观看数-Top10" class="headerlink" title="4.1 统计视频观看数 Top10"></a>4.1 统计视频观看数 Top10</h3><p>思路：使用 order by 按照 views 字段做一个全局排序即可，同时我们设置只显示前 10 条。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> views<span class="token keyword">FROM</span> gulivideo_orc<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> views <span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829133543921.png" alt="案例结果"></p><h3 id="4-2-统计视频类别热度-Top10"><a href="#4-2-统计视频类别热度-Top10" class="headerlink" title="4.2 统计视频类别热度 Top10"></a>4.2 统计视频类别热度 Top10</h3><p>思路： </p><p>（1）即统计每个类别有多少个视频，显示出包含视频最多的前 10 个类别。 </p><p>（2）我们需要按照类别 group by 聚合，然后 count 组内的 videoId 个数即可。 </p><p>（3）因为当前表结构为：一个视频对应一个或多个类别。所以如果要 group by 类别， 需要先将类别进行列转行(展开)，然后再进行 count 即可。 </p><p>（4）最后按照热度排序，显示前 10 条。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> t1<span class="token punctuation">.</span>category_name <span class="token punctuation">,</span> <span class="token function">COUNT</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>videoId<span class="token punctuation">)</span> hot<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> category_name<span class="token keyword">FROM</span> gulivideo_orclateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp <span class="token keyword">AS</span> category_name<span class="token punctuation">)</span> t1<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> t1<span class="token punctuation">.</span>category_name<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> hot<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134211821.png" alt="结果展示"></p><h3 id="4-3-统计出视频观看数最高的-20-个视频的所属类别以及类别包含-Top20-视频的个数"><a href="#4-3-统计出视频观看数最高的-20-个视频的所属类别以及类别包含-Top20-视频的个数" class="headerlink" title="4.3 统计出视频观看数最高的 20 个视频的所属类别以及类别包含 Top20 视频的个数"></a>4.3 统计出视频观看数最高的 20 个视频的所属类别以及类别包含 Top20 视频的个数</h3><p>思路： </p><p>（1）先找到观看数最高的 20 个视频所属条目的所有信息，降序排列 </p><p>（2）把这 20 条信息中的 category 分裂出来(列转行) </p><p>（3）最后查询视频分类名称和该分类下有多少个 Top20 的视频</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span>  t2<span class="token punctuation">.</span>category_name<span class="token punctuation">,</span> <span class="token function">COUNT</span><span class="token punctuation">(</span>t2<span class="token punctuation">.</span>videoId<span class="token punctuation">)</span> video_sum<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> t1<span class="token punctuation">.</span>videoId<span class="token punctuation">,</span> category_name<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> views <span class="token punctuation">,</span> category<span class="token keyword">FROM</span> gulivideo_orc<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> views<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">20</span><span class="token punctuation">)</span> t1lateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>t1<span class="token punctuation">.</span>category<span class="token punctuation">)</span> t1_tmp <span class="token keyword">AS</span> category_name<span class="token punctuation">)</span> t2<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> t2<span class="token punctuation">.</span>category_name<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134303809.png" alt="结果展示"></p><h3 id="4-4-统计视频观看数-Top50-所关联视频的所属类别排序"><a href="#4-4-统计视频观看数-Top50-所关联视频的所属类别排序" class="headerlink" title="4.4 统计视频观看数 Top50 所关联视频的所属类别排序"></a>4.4 统计视频观看数 Top50 所关联视频的所属类别排序</h3><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> t6<span class="token punctuation">.</span>category_name<span class="token punctuation">,</span> t6<span class="token punctuation">.</span>video_sum<span class="token punctuation">,</span> rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">over</span><span class="token punctuation">(</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> t6<span class="token punctuation">.</span>video_sum <span class="token keyword">DESC</span> <span class="token punctuation">)</span> rk<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> t5<span class="token punctuation">.</span>category_name<span class="token punctuation">,</span> <span class="token function">COUNT</span><span class="token punctuation">(</span>t5<span class="token punctuation">.</span>relatedid_id<span class="token punctuation">)</span> video_sum<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> t4<span class="token punctuation">.</span>relatedid_id<span class="token punctuation">,</span> category_name<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> t2<span class="token punctuation">.</span>relatedid_id <span class="token punctuation">,</span> t3<span class="token punctuation">.</span>category<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> relatedid_id<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> views<span class="token punctuation">,</span> relatedid<span class="token keyword">FROM</span> gulivideo_orc<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> views<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">50</span><span class="token punctuation">)</span>t1lateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>t1<span class="token punctuation">.</span>relatedid<span class="token punctuation">)</span> t1_tmp <span class="token keyword">AS</span> relatedid_id<span class="token punctuation">)</span>t2<span class="token keyword">JOIN</span> gulivideo_orc t3<span class="token keyword">ON</span>t2<span class="token punctuation">.</span>relatedid_id <span class="token operator">=</span> t3<span class="token punctuation">.</span>videoId<span class="token punctuation">)</span> t4lateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>t4<span class="token punctuation">.</span>category<span class="token punctuation">)</span> t4_tmp <span class="token keyword">AS</span> category_name<span class="token punctuation">)</span> t5<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> t5<span class="token punctuation">.</span>category_name<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> video_sum<span class="token keyword">DESC</span><span class="token punctuation">)</span> t6<span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134352131.png" alt="结果展示"></p><h3 id="4-5-统计每个类别中的视频热度-Top10，以-Music-为例"><a href="#4-5-统计每个类别中的视频热度-Top10，以-Music-为例" class="headerlink" title="4.5 统计每个类别中的视频热度 Top10，以 Music 为例"></a>4.5 统计每个类别中的视频热度 Top10，以 Music 为例</h3><p>思路： </p><p>（1）要想统计 Music 类别中的视频热度 Top10，需要先找到 Music 类别，那么就需要将 category 展开，所以可以创建一张表用于存放 categoryId 展开的数据。 </p><p>（2）向 category 展开的表中插入数据。 </p><p>（3）统计对应类别（Music）中的视频热度。 </p><p>统计 Music 类别的 Top10（也可以统计其他）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> t1<span class="token punctuation">.</span>videoId<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>views<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>category_name<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> views<span class="token punctuation">,</span> category_name<span class="token keyword">FROM</span> gulivideo_orclateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp <span class="token keyword">AS</span> category_name<span class="token punctuation">)</span>t1<span class="token keyword">WHERE</span> t1<span class="token punctuation">.</span>category_name <span class="token operator">=</span> <span class="token string">"Music"</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> t1<span class="token punctuation">.</span>views<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134507742.png" alt="结果展示"></p><h3 id="4-6-统计每个类别视频观看数-Top10"><a href="#4-6-统计每个类别视频观看数-Top10" class="headerlink" title="4.6 统计每个类别视频观看数 Top10"></a>4.6 统计每个类别视频观看数 Top10</h3><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> t2<span class="token punctuation">.</span>videoId<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>views<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>category_name<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>rk<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> t1<span class="token punctuation">.</span>videoId<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>views<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>category_name<span class="token punctuation">,</span> rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">over</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> t1<span class="token punctuation">.</span>category_name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> t1<span class="token punctuation">.</span>views <span class="token keyword">DESC</span> <span class="token punctuation">)</span> rk<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> videoId<span class="token punctuation">,</span> views<span class="token punctuation">,</span> category_name<span class="token keyword">FROM</span> gulivideo_orclateral <span class="token keyword">VIEW</span> explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp <span class="token keyword">AS</span> category_name<span class="token punctuation">)</span>t1<span class="token punctuation">)</span>t2<span class="token keyword">WHERE</span> t2<span class="token punctuation">.</span>rk <span class="token operator">&lt;=</span><span class="token number">10</span> <span class="token keyword">limit</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134601089.png" alt="结果展示"></p><h3 id="4-7-统计上传视频最多的用户-Top10以及他们上传的视频-观看次数在前-20-的视频"><a href="#4-7-统计上传视频最多的用户-Top10以及他们上传的视频-观看次数在前-20-的视频" class="headerlink" title="4.7 统计上传视频最多的用户 Top10以及他们上传的视频 观看次数在前 20 的视频"></a>4.7 统计上传视频最多的用户 Top10以及他们上传的视频 观看次数在前 20 的视频</h3><p>思路：</p><p> （1）求出上传视频最多的 10 个用户 </p><p>（2）关联 gulivideo_orc 表，求出这 10 个用户上传的所有的视频，按照观看数取前 20</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> t2<span class="token punctuation">.</span>videoId<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>views<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>uploader<span class="token keyword">FROM</span><span class="token punctuation">(</span><span class="token keyword">SELECT</span> uploader<span class="token punctuation">,</span> videos<span class="token keyword">FROM</span> gulivideo_user_orc<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> videos<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">10</span><span class="token punctuation">)</span> t1<span class="token keyword">JOIN</span> gulivideo_orc t2<span class="token keyword">ON</span> t1<span class="token punctuation">.</span>uploader <span class="token operator">=</span> t2<span class="token punctuation">.</span>uploader<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> t2<span class="token punctuation">.</span>views<span class="token keyword">DESC</span><span class="token keyword">LIMIT</span> <span class="token number">20</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210829134650961.png" alt="结果展示"></p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java中annotation注解</title>
      <link href="/2021/08/29/java%E4%B8%ADannotation%E6%B3%A8%E8%A7%A3/"/>
      <url>/2021/08/29/java%E4%B8%ADannotation%E6%B3%A8%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="java中annotation注解"><a href="#java中annotation注解" class="headerlink" title="java中annotation注解"></a>java中annotation注解</h1>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lombok注解</title>
      <link href="/2021/08/29/lombok%E6%B3%A8%E8%A7%A3/"/>
      <url>/2021/08/29/lombok%E6%B3%A8%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="lombok注解"><a href="#lombok注解" class="headerlink" title="lombok注解"></a>lombok注解</h1>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java注解的使用</title>
      <link href="/2021/08/29/java%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/08/29/java%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="java注解的使用"><a href="#java注解的使用" class="headerlink" title="java注解的使用"></a>java注解的使用</h1>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis经典案例场景</title>
      <link href="/2021/08/28/Redis%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%E5%9C%BA%E6%99%AF/"/>
      <url>/2021/08/28/Redis%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%E5%9C%BA%E6%99%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis经典案例场景"><a href="#Redis经典案例场景" class="headerlink" title="Redis经典案例场景"></a>Redis经典案例场景</h1><p><a href="https://zhuanlan.zhihu.com/p/146082021" target="_blank" rel="noopener">参考链接</a></p>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Geographic经纬度需求</title>
      <link href="/2021/08/28/Geographic%E7%BB%8F%E7%BA%AC%E5%BA%A6%E9%9C%80%E6%B1%82/"/>
      <url>/2021/08/28/Geographic%E7%BB%8F%E7%BA%AC%E5%BA%A6%E9%9C%80%E6%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="Geographic经纬度需求"><a href="#Geographic经纬度需求" class="headerlink" title="Geographic经纬度需求"></a>Geographic经纬度需求</h1><h2 id="知识点描述"><a href="#知识点描述" class="headerlink" title="知识点描述"></a>知识点描述</h2><p>Redis 3.2 中增加了对GEO类型的支持。GEO、Geographic，地理信息的缩写，该类型就是元素的2维坐标，在地图上就是经纬度，Redis基于该类型，提供了经纬度设置，查询，范围查询，经纬度hash等常见操作。</p>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis布隆过滤器算法</title>
      <link href="/2021/08/28/redis%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E7%AE%97%E6%B3%95/"/>
      <url>/2021/08/28/redis%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="redis布隆过滤器算法"><a href="#redis布隆过滤器算法" class="headerlink" title="redis布隆过滤器算法"></a>redis布隆过滤器算法</h1><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>在Flink编程过程当中用到的非常多</p>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RedisHyperLogLog基数统计算法</title>
      <link href="/2021/08/28/RedisHyperLogLog%E5%9F%BA%E6%95%B0%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%B3%95/"/>
      <url>/2021/08/28/RedisHyperLogLog%E5%9F%BA%E6%95%B0%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis位图法bitmap统计活跃用户</title>
      <link href="/2021/08/28/redis%E4%BD%8D%E5%9B%BE%E6%B3%95bitmap%E7%BB%9F%E8%AE%A1%E6%B4%BB%E8%B7%83%E7%94%A8%E6%88%B7/"/>
      <url>/2021/08/28/redis%E4%BD%8D%E5%9B%BE%E6%B3%95bitmap%E7%BB%9F%E8%AE%A1%E6%B4%BB%E8%B7%83%E7%94%A8%E6%88%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="redis位图法bitmap统计活跃用户"><a href="#redis位图法bitmap统计活跃用户" class="headerlink" title="redis位图法bitmap统计活跃用户"></a>redis位图法bitmap统计活跃用户</h1><p><a href="https://blog.csdn.net/angellee1988/article/details/104223088" target="_blank" rel="noopener">参考链接</a></p><p><a href="http://redisdoc.com/bitmap/index.html" target="_blank" rel="noopener">官方文档</a></p><h2 id="位图法"><a href="#位图法" class="headerlink" title="位图法"></a>位图法</h2><p>位图（bitmap），就是用位(bit)来表示存放的某种状态，如开关，有无。在redis中，字符串是以二进制的形式存储的，因此位图在redis中并不是一种数据类型，而是一种字符串的表现形式。位图中每个元素在内存中占用1位，所以可以节省存储空间。</p><h2 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h2><h3 id="1、SETBIT-key-offset-value"><a href="#1、SETBIT-key-offset-value" class="headerlink" title="1、SETBIT key offset value"></a>1、SETBIT key offset value</h3><p>该命令时间复杂度: O(1)，效率极高；</p><p>对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。</p><p>位的设置或清除取决于 value 参数，可以是 0 也可以是 1 。</p><p>当 key 不存在时，自动生成一个新的字符串值。</p><p>字符串会进行伸展以确保它可以将 value 保存在指定的偏移量上。当字符串值进行伸展时，空白位置以 0 填充。</p><p>offset 参数必须大于或等于 0 ，小于 2^32 (bit 映射被限制在 512 MB 之内)。</p><h3 id="2、BITOP-operation-destkey-key-key-…"><a href="#2、BITOP-operation-destkey-key-key-…" class="headerlink" title="2、BITOP operation destkey key [key …]"></a>2、BITOP operation destkey key [key …]</h3><p>该命令时间复杂度: O(N)</p><p>对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。</p><p>operation 可以是 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种：</p><p>BITOP AND destkey key [key …] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。</p><p>BITOP OR destkey key [key …] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。</p><p>BITOP XOR destkey key [key …] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。</p><p>BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。</p><p>除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。</p><h3 id="3、BITCOUNT-key-start-end"><a href="#3、BITCOUNT-key-start-end" class="headerlink" title="3、BITCOUNT key [start] [end]"></a>3、BITCOUNT key [start] [end]</h3><p>该命令时间复杂度: O(N)</p><p>计算给定字符串中，被设置为 1 的比特位的数量。</p><p>一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。</p><p>start 和 end 参数的设置和 GETRANGE key start end 命令类似，都可以使用负数值： 比如 -1 表示最后一个字节， -2 表示倒数第二个字节，以此类推。</p><p>不存在的 key 被当成是空字符串来处理，因此对一个不存在的 key 进行 BITCOUNT 操作，结果为 0 。</p><h2 id="统计活跃用户"><a href="#统计活跃用户" class="headerlink" title="统计活跃用户"></a>统计活跃用户</h2><p>为了统计每日登录的用户数，建立了一个bitmap, 每一位标识一个用户ID。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210828175935470.png" alt="统计活跃用户数"></p><p>当某个用户登录了网站或执行了某个操作，就在bitmap中把标识此用户的位置为1，未登录默认为0，</p><p>然后进行and操作，即可统计活跃用户数。</p><p>redis key可以设计成：时间周；</p><p>offset 用user_id。</p><p>用户登录数据如下：</p><p>100 、101、102、103 代表用户user_id</p><p>周一：{100,101,102,103}</p><p>周二：{101,102,103}</p><p>周三：{101,102}</p><p>周四：{100,101,102}</p><p>周五：{100,101,102,103}</p><p>周六：{101,102,103}</p><p>周日：{100,101,102}</p><p>一周内连续登录的用户有2个:{101,102}</p><p>以下使用命令演示一周的用户登录情况：</p><pre class=" language-sql"><code class="language-sql">dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token number">2</span>OKdm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT mon <span class="token number">100</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT mon <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT mon <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT mon <span class="token number">103</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT tue <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT tue <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT tue <span class="token number">103</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT wed <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT wed <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT wed <span class="token number">103</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT wed <span class="token number">103</span> <span class="token number">0</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">1</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT thu <span class="token number">100</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT thu <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT thu <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT fri <span class="token number">100</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT fri <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT fri <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT fri <span class="token number">103</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sat <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sat <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sat <span class="token number">103</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sun <span class="token number">100</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sun <span class="token number">101</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span>dm:<span class="token number">6379</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span> SETBIT sun <span class="token number">102</span> <span class="token number">1</span><span class="token punctuation">(</span><span class="token keyword">integer</span><span class="token punctuation">)</span> <span class="token number">0</span></code></pre><p>统计一周内连续登录用户：</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">dm</span><span class="token punctuation">:</span><span class="token attr-value">6379[2]> </span><span class="token attr-name">dm</span><span class="token punctuation">:</span><span class="token attr-value">6379[2]> BITOP AND result mon tue wed thu fri sat sun</span><span class="token attr-name">(integer)</span> <span class="token attr-value">13</span><span class="token attr-name">dm</span><span class="token punctuation">:</span><span class="token attr-value">6379[2]> BITCOUNT result</span><span class="token attr-name">(integer)</span> <span class="token attr-value">2</span></code></pre><h2 id="使用-bitmap-实现用户上线次数统计"><a href="#使用-bitmap-实现用户上线次数统计" class="headerlink" title="使用 bitmap 实现用户上线次数统计"></a>使用 bitmap 实现用户上线次数统计</h2><p>假设现在我们希望记录自己网站上的用户的上线频率，比如说，计算用户 A 上线了多少天，用户 B 上线了多少天，诸如此类，以此作为数据，从而决定让哪些用户参加 beta 测试等活动 —— 这个模式可以使用 SETBIT key offset value 和 BITCOUNT key [start] [end] 来实现。</p><p>比如说，每当用户在某一天上线的时候，我们就使用 SETBIT key offset value ，以用户名作为 key ，将那天所代表的网站的上线日作为 offset 参数，并将这个 offset 上的value设置为 1 。</p><p>举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。</p><p>当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT key [start] [end] 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。</p>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive3.1.2解决控制台日志</title>
      <link href="/2021/08/27/hive3-1-2%E8%A7%A3%E5%86%B3%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%97%A5%E5%BF%97/"/>
      <url>/2021/08/27/hive3-1-2%E8%A7%A3%E5%86%B3%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h1 id="hive3-1-2解决控制台日志"><a href="#hive3-1-2解决控制台日志" class="headerlink" title="hive3.1.2解决控制台日志"></a>hive3.1.2解决控制台日志</h1><p>hive3.1.2这个版本，感觉到非常的烦，特点就是它的控制台的日志的问题，今天终于解决这个问题了，非常的开心</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827152035980.png" alt="大量控制台日志"></p><p>在hive/conf目录下添加这些信息 log4j.properties</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">log4j.rootLogger</span><span class="token punctuation">=</span><span class="token attr-value">WARN, CA</span><span class="token attr-name">log4j.appender.CA</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.ConsoleAppender</span><span class="token attr-name">log4j.appender.CA.layout</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.PatternLayout</span><span class="token attr-name">log4j.appender.CA.layout.ConversionPattern</span><span class="token punctuation">=</span><span class="token attr-value">%-4r [%t] %-5p %c %x - %m%n</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827152141335.png" alt="启动成功"></p><p>正常的提交命令都是可以去执行的，网上大多数的案例都是不能实现的，所以特别烦，纠结了我很久，这个大量的日志，让我看见就非常的烦</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827152342900.png" alt="正常的去执行"></p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis6学习记录</title>
      <link href="/2021/08/26/Redis6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
      <url>/2021/08/26/Redis6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink运行时架构</title>
      <link href="/2021/08/26/Flink%E8%BF%90%E8%A1%8C%E6%97%B6%E6%9E%B6%E6%9E%84/"/>
      <url>/2021/08/26/Flink%E8%BF%90%E8%A1%8C%E6%97%B6%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink运行时架构"><a href="#Flink运行时架构" class="headerlink" title="Flink运行时架构"></a>Flink运行时架构</h1><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826164621451.png" alt="运行架构"></p><h2 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h2><p>TaskManager心跳信息和统计信息，批次之间也会进行交换数据</p><p>一个任务执行完成之后，分发，有时候时slot和taskmanager，中涉及到一个传输数据的过程</p><h2 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h2><p>Jobmanager向ResourceManager申请，有则分配，无则等待，进一步yarn申请，启动taskManager来进行分配执行</p><h2 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h2><p>Job提交任务，提交一个接口，不是必须的，使用什么平台，有时候就没有整合，就是一个桥梁的作业</p><h2 id="任务提交流程"><a href="#任务提交流程" class="headerlink" title="任务提交流程"></a>任务提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826165847406.png" alt="任务提交流程"></p><p>ResourceManager只在分配的时候有关系</p><p>TaskManager时最重要的</p><h2 id="Yarn任务提交流程"><a href="#Yarn任务提交流程" class="headerlink" title="Yarn任务提交流程"></a>Yarn任务提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826171215444.png" alt="任务提交流程"></p><h2 id="任务调度原理"><a href="#任务调度原理" class="headerlink" title="任务调度原理"></a>任务调度原理</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826171236046.png" alt="任务调度"></p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul><li><input disabled="" type="checkbox"> 怎么样实现并行计算</li><li><input disabled="" type="checkbox"> 并行的任务，需要占用多少个slot</li><li><input disabled="" type="checkbox"> 一个流处理程序，到底包含多少个任务</li></ul>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink部署模式</title>
      <link href="/2021/08/26/Flink%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/"/>
      <url>/2021/08/26/Flink%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink部署模式"><a href="#Flink部署模式" class="headerlink" title="Flink部署模式"></a>Flink部署模式</h1><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/resource-providers/yarn/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/resource-providers/yarn/</a></p><h2 id="Flink-on-Yarn"><a href="#Flink-on-Yarn" class="headerlink" title="Flink on Yarn"></a>Flink on Yarn</h2><p>以 Yarn 模式部署 Flink 任务时，要求 Flink 是有 Hadoop 支持的版本，Hadoop 环境需要保证版本在 2.2 以上，并且集群中安装有 HDFS 服务。</p><p>Flink 提供了两种在 yarn 上运行的模式，分别为 Session-Cluster 和 Per-Job-Cluster 模式。</p><h3 id="1、-Session-cluster-模式"><a href="#1、-Session-cluster-模式" class="headerlink" title="1、 Session-cluster 模式"></a>1、 Session-cluster 模式</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826154328964.png" alt="Session-cluster模式"></p><p>​    Session-Cluster 模式需要先启动集群，然后再提交作业，接着会向 yarn 申请一 块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到 yarn 中的其中一个作业执行完成后，释放了资源，下个作业才会正常提交。所有作 业共享 Dispatcher 和 ResourceManager；共享资源；适合规模小执行时间短的作业。</p><p>​    在 yarn 中初始化一个 flink 集群，开辟指定的资源，以后提交任务都向这里提 交。这个 flink 集群会常驻在 yarn 集群中，除非手工停止。</p><h3 id="2、Session-Cluster"><a href="#2、Session-Cluster" class="headerlink" title="2、Session Cluster"></a>2、Session Cluster</h3><ul><li><input disabled="" type="checkbox"> 启动hd.sh start</li><li><input disabled="" type="checkbox"> 启动yarn-session</li></ul><pre class=" language-xml"><code class="language-xml">yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm wmy-flink-session -d</code></pre><p>-n 现在不能直接指定taskmanager的数量，一般设置了都是没有效果的</p><pre class=" language-xml"><code class="language-xml">其中：-n(--container)：TaskManager 的数量。-s(--slots)： 每个 TaskManager 的 slot 数量，默认一个 slot 一个 core，默认每个taskmanager 的 slot 的个数为 1，有时可以多一些 taskmanager，做冗余。-jm：JobManager 的内存（单位 MB)。-tm：每个 taskmanager 的内存（单位 MB)。-nm：yarn 的 appName(现在 yarn 的 ui 上的名字)。-d：后台执行。</code></pre><h4 id="2-1-错误"><a href="#2-1-错误" class="headerlink" title="2.1 错误"></a>2.1 错误</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826155759424.png" alt="启动yarn session失败"></p><p>这个是Flink 1.11.1  使用yarn-session 出现的错误；原因是在Flink1.11 之后不再提供flink-shaded-hadoop-*” jars</p><p>需要在yarn-session.sh 文件中添加 或者在环境变量中添加</p><p>export HADOOP_CLASSPATH=<code>hadoop classpath</code></p><p>分发给每一个flink节点</p><h4 id="2-2-yarn界面上就可以看到"><a href="#2-2-yarn界面上就可以看到" class="headerlink" title="2.2 yarn界面上就可以看到"></a>2.2 yarn界面上就可以看到</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826160342492.png" alt="yarn界面"></p><pre class=" language-shell"><code class="language-shell">flink run -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar --host flink01 --port</code></pre><h4 id="2-3-取消-yarn-session"><a href="#2-3-取消-yarn-session" class="headerlink" title="2.3  取消 yarn-session"></a>2.3  取消 yarn-session</h4><p>flink cancel appid</p><h3 id="3、yarn-pre-job"><a href="#3、yarn-pre-job" class="headerlink" title="3、yarn-pre-job"></a>3、yarn-pre-job</h3><pre class=" language-xml"><code class="language-xml">flink run –m yarn-cluster -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host flink04 –port 7777</code></pre><p>这个的话直接运行这个程序就可以了</p><h4 id="3-1-错误"><a href="#3-1-错误" class="headerlink" title="3.1 错误"></a>3.1 错误</h4><p>Please make sure to export the HADOOP_CLASSPATH environment variable or have hadoop in your classpath.</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkCDC读取mysql中时间类型</title>
      <link href="/2021/08/26/FlinkCDC%E8%AF%BB%E5%8F%96mysql%E4%B8%AD%E6%97%B6%E9%97%B4%E7%B1%BB%E5%9E%8B/"/>
      <url>/2021/08/26/FlinkCDC%E8%AF%BB%E5%8F%96mysql%E4%B8%AD%E6%97%B6%E9%97%B4%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 大数据采集系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据采集系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM学习总结</title>
      <link href="/2021/08/26/JVM%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
      <url>/2021/08/26/JVM%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Java_JUC并非编程</title>
      <link href="/2021/08/26/Java-JUC%E5%B9%B6%E9%9D%9E%E7%BC%96%E7%A8%8B/"/>
      <url>/2021/08/26/Java-JUC%E5%B9%B6%E9%9D%9E%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Java-JUC并非编程"><a href="#Java-JUC并非编程" class="headerlink" title="Java_JUC并非编程"></a>Java_JUC并非编程</h1>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink异步查询</title>
      <link href="/2021/08/26/Flink%E5%BC%82%E6%AD%A5%E6%9F%A5%E8%AF%A2/"/>
      <url>/2021/08/26/Flink%E5%BC%82%E6%AD%A5%E6%9F%A5%E8%AF%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink异步查询"><a href="#Flink异步查询" class="headerlink" title="Flink异步查询"></a>Flink异步查询</h1><h2 id="官网介绍"><a href="#官网介绍" class="headerlink" title="官网介绍"></a>官网介绍</h2><h3 id="用于外部数据访问的异步-I-O"><a href="#用于外部数据访问的异步-I-O" class="headerlink" title="用于外部数据访问的异步 I/O"></a>用于外部数据访问的异步 I/O</h3><p>此页面解释了 Flink 的 API 在外部数据存储中用于异步 I/O 的原因。对于不熟悉异步或事件驱动编程的用户，有关”未来”和”事件驱动”编程的文章可能是有用的准备。</p><p>注：有关异步I/O实用程序的设计和实施的细节，请见提案和设计文档<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65870673" target="_blank" rel="noopener">FLIP-12：异步 I/O 设计和实施</a>。</p><h3 id="异步-I-O-操作的需要"><a href="#异步-I-O-操作的需要" class="headerlink" title="异步 I/O 操作的需要"></a>异步 I/O 操作的需要</h3><p>在与外部系统交互时（例如，在用存储在数据库中的数据丰富流事件时），需要注意与外部系统的通信延迟并不主导流应用的总工作。</p><p>天真地访问外部数据库中的数据，例如在”同步交互”中，通常意味着<strong>同步</strong>交互：请求发送到数据库，并等待直到收到响应。在许多情况下，这种等待占功能的绝大多数时间。<code>MapFunction``MapFunction</code></p><p>与数据库的异步交互意味着单个并行函数实例可以同时处理许多请求并同时接收响应。这样，等待时间可以与发送其他请求和接收回复相叠加。至少，等待时间会因多个请求而摊销。这在大多数情况下导致更高的流处理量。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827135057327.png" alt="异步I/O"></p><p><em>注意：</em>在某些情况下，仅仅将吞吐量扩展至非常高的并行性也是可能的，但通常要付出非常高的资源成本：拥有更多并行 Map 功能实例意味着更多的任务、线程、Flink 内部网络连接、数据库的网络连接、缓冲和一般内部簿记开销。<code>MapFunction</code></p><h3 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h3><p>如上文所示，在数据库（或密钥/值存储）上实现适当的异步 I/O 需要客户端到支持异步请求的数据库。许多流行的数据库提供这样的客户端。</p><p>在没有此类客户端的情况下，可以通过创建多个客户端并使用线程池处理同步呼叫，尝试将同步客户端转变为有限的并发客户端。但是，这种方法通常不如适当的异步客户端效率低。</p><h3 id="Flink-I-O-API"><a href="#Flink-I-O-API" class="headerlink" title="Flink I/O API"></a>Flink I/O API</h3><p>Flink 的 Async I/O API 允许用户使用具有数据流的异步请求客户端。API 处理与数据流的集成，以及处理顺序、事件时间、故障容差等。</p><p>假设目标数据库具有异步客户端，则需要三个部分才能实现与数据库具有异步 I/O 的流转换：</p><ul><li>执行该执行该执行该请求<code>AsyncFunction</code></li><li><em>回调</em>，将操作结果交给<code>ResultFuture</code></li><li>将 Async I/O 操作应用于数据流作为转换</li></ul><p>以下代码示例说明了基本模式：</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// This example implements the asynchronous request and callback with Futures that have the</span><span class="token comment" spellcheck="true">// interface of Java 8's futures (which is the same one followed by Flink's Future)</span><span class="token comment" spellcheck="true">/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */</span><span class="token keyword">class</span> <span class="token class-name">AsyncDatabaseRequest</span> <span class="token keyword">extends</span> <span class="token class-name">RichAsyncFunction</span><span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Tuple2<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">>></span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">/** The database specific client that can issue concurrent requests with callbacks */</span>    <span class="token keyword">private</span> <span class="token keyword">transient</span> DatabaseClient client<span class="token punctuation">;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">open</span><span class="token punctuation">(</span>Configuration parameters<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        client <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DatabaseClient</span><span class="token punctuation">(</span>host<span class="token punctuation">,</span> post<span class="token punctuation">,</span> credentials<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        client<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">asyncInvoke</span><span class="token punctuation">(</span>String key<span class="token punctuation">,</span> <span class="token keyword">final</span> ResultFuture<span class="token operator">&lt;</span>Tuple2<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">>></span> resultFuture<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// issue the asynchronous request, receive a future for result</span>        <span class="token keyword">final</span> Future<span class="token operator">&lt;</span>String<span class="token operator">></span> result <span class="token operator">=</span> client<span class="token punctuation">.</span><span class="token function">query</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// set the callback to be executed once the request by the client is complete</span>        <span class="token comment" spellcheck="true">// the callback simply forwards the result to the result future</span>        CompletableFuture<span class="token punctuation">.</span><span class="token function">supplyAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Supplier</span><span class="token operator">&lt;</span>String<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> String <span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">try</span> <span class="token punctuation">{</span>                    <span class="token keyword">return</span> result<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">InterruptedException</span> <span class="token operator">|</span> ExecutionException e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token comment" spellcheck="true">// Normally handled explicitly.</span>                    <span class="token keyword">return</span> null<span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">thenAccept</span><span class="token punctuation">(</span> <span class="token punctuation">(</span>String dbResult<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>            resultFuture<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span>Collections<span class="token punctuation">.</span><span class="token function">singleton</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> dbResult<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// create the original stream</span>DataStream<span class="token operator">&lt;</span>String<span class="token operator">></span> stream <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// apply the async I/O transformation</span>DataStream<span class="token operator">&lt;</span>Tuple2<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">>></span> resultStream <span class="token operator">=</span>    AsyncDataStream<span class="token punctuation">.</span><span class="token function">unorderedWait</span><span class="token punctuation">(</span>stream<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">AsyncDatabaseRequest</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> TimeUnit<span class="token punctuation">.</span>MILLISECONDS<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong>重要说明</strong>：完成与第一次调用。所有后续呼叫都将被忽略。<code>ResultFuture``ResultFuture.complete``complete</code></p><p>以下两个参数控制异步操作：</p><ul><li><strong>超时</strong>：超时定义异步请求在被视为失败之前可能需要多长时间。此参数可以防止死/失效请求。</li><li><strong>容量</strong>：此参数定义了同时可能同时进行多少个异步请求。尽管 Async I/O 方法通常导致更好的吞吐量，但操作员仍可能是流媒体应用的瓶颈。限制并发请求的数量可确保运营商不会累积不断增加的待决请求积压，但一旦容量耗尽，就会触发反压力。</li></ul><h3 id="超时处理"><a href="#超时处理" class="headerlink" title="超时处理"></a>超时处理</h3><p>当无音位 I/O 请求被发出时，默认情况下会抛出一个例外并重新启动作业。如果您想要处理超时，您可以覆盖该方法。<code>AsyncFunction#timeout</code></p><p>这个方法是可以进行处理超时的，否则如果要是不处理的话就会导致数据丢失的情况</p><h3 id="结果顺序"><a href="#结果顺序" class="headerlink" title="结果顺序"></a>结果顺序</h3><p>经常以某些未定义的顺序完成的并发请求，基于该请求先完成。为了控制生成记录的发出顺序，Flink 提供两种模式：<code>AsyncFunction</code></p><ul><li><strong>未编序</strong>：异步请求完成后，立即会发出结果记录。在 async I/O 操作员之后，流中的记录顺序与以前不同。当以<em>处理时间</em>为基本时间特征时，此模式具有最低延迟和最低开销。用于此模式。<code>AsyncDataStream.unorderedWait(...)</code></li><li><strong>已订购</strong>：在这种情况顺序被保留。结果记录的发出顺序与触发异步请求（操作员输入记录的顺序）相同。为此，操作员缓冲结果记录，直到其所有先前的记录被发出（或过时）。这通常会在检查中引入一些额外的延迟和一些开销，因为与未定序模式相比，记录或结果在检查状态下维护的时间更长。用于此模式。<code>AsyncDataStream.orderedWait(...)</code></li></ul><h3 id="活动时间"><a href="#活动时间" class="headerlink" title="活动时间"></a>活动时间</h3><p>当流媒体应用程序与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/concepts/time/" target="_blank" rel="noopener">事件时间</a>配合使用时，水印将由异步 I/O 操作员正确处理。这意味着两种订单模式的具体含义如下：</p><ul><li><p><strong>无序</strong>：水印不会超过记录，反之亦然，这意味着水印会建立<em>一个顺序边界</em>。记录仅在水印之间无序发出。只有在发出水印后才会发出某个水印后发生的记录。水印在发出水印之前，只有在输入的所有结果记录之后才会发出。</p><p>这意味着，在水印的存在下，<em>无序</em>模式引入了一些与<em>订购</em>模式相同的延迟和管理开销。该架空的金额取决于水印频率。</p></li><li><p><strong>订购</strong>：水印和记录顺序被保存，就像记录之间的顺序被保存。与<em>处理时间</em>相比，开销没有显著变化。</p></li></ul><p>请记住，<em>摄入时间</em>是<em>事件时间</em>的特殊情况，具有基于源处理时间的自动生成水印。</p><h3 id="过错耐受性保证"><a href="#过错耐受性保证" class="headerlink" title="过错耐受性保证"></a>过错耐受性保证</h3><p>异步 I/O 操作员提供完全完全一次的故障容差保证。它将机上异步请求的记录存储在检查点中，并在从故障中恢复时恢复/重新触发请求。</p><h3 id="实施提示"><a href="#实施提示" class="headerlink" title="实施提示"></a>实施提示</h3><p>对于具有<em>执行器</em>（或 Scala 中的<em>执行文本</em>）进行回调的*”期货*”实施，我们建议使用 a，因为回调通常执行最小的工作，并且避免了额外的线程到线程交配开销。回调通常只将结果交给输出缓冲器，从而将其添加到输出缓冲中。从那里，包括记录发射和与检查点簿记交互的重逻辑无论如何发生在一个专门的螺纹池。<code>DirectExecutor``DirectExecutor``ResultFuture</code></p><p>A 可以通过或 。<code>DirectExecutor``org.apache.flink.runtime.concurrent.Executors.directExecutor()``com.google.common.util.concurrent.MoreExecutors.directExecutor()</code></p><h3 id="警告"><a href="#警告" class="headerlink" title="警告"></a>警告</h3><p><strong>无故障功能不称为多线程</strong></p><p>我们想在这里明确指出的一个常见的困惑是，这不是以多线程的方式调用的。只有一个实例，它被称为顺序为每个记录在流的每个分区。除非该方法返回快，并依赖于回调（由客户端），它不会导致适当的异步I/O。<code>AsyncFunction``AsyncFunction``asyncInvoke(...)</code></p><p>例如，以下模式会导致阻塞功能，从而使异步行为失效：<code>asyncInvoke(...)</code></p><ul><li>使用数据库客户端，其查找/查询方法呼叫阻止，直到结果被接收回来</li><li>阻止/等待方法内异步客户返回的未来类型对象<code>asyncInvoke(...)</code></li></ul><p><strong>出于一致性原因，Async 功能 （AsyncWait 操作器） 的操作员当前必须处于运营商链的头部</strong></p><p>由于问题的原因，我们目前必须打破运营商链，以防止潜在的一致性问题。这是对支持链条的以前行为的更改。需要旧行为并接受可能违反一致性保证的用户可以进行实例化，并将手动添加到工作图中，并将链条策略设置为链条通过。</p><pre class=" language-java"><code class="language-java">FLINK<span class="token operator">-</span><span class="token number">13063</span>``AsyncWaitOperator``AsyncWaitOperator``AsyncWaitOperator#<span class="token function">setChainingStrategy</span><span class="token punctuation">(</span>ChainingStrategy<span class="token punctuation">.</span>ALWAYS<span class="token punctuation">)</span></code></pre><h2 id="Flink-异步IO实战"><a href="#Flink-异步IO实战" class="headerlink" title="Flink 异步IO实战"></a>Flink 异步IO实战</h2><p>首先通过官网的一个图片了解一下Asynchronous I/O Operation</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827141057088.png" alt="Flink 异步IO实战"></p><p>Flink source收到一条数据就会进行处理，如果需要通过这条数据关联外部数据源，例如mysql，在发出查询请求后，同步IO的方式是会等待查询结果再处理下一条数据的查询，也就是每一条数据都要等待上一个查询结束。而异步IO是指数据来了以后发出查询请求，先不等查询结果，直接继续发送下一条的查询请求，对于查询结果是异步返回的，返回结果之后再进入下一个算子的计算。这两种方式性能差距请看下的样例。</p><h3 id="1、生成6条数据，从0开始递增的6个数字。模拟异步查询之后，加上时间戳输出"><a href="#1、生成6条数据，从0开始递增的6个数字。模拟异步查询之后，加上时间戳输出" class="headerlink" title="1、生成6条数据，从0开始递增的6个数字。模拟异步查询之后，加上时间戳输出"></a>1、生成6条数据，从0开始递增的6个数字。模拟异步查询之后，加上时间戳输出</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>jc<span class="token punctuation">.</span>operation<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>MapFunction<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>AsyncDataStream<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>DataStream<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span>DataStreamSource<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>environment<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>async<span class="token punctuation">.</span>AsyncFunction<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>source<span class="token punctuation">.</span>SourceFunction<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>concurrent<span class="token punctuation">.</span>TimeUnit<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: wmyFlinkWarehouseItem * @package_name: com.wmy.flink.jc.operation * @Author: wmy * @Date: 2021/8/27 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: Flink 异步查询 * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">AsyncIODemo</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>         <span class="token comment" spellcheck="true">// 打印版本的info</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"wmy-version-01"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 创建测试环境</span>        StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        env<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">final</span> <span class="token keyword">int</span> maxCount <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">;</span>        <span class="token keyword">final</span> <span class="token keyword">int</span> taskNum <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">final</span> <span class="token keyword">long</span> timeout <span class="token operator">=</span> <span class="token number">40000</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 读取数据源</span>        DataStreamSource<span class="token operator">&lt;</span>Integer<span class="token operator">></span> inputStream <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">SimpleSource</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        AsyncFunction<span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token operator">></span> function <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SampleAsyncFunction</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        DataStream<span class="token operator">&lt;</span>String<span class="token operator">></span> result <span class="token operator">=</span> AsyncDataStream<span class="token punctuation">.</span><span class="token function">unorderedWait</span><span class="token punctuation">(</span>                inputStream<span class="token punctuation">,</span>                function<span class="token punctuation">,</span>                timeout<span class="token punctuation">,</span>                TimeUnit<span class="token punctuation">.</span>MILLISECONDS<span class="token punctuation">,</span>                <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span>taskNum<span class="token punctuation">)</span><span class="token punctuation">;</span>        result<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MapFunction</span><span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> String <span class="token function">map</span><span class="token punctuation">(</span>String value<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>                <span class="token keyword">return</span> value <span class="token operator">+</span> <span class="token string">","</span> <span class="token operator">+</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 启动程序</span>        env<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"wmy-version-01"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 自定义数据源</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">SimpleSource</span> <span class="token keyword">implements</span> <span class="token class-name">SourceFunction</span><span class="token operator">&lt;</span>Integer<span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token keyword">private</span> <span class="token keyword">volatile</span> <span class="token keyword">boolean</span> isRunning <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>        <span class="token keyword">private</span> <span class="token keyword">int</span> counter <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">private</span> <span class="token keyword">int</span> start <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">public</span> <span class="token function">SimpleSource</span><span class="token punctuation">(</span><span class="token keyword">int</span> counter<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">this</span><span class="token punctuation">.</span>counter <span class="token operator">=</span> counter<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">run</span><span class="token punctuation">(</span>SourceContext<span class="token operator">&lt;</span>Integer<span class="token operator">></span> ctx<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>start <span class="token operator">&lt;</span> counter <span class="token operator">||</span> counter <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> isRunning<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>ctx<span class="token punctuation">.</span><span class="token function">getCheckpointLock</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"send data: "</span> <span class="token operator">+</span> start<span class="token punctuation">)</span><span class="token punctuation">;</span>                    ctx<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token operator">++</span>start<span class="token punctuation">;</span>                <span class="token punctuation">}</span>                Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span>10L<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">cancel</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            isRunning <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>wmy<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>jc<span class="token punctuation">.</span>operation<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>async<span class="token punctuation">.</span>ResultFuture<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>async<span class="token punctuation">.</span>RichAsyncFunction<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ArrayList<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collections<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>concurrent<span class="token punctuation">.</span>CompletableFuture<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>function<span class="token punctuation">.</span>Supplier<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @project_name: wmyFlinkWarehouseItem * @package_name: com.wmy.flink.jc.operation * @Author: wmy * @Date: 2021/8/27 * @Major: 数据科学与大数据技术 * @Post：大数据实时开发 * @Email：wmy_2000@163.com * @Desription: * @Version: wmy-version-01 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SampleAsyncFunction</span> <span class="token keyword">extends</span> <span class="token class-name">RichAsyncFunction</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token operator">></span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">long</span><span class="token punctuation">[</span><span class="token punctuation">]</span> sleep <span class="token operator">=</span> <span class="token punctuation">{</span>100L<span class="token punctuation">,</span> 1000L<span class="token punctuation">,</span> 5000L<span class="token punctuation">,</span> 2000L<span class="token punctuation">,</span> 6000L<span class="token punctuation">,</span> 100L<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">open</span><span class="token punctuation">(</span>Configuration parameters<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span>parameters<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">asyncInvoke</span><span class="token punctuation">(</span>Integer input<span class="token punctuation">,</span> ResultFuture<span class="token operator">&lt;</span>String<span class="token operator">></span> resultFuture<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"-input:"</span> <span class="token operator">+</span> input <span class="token operator">+</span> <span class="token string">" will sleep "</span> <span class="token operator">+</span> sleep<span class="token punctuation">[</span>input<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" ms"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">query</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> resultFuture<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">private</span> <span class="token keyword">void</span> <span class="token function">query</span><span class="token punctuation">(</span><span class="token keyword">final</span> Integer input<span class="token punctuation">,</span> <span class="token keyword">final</span> ResultFuture<span class="token operator">&lt;</span>String<span class="token operator">></span> resultFuture<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>            Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span>sleep<span class="token punctuation">[</span>input<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            resultFuture<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span>Collections<span class="token punctuation">.</span><span class="token function">singletonList</span><span class="token punctuation">(</span>String<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">InterruptedException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>            resultFuture<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">private</span> <span class="token keyword">void</span> <span class="token function">asyncQuery</span><span class="token punctuation">(</span><span class="token keyword">final</span> Integer input<span class="token punctuation">,</span> <span class="token keyword">final</span> ResultFuture<span class="token operator">&lt;</span>String<span class="token operator">></span> resultFuture<span class="token punctuation">)</span> <span class="token punctuation">{</span>        CompletableFuture<span class="token punctuation">.</span><span class="token function">supplyAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Supplier</span><span class="token operator">&lt;</span>Integer<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> Integer <span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">try</span> <span class="token punctuation">{</span>                    Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span>sleep<span class="token punctuation">[</span>input<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">return</span> input<span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token keyword">return</span> null<span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">thenAccept</span><span class="token punctuation">(</span><span class="token punctuation">(</span>Integer dbResult<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>            resultFuture<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span>Collections<span class="token punctuation">.</span><span class="token function">singleton</span><span class="token punctuation">(</span>String<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>dbResult<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis读写分离实战</title>
      <link href="/2021/08/26/Redis%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E5%AE%9E%E6%88%98/"/>
      <url>/2021/08/26/Redis%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkCDC-SQL-debezium实战案例</title>
      <link href="/2021/08/26/FlinkCDC-SQL-debezium%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B/"/>
      <url>/2021/08/26/FlinkCDC-SQL-debezium%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink安装和配置</title>
      <link href="/2021/08/25/Flink%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
      <url>/2021/08/25/Flink%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink安装和配置"><a href="#Flink安装和配置" class="headerlink" title="Flink安装和配置"></a>Flink安装和配置</h1><h2 id="1、Flink安装和配置"><a href="#1、Flink安装和配置" class="headerlink" title="1、Flink安装和配置"></a>1、Flink安装和配置</h2><p>flink的安装是非常简单的，其中只需要配置一些文件和参数就好了</p><p>flink的运行时环境是非常重要的</p><p>RPC是远程过程调用，这样的RPC调用过程</p><p>Jobmanager是配置运行的JVM进程，堆内存的 大小，TaskManager的大小，定义的，和堆内存是有区别的，Flink集群是有状态的流式计算，如果要让Flink管理的内存是放在堆外的空间，堆内存创建的对象都是放到里面去，堆内存=堆内内存 + 堆外内存</p><p>heap.size堆内存，堆外内存很难区分，后面就分开了</p><p>问题，TaskManager &gt;  Jobmanager</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">numberOfTaskSlots</span> <span class="token attr-value"> ---> 当前的并行处理，一个taskmanager如何多线程勒，运行独立的线程，一个节点只有要给槽位，多线程执行的话只能一个执行，这样的就是要给并行执行的，这个是静态的能力</span>这个是针对于一个taskmanager来进行决定的<span class="token attr-name">最大的并行能力</span> <span class="token punctuation">=</span> <span class="token attr-value">ts * numberOfTaskSlots</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">workers</span> <span class="token attr-value">这个是对于taskmanager</span>记住在新版本中，主节点是单独的作为要给节点的可以在网上看这些配置，可以在这个网上就行管理这些内存，可以去查看这些的配置信息</code></pre><h2 id="2、提交任务"><a href="#2、提交任务" class="headerlink" title="2、提交任务"></a>2、提交任务</h2><p>我们在实际生产过程当中都是采用jar上传的方式来进行上传的过程，很少是采用在页面上提交的方式，集群的默认并行度是在本地来进行测试的，并行度的灵活配置方式，env的全局环境配置</p><p>并行度的设置都是可以在每个操作的算子中来进行配置的</p><p>要习惯在实际的生产过程当中的测试环境和我们在编写代码的这个过程当中的一个环境</p><p>了解配置的选项我们的实际工作当中的写代码是有好处的，提交jar的参数也是可以设置很多的命令行的提交</p><p>这个的默认的配置是和算子以及我们在提交Job的时候是非常重要的</p><h2 id="3、观察提交的任务链"><a href="#3、观察提交的任务链" class="headerlink" title="3、观察提交的任务链"></a>3、观察提交的任务链</h2><pre class=" language-properties"><code class="language-properties">在实际生产的过程当中看web端的链条可以简单的看出我们的代码的优化量有那些的</code></pre><h2 id="4、保存点"><a href="#4、保存点" class="headerlink" title="4、保存点"></a>4、保存点</h2><pre class=" language-properties"><code class="language-properties">这些都是面试的重点同时的话也是生产的重点</code></pre><h2 id="5、提交资料的申请"><a href="#5、提交资料的申请" class="headerlink" title="5、提交资料的申请"></a>5、提交资料的申请</h2><pre class=" language-properties"><code class="language-properties">如果资源不够的情况下是会报错的，所以我们要注意并行的这些的算子的操作是否是和我们的算子是一样的</code></pre><h2 id="6、资源的分配和并行度"><a href="#6、资源的分配和并行度" class="headerlink" title="6、资源的分配和并行度"></a>6、资源的分配和并行度</h2><pre class=" language-properties"><code class="language-properties">如何才能让作业才能运行起来的，调大集群的资源，这个在生产中是不允许的额，可以把任务的并行度调低，四个任务，slot，每一个线程占用要slot，推荐给当前的CPU的核心数<span class="token attr-name">最大的并行度</span> <span class="token attr-value">就是和最大的那个</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0005-数据开发规范</title>
      <link href="/2021/08/25/0005-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/"/>
      <url>/2021/08/25/0005-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<h1 id="0005-数据开发规范"><a href="#0005-数据开发规范" class="headerlink" title="0005-数据开发规范"></a>0005-数据开发规范</h1><p>话说，无规矩不成方圆，在数据开发领域同样也是适用的。</p><p>是不是经常听到这样的抱怨：</p><ul><li><p>我擦，这张表怎么特么的注释都没有？</p></li><li><p>我日，这个字段命名怎么是拼音全称？</p></li><li><p>怎么这张表的订单id是string类型，那张表的是订单id是bigint类型？</p></li><li><p>害，烦死了，这是谁开发的任务，库名/项目名都没写，我这去其他项目下跑还得一个个加上。</p></li><li><p>这代码谁写的？2张表join获取相关字段，也不写一下表的别名，我哪知道这是这是取自哪张表的 ？</p></li><li><p>运维任务又报错了？ 这是谁写的select * ，union all呀，害死人，上游表加字段了，这搞的。</p></li></ul><h2 id="一-词根"><a href="#一-词根" class="headerlink" title="一 词根"></a>一 词根</h2><p>词根的作用就是大家在对字段，表，指标等命名的时候作为查询使用，需要陆续完善</p><table><thead><tr><th>中文名</th><th>英文</th><th>评审时间</th><th>负责人</th></tr></thead><tbody><tr><td>数量</td><td>cnt</td><td>20210816</td><td>数据研发工程师</td></tr></tbody></table><h2 id="二-词典"><a href="#二-词典" class="headerlink" title="二 词典"></a>二 词典</h2><p>词典比词根更近一步，大家在对字段命名的时候，首先可以参考词典，以及可以参考其字段类型，使各个开发对同一个中文的字段命名及类型全部一致</p><table><thead><tr><th>中文名</th><th>英文名</th><th>字段类型</th><th>评审时间</th></tr></thead><tbody><tr><td>订单id</td><td>order_id</td><td>String</td><td>20210816</td></tr></tbody></table><h2 id="三-命名规范"><a href="#三-命名规范" class="headerlink" title="三 命名规范"></a>三 命名规范</h2><p>表命名</p><p>ods层</p><p>dwd层</p><p>dim层</p><p>dws层</p><p>ads层</p><p>临时表</p><p>手工表</p><p>表命名一般需要加入一及数据域，二级数据域，所以最开始需要划分数据域/主题域</p><p>然后详细的命名方式，根据每家公司有所不同</p><p>任务命名</p><p>一般情况下，任务名与输出表名一致</p><p>字段命名</p><p>参考 词根，词典</p><p>指标命名</p><p>略，之后讲指标平台的时候详细讲</p><h2 id="四-ddl规范"><a href="#四-ddl规范" class="headerlink" title="四 ddl规范"></a>四 ddl规范</h2><p>需不需要加生命周期</p><p>需不需要加创建人，创建时间</p><p>ddl语句需不需要单独维护</p><h2 id="五-sql编写规范"><a href="#五-sql编写规范" class="headerlink" title="五 sql编写规范"></a>五 sql编写规范</h2><p>关于缩进，大小写，空格，注释啥的细节就不说了，讲一下重点</p><p>多表关联时，需要使用别名来引用列 如： t1.user_id</p><p>表名前面需要加上库名/项目名，如 ods.test</p><p>不要出现select * 操作</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825170806956.png" alt="数据开发规范"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0004-数据清洗</title>
      <link href="/2021/08/25/0004-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
      <url>/2021/08/25/0004-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</url>
      
        <content type="html"><![CDATA[<h1 id="0004-数据清洗"><a href="#0004-数据清洗" class="headerlink" title="0004-数据清洗"></a>0004-数据清洗</h1><p>在我们想尽各种办法把数据弄进数据仓库ods层后，接下来的事情就比较有意思了，并且比较重要，对后续的数据模型建设，数据质量的保证，甚至影响管理层的决策(就问你怕不怕？)</p><p>那么，对于ETL过程中的数据清洗，你一般会怎么做呢？但凡你真正的做过数仓，我认为这些都是轻车熟路的，因为这是数据研发的必经之路</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164340099.png" alt="ETL"></p><p>我在对候选人进行考察的时候，也经常会问到这个问题，主要是看一下候选人有没有真实工作经验，然后在开发过程中有没有独立思考，并且知其然更知其所以然，但是候选人回答的都比较片面，比如只是处理空值，可能是公司数据质量的原因，但是就算公司业务库数据质量比较高，我们也应该需要全面的了解一些数据清洗规范。</p><p>那么我就来跟大家分享一下，工作中一般的数据清洗包含哪些并且一般怎么处理的吧。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164432662.png" alt="数据清洗"></p><h2 id="一-空值处理"><a href="#一-空值处理" class="headerlink" title="一 空值处理"></a>一 空值处理</h2><p>什么是空值，就是我们数据抽过来之后，发现有很多的字段为空，为了保证数据完整性或者方面后续的数据处理，这些我们都是需要预先处理的。一般使用默认值或者中位数，使用默认值的成本更低一点吧。</p><p>那么我们需要怎么处理呢，这个就要区分字段类型了。</p><table><thead><tr><th>类型</th><th>默认值</th><th>备注</th></tr></thead><tbody><tr><td>string</td><td>‘未知’，’unkown’，’-999’</td><td>描述，姓名</td></tr><tr><td>int</td><td>0</td><td>次数</td></tr><tr><td>bigint</td><td>0</td><td>次数</td></tr><tr><td>smallint/tinyint</td><td>0</td><td>是否</td></tr><tr><td>double</td><td>0</td><td>金额</td></tr><tr><td>decimal(16,4)</td><td>0</td><td>金额</td></tr><tr><td>datetime</td><td>1970-01-01 00:00:00</td><td>时间</td></tr></tbody></table><p>以上会有一个小问题，string类型给了默认值之后，如果是主键，在进行join操作的时候，本来关联不上的，由于给了默认值，就能关联上了，这种情况可能要单独处理一下。</p><p>但是每种类型的默认值需要和数据分析师，业务方，需求方等其他部门同步一下，避免出现懵逼情况，比如 1970-01-01 00:00:00是个啥</p><h2 id="二-数据格式处理"><a href="#二-数据格式处理" class="headerlink" title="二 数据格式处理"></a>二 数据格式处理</h2><p>数据格式处理，主要包含2个方面，一个是复杂数据类型的解析处理，比如json解析等</p><p>另一方面是内容的统一，对于日期字段数据格式，可以统一为 yyyy-MM-dd</p><h2 id="三-枚举值处理"><a href="#三-枚举值处理" class="headerlink" title="三 枚举值处理"></a>三 枚举值处理</h2><p>我们的数据源可能会来自很多系统，业务开发的习惯可能不太相同，导致枚举值也不太一样，举个例子  a表的 1 代表 男，2代表女，b表的1代表女，2代表男，如果不统一一下的话，将来使用数据的时候就会产生误解，当作为关联条件去关联的时候，更会直接导致数据错误</p><h2 id="四-字段类型处理"><a href="#四-字段类型处理" class="headerlink" title="四 字段类型处理"></a>四 字段类型处理</h2><p>对于同一个字段，他们的字段类型我们需要做到统一，比如 订单id，不能a表是string类型，b表是bigint类型，统一为string类型不好吗？</p><h2 id="五-注释处理"><a href="#五-注释处理" class="headerlink" title="五 注释处理"></a>五 注释处理</h2><p>主要是对业务库没有注释的表或者字段进行补充完善。自己不太确定的需要及时找业务开发确认清楚。</p><h2 id="六-敏感数据处理"><a href="#六-敏感数据处理" class="headerlink" title="六 敏感数据处理"></a>六 敏感数据处理</h2><p>理论上来说数仓的敏感数据都是需要加密的，直到输出到业务库或者其他服务于业务方的存储系统，我们在同步的时候需要解密传输。</p><p>对于姓名，手机号，身份证号，邮箱，银行卡号这些，我们都需要通过自带的加密函数或者udf函数进行加密，保证数据安全。</p><h2 id="七-数据单位统一"><a href="#七-数据单位统一" class="headerlink" title="七 数据单位统一"></a>七 数据单位统一</h2><p>数据单位统一，这个比较好理解了，比如金额，一张表是美元，一张表是人民币，你在对2张表的金额做操作的时候，是不是会有问题，所以这个需要提前定好，统一清洗。</p><h2 id="八-逻辑错误清洗"><a href="#八-逻辑错误清洗" class="headerlink" title="八 逻辑错误清洗"></a>八 逻辑错误清洗</h2><p>举几个简单的例子：年龄不可能超过200岁吧，身份证号不可能是5位吧？</p><p>这个具体怎么清洗，需要根据实际的业务场景。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825165711383.png" alt="数据清洗流程"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0003-数据同步策略</title>
      <link href="/2021/08/25/0003-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/"/>
      <url>/2021/08/25/0003-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0003-数据同步策略"><a href="#0003-数据同步策略" class="headerlink" title="0003-数据同步策略"></a>0003-数据同步策略</h1><p>我们要搭建数据仓库，那么数据是来自哪里呢？</p><p>答案很明显，一般来说，数据会来自3个方向，业务库数据，日志数据及爬虫数据，对于爬虫数据，我们就不做过多的分析，一般就是爬一些竞品信息，维表信息。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825163624184.png" alt="数据同步"></p><p>业务库数据</p><p>什么是业务库数据，指的是业务系统产生的数据，这个业务系统可以是你们公司的官网，app或者小程序。</p><p>对于业务库数据，如果是t+1同步抽取的话，我们一般会选择sqoop或者datax，当然，有些传统企业会使用kettle，不过个人认为kettle还是比较重的，而且好像也不太方便，所以不是很推荐；</p><p>如果是需要实时同步的话，我们一般使用开启binlog，采用canal+kafka的模式实时监听同步。或者使用flink-cdc都是可以实现的</p><p>对于离线抽取，不管是使用什么工具，都会设计到一个抽取策略问题，比如500数据，5万数据，30万数据，1000万数据分别需要怎么存储？</p><p>对于1000万数据来说，我们肯定是需要增量进行抽取的，但是业务库一般来说需要满足如下3个条件，我们才能进行增量抽取：</p><ul><li><p>主键唯一</p></li><li><p>有更新时间并且数据更新时，该时间也会改变</p></li><li><p>不存在物理删除的情况</p></li></ul><p>那么500数据，5w数据和30万数据呢？这个不能拍脑袋就决定了，需要综合考虑，比如该业务上线时长，该业务预期的数据增长等，这个需要与业务方和对应的业务开发进行沟通</p><p>比如50条数据是维表数据，那么全量抽取问题不大，如果是刚上线的业务，且可预见的后期数据量会爆发式增长，那么还是增量抽取比较靠谱，免得后期频繁的修改抽取任务</p><p>日志数据</p><p>日志数据一般来说指的是埋点日志，但是有些ng日志或者其他的日志也会上报进行采集。</p><p>对于日志数据，目前主流的工具可能是flume和logstash，这个好像就没啥特别好说的了，生产环境对于日志数据的要求不会太高，多一点少一点其实问题都不太大。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825164015829.png" alt="数据同步"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0002-数据探查</title>
      <link href="/2021/08/25/0002-%E6%95%B0%E6%8D%AE%E6%8E%A2%E6%9F%A5/"/>
      <url>/2021/08/25/0002-%E6%95%B0%E6%8D%AE%E6%8E%A2%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="0002-数据探查"><a href="#0002-数据探查" class="headerlink" title="0002-数据探查"></a>0002-数据探查</h1><p><font color="blue">给你一个需求或者一条新的业务线可能需要搭建数仓，做数据处理，然后提供一些指标数据给到需求方，你这边会怎么开始呢？</font></p><p><font color="blue">当你接到这个需求的时候，直接开干？抽表，清洗，分层，建模？</font></p><p><font color="blue">呵呵，这就是瞎搞。啥也不知道不了解的情况下，对数据源一无所知的情况下，是谁给你勇气直接开干的？</font></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825161712447.png" alt="数据探查"></p><p>正常情况下，接到新的数据需求或者任务的时候，我们最开始应该需要做一下数据调研，或者说数据探查。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825162346421.png" alt="数据调研"></p><p>那么问题来了，具体我们应该做些啥呢？就拿接到一个需求来举例说明吧：</p><p>那么问题来了，具体我们应该做些啥呢？就拿接到一个需求来举例说明吧：</p><p>业务方让你统计一些数据指标，给了你相关口径，表给没给这个就不清楚了，理论上是要给的，但是如果他自己也不知道，那这个也是没办法的。</p><p>给了的话还好，你直接去业务库查看这些表，但是没给的话，你可能要多一些步骤，可能要去找对应的产品经理或者业务开发帮你一起找。最后，你终于找到需要的那些表了。</p><p>拿到表之后怎么办？慌了吗？</p><p>duck不必，我们可以开心的做数据探查了，对数据有一个整体的把控。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825163205224.png" alt="整体把控"></p><ul><li><p>了解数据表字段能不能满足需求发需求，看看字段有没有少，如果发现少了或者提供的数据源没法满足需求，需要及时找对应的人员沟通，可别自己瞎搞或者跑路了</p></li><li><p>看一下相关表的数据结构，有没有json数据或者或者其他比较复杂的数据，便于数仓处理</p></li><li><p>看一下数据内容，是不是有些字段毫无意义，不需要存储在数仓，或者看一下是不是有很多空置的情况，脏数据情况，其实就是探查一下业务库的数据质量，通过这些我们可以大概判断在数仓会有多少的清洗机制</p></li><li><p>看一下数据量，方便我们在抽取的时候选择增量抽取或者全量抽取，甚至可以问一下业务方业务增长情况，更能准确的决定数据抽取策略。</p></li></ul><p>全部探查完以后，你就可以继续以下的步骤了。</p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0001-数据仓库体系</title>
      <link href="/2021/08/25/0001-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%BD%93%E7%B3%BB/"/>
      <url>/2021/08/25/0001-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%BD%93%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="0001-数据仓库体系"><a href="#0001-数据仓库体系" class="headerlink" title="0001-数据仓库体系"></a>0001-数据仓库体系</h1><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825160959524.png" alt="数据仓库体系"></p>]]></content>
      
      
      <categories>
          
          <category> 新数据仓库体系 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新数据仓库体系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive高级源码</title>
      <link href="/2021/08/25/Hive%E9%AB%98%E7%BA%A7%E6%BA%90%E7%A0%81/"/>
      <url>/2021/08/25/Hive%E9%AB%98%E7%BA%A7%E6%BA%90%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive高级源码"><a href="#Hive高级源码" class="headerlink" title="Hive高级源码"></a>Hive高级源码</h1><ul><li><input disabled="" type="checkbox"> 组件分析和核心流程 重点</li><li><input disabled="" type="checkbox"> Hive源码解读 —&gt; 分支解读</li><li><input disabled="" type="checkbox"> Hive Debug —&gt; MR,Spark,Tez —&gt; 某个分支解读</li></ul><h2 id="第-1-章-HQL-是如何转换为-MR-任务的"><a href="#第-1-章-HQL-是如何转换为-MR-任务的" class="headerlink" title="第 1 章 HQL 是如何转换为 MR 任务的"></a>第 1 章 HQL 是如何转换为 MR 任务的</h2><h3 id="1-1-Hive-的核心组成介绍"><a href="#1-1-Hive-的核心组成介绍" class="headerlink" title="1.1 Hive 的核心组成介绍"></a>1.1 Hive 的核心组成介绍</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825153641750.png" alt="hive架构"></p><h4 id="1）用户接口：Client"><a href="#1）用户接口：Client" class="headerlink" title="1）用户接口：Client"></a>1）用户接口：Client</h4><p>​    CLI（command-line interface）、JDBC/ODBC(jdbc 访问 hive)、WEBUI（浏览器访问 hive）</p><h4 id="2）元数据：Metastore"><a href="#2）元数据：Metastore" class="headerlink" title="2）元数据：Metastore"></a>2）元数据：Metastore</h4><p>​    元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表 的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore</p><h4 id="3）Hadoop"><a href="#3）Hadoop" class="headerlink" title="3）Hadoop"></a>3）Hadoop</h4><p>​    使用 HDFS 进行存储，使用 MapReduce 进行计算。</p><h4 id="4）驱动器：Driver"><a href="#4）驱动器：Driver" class="headerlink" title="4）驱动器：Driver"></a>4）驱动器：Driver</h4><h4 id="5）解析器（SQL-Parser）"><a href="#5）解析器（SQL-Parser）" class="headerlink" title="5）解析器（SQL Parser）"></a>5）解析器（SQL Parser）</h4><p>​    将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr； 对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误</p><h4 id="6）编译器（Physical-Plan）"><a href="#6）编译器（Physical-Plan）" class="headerlink" title="6）编译器（Physical Plan）"></a>6）编译器（Physical Plan）</h4><p>​    将 AST 编译生成逻辑执行计划。</p><h4 id="7）优化器（Query-Optimizer）"><a href="#7）优化器（Query-Optimizer）" class="headerlink" title="7）优化器（Query Optimizer）"></a>7）优化器（Query Optimizer）</h4><p>​    对逻辑执行计划进行优化。</p><h4 id="8）执行器（Execution）"><a href="#8）执行器（Execution）" class="headerlink" title="8）执行器（Execution）"></a>8）执行器（Execution）</h4><p>​    把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。</p><h3 id="1-2-HQL-转换为-MR-任务流程说明"><a href="#1-2-HQL-转换为-MR-任务流程说明" class="headerlink" title="1.2 HQL 转换为 MR 任务流程说明"></a>1.2 HQL 转换为 MR 任务流程说明</h3><ul><li><input disabled="" type="checkbox"> 1.进入程序，利用Antlr框架定义HQL的语法规则，对HQL完成词法语法解析，将HQL转换为为AST（抽象语法树）；</li><li><input disabled="" type="checkbox"> 2.遍历AST，抽象出查询的基本组成单元QueryBlock（查询块），可以理解为最小的查询执行单元；</li><li><input disabled="" type="checkbox"> 3.遍历QueryBlock，将其转换为OperatorTree（操作树，也就是逻辑执行计划），可以理解为不可拆分的一个逻辑执行单元；</li><li><input disabled="" type="checkbox"> 4.使用逻辑优化器对OperatorTree（操作树）进行逻辑优化。例如合并不必要的ReduceSinkOperator，减少Shuffle数据量；</li><li><input disabled="" type="checkbox"> 5.遍历OperatorTree，转换为TaskTree。也就是翻译为MR任务的流程，将逻辑执行计划转换为物理执行计划；</li><li><input disabled="" type="checkbox"> 6.使用物理优化器对TaskTree进行物理优化；</li><li><input disabled="" type="checkbox"> 7.生成最终的执行计划，提交任务到Hadoop集群运行</li></ul><p>总结：解析，查询块，操作树，逻辑优化，任务树，物理优化，提交任务执行</p><h3 id="1-3-HQL转换为MR核心流程"><a href="#1-3-HQL转换为MR核心流程" class="headerlink" title="1.3 HQL转换为MR核心流程"></a>1.3 HQL转换为MR核心流程</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210827144442869.png" alt="HQL 转换流程"></p><h3 id="1-4-导入源码工程"><a href="#1-4-导入源码工程" class="headerlink" title="1.4 导入源码工程"></a>1.4 导入源码工程</h3><h3 id="1-5-准备工作"><a href="#1-5-准备工作" class="headerlink" title="1.5 准备工作"></a>1.5 准备工作</h3><p>众所周知，我们执行一个 HQL 语句通常有以下几种方式：</p><p> 1）$HIVE_HOME/bin/hive 进入客户端，然后执行 HQL；</p><p> 2）$HIVE_HOME/bin/hive -e “hql”；</p><p> 3）$HIVE_HOME/bin/hive -f hive.sql；</p><p> 4）先开启 hivesever2 服务端，然后通过 JDBC 方式连接远程提交 HQL。 可 以 知 道 我 们 执 行 HQL 主 要 依 赖 于 $HIVE_HOME/bin/hive 和 $HIVE_HOME/bin/ hivesever2 两种脚本来实现提交 HQL，而在这两个脚本中，最终启动的 JAVA 程序的主类为 “org.apache.hadoop.hive.cli.CliDriver”，所以其实 Hive 程序的入口就是“CliDriver”这个类。</p><h2 id="第二章-源码阅读"><a href="#第二章-源码阅读" class="headerlink" title="第二章 源码阅读"></a>第二章 源码阅读</h2><h3 id="2-1-Hive源码阅读（入口程序）"><a href="#2-1-Hive源码阅读（入口程序）" class="headerlink" title="2.1 Hive源码阅读（入口程序）"></a>2.1 Hive源码阅读（入口程序）</h3><p>详细得事情大致得去了解一下就行</p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimpleDateFormat线程安全问题</title>
      <link href="/2021/08/25/SimpleDateFormat%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/"/>
      <url>/2021/08/25/SimpleDateFormat%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>superset搭建</title>
      <link href="/2021/08/24/superset%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/24/superset%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="superset搭建"><a href="#superset搭建" class="headerlink" title="superset搭建"></a>superset搭建</h1><h2 id="python3-6-环境搭建"><a href="#python3-6-环境搭建" class="headerlink" title="python3.6 环境搭建"></a>python3.6 环境搭建</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">yum</span> <span class="token attr-value">upgrade python-setuptools</span><span class="token attr-name">yum</span> <span class="token attr-value">install gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel</span><span class="token attr-name">pip</span> <span class="token attr-value">install cryptography</span><span class="token attr-name">yum</span> <span class="token attr-value">-y install wget</span><span class="token attr-name">wget</span> <span class="token attr-value">https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz</span><span class="token attr-name">tar</span> <span class="token attr-value">-zxvf Python-3.6.0.tgz</span><span class="token attr-name">cd</span> <span class="token attr-value">Python-3.6.0</span><span class="token attr-name">./configure</span> <span class="token attr-value">--prefix=/usr/local/python</span><span class="token attr-name">make</span> <span class="token attr-value">&amp;&amp; make install</span><span class="token comment" spellcheck="true"># 添加Python3的环境变量</span><span class="token attr-name">export</span> <span class="token attr-value">PYTHON_HOME=/usr/local/python</span><span class="token attr-name">export</span> <span class="token attr-value">PATH=$PATH:$PYTHON_HOME/bin</span><span class="token attr-name">source</span> <span class="token attr-value">/etc/profile</span>python3.6<span class="token attr-name">mv</span> <span class="token attr-value">/usr/bin/python /usr/bin/python-2.7.5</span><span class="token attr-name">ln</span> <span class="token attr-value">-s /usr/local/python/bin/python3.6 /usr/bin/python</span><span class="token attr-name">ln</span> <span class="token attr-value">-s /usr/local/python/bin/pip3 /usr/bin/pip</span><span class="token comment" spellcheck="true">#! /usr/bin/python-2.7.5</span><span class="token attr-name">vi</span> <span class="token attr-value">/usr/bin/yum</span><span class="token attr-name"> vi</span> <span class="token attr-value">/usr/libexec/urlgrabber-ext-down</span>  python</code></pre><h2 id="Superset安装"><a href="#Superset安装" class="headerlink" title="Superset安装"></a>Superset安装</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">bash</span> <span class="token attr-value">Miniconda3-latest-Linux-x86_64.sh</span><span class="token attr-name">source</span> <span class="token attr-value">~/.bashrc</span><span class="token attr-name">conda</span> <span class="token attr-value">config --set auto_activate_base false</span><span class="token attr-name">conda</span> <span class="token attr-value">config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><span class="token attr-name">conda</span> <span class="token attr-value">config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><span class="token attr-name">conda</span> <span class="token attr-value">config --set show_channel_urls yes</span><span class="token attr-name">conda</span> <span class="token attr-value">create --name superset python=3.6</span><span class="token attr-name">conda</span> <span class="token attr-value">activate superset</span><span class="token attr-name">conda</span> <span class="token attr-value">deactivate</span><span class="token attr-name">sudo</span> <span class="token attr-value">yum install -y python-setuptools</span><span class="token attr-name">sudo</span> <span class="token attr-value">yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel</span><span class="token attr-name">pip</span> <span class="token attr-value">install --upgrade setuptools pip -i https://pypi.douban.com/simple/</span><span class="token attr-name">pip</span> <span class="token attr-value">install apache-superset -i https://pypi.douban.com/simple/</span><span class="token attr-name">pip</span> <span class="token attr-value">install dataclasses</span><span class="token attr-name">superset</span> <span class="token attr-value">db upgrade</span><span class="token attr-name">export</span> <span class="token attr-value">FLASK_APP=superset</span><span class="token attr-name">flask</span> <span class="token attr-value">fab create-admin</span><span class="token attr-name">superset</span> <span class="token attr-value">init</span><span class="token attr-name">pip</span> <span class="token attr-value">install gunicorn -i https://pypi.douban.com/simple/</span><span class="token attr-name">conda</span> <span class="token attr-value">install mysqlclient</span><span class="token attr-name">gunicorn</span> <span class="token attr-value">--workers 5 --timeout 120 --bind 192.168.21.166:8787  "superset.app:create_app()" --daemon</span><span class="token attr-name">ps</span> <span class="token attr-value">-ef | awk '/superset/ &amp;&amp; !/awk/{print $2}' | xargs kill -9</span><span class="token attr-name">conda</span> <span class="token attr-value">deactivate</span><span class="token attr-name"> conda</span> <span class="token attr-value">activate superset</span></code></pre><h2 id="superset搭建mysql"><a href="#superset搭建mysql" class="headerlink" title="superset搭建mysql"></a>superset搭建mysql</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">mysql</span><span class="token punctuation">:</span><span class="token attr-value">//root:000000@192.168.21.166/superset</span><span class="token attr-name">pip</span> <span class="token attr-value">install dataclasses -i https://pypi.tuna.tsinghua.edu.cn/simple/</span><span class="token attr-name">root</span> <span class="token attr-value">root</span></code></pre><p>clickhouse://default:<a href="mailto:password@192.168.21.113">password@192.168.21.113</a>:8123/superset</p><p>这个重要的是，可以连接clickhouse和superset了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse单节点部署</title>
      <link href="/2021/08/24/Clickhouse%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/24/Clickhouse%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="Clickhouse环境搭建"><a href="#Clickhouse环境搭建" class="headerlink" title="Clickhouse环境搭建"></a>Clickhouse环境搭建</h1><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">sudo</span> <span class="token attr-value">rpm -ivh *.rpm</span><span class="token attr-name"> sudo</span> <span class="token attr-value">vim /etc/clickhouse-server/config.xml</span> <span class="token attr-name"> &lt;listen_host></span><span class="token punctuation">:</span><span class="token attr-value">:&lt;/listen_host></span> <span class="token attr-name"> sudo</span> <span class="token attr-value">systemctl start clickhouse-server</span><span class="token attr-name"> sudo</span> <span class="token attr-value">systemctl stop clickhouse-server</span> </code></pre><h2 id="分布式部署"><a href="#分布式部署" class="headerlink" title="分布式部署"></a>分布式部署</h2><p>vim /etc/clickhouse-server/config.d/metrika-shard.xml </p><p>clickhouse01</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>yandex</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>remote_servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>clickhouse</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 集群名称--></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第一个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第二个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第二个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>clickhouse</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>remote_servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>zookeeper-servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>3<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>zookeeper-servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>macros</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span>01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的分片数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span>rep_1_1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的副本数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>macros</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>yandex</span><span class="token punctuation">></span></span></code></pre><p>clickhouse02</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>yandex</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>remote_servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>clickhouse</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 集群名称--></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第一个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第二个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第二个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>clickhouse</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>remote_servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>zookeeper-servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>3<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>zookeeper-servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>macros</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span>01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的分片数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span>rep_1_2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的副本数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>macros</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>yandex</span><span class="token punctuation">></span></span></code></pre><p>clickhouse03</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>yandex</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>remote_servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>clickhouse</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 集群名称--></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第一个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第二个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--集群的第二个分片--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>internal_replication</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>internal_replication</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--该分片的第一个副本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>clickhouse</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>remote_servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>zookeeper-servers</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>node</span> <span class="token attr-name">index</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>3<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>host</span><span class="token punctuation">></span></span>clickhouse03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>host</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>port</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>port</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>node</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>zookeeper-servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>macros</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>shard</span><span class="token punctuation">></span></span>02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>shard</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的分片数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replica</span><span class="token punctuation">></span></span>rep_2_1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replica</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--不同机器放的副本数不一样--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>macros</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>yandex</span><span class="token punctuation">></span></span></code></pre><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>zookeeper</span> <span class="token attr-name">incl</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>zookeeper-servers<span class="token punctuation">"</span></span> <span class="token attr-name">optional</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>include_from</span><span class="token punctuation">></span></span>/etc/clickhouse-server/config.d/metrika-shard.xml<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>include_from</span><span class="token punctuation">></span></span></code></pre><p>这个是切片的名称</p><table><thead><tr><th>clickhouse01</th><th>clickhouse02</th><th>clickhouse03</th></tr></thead><tbody><tr><td><macros>  <shard>01</shard>    <replica>rep_1_1</replica>  </macros></td><td><macros>  <shard>01</shard>    <replica>rep_1_2</replica>  </macros></td><td><macros>  <shard>02</shard>    <replica>rep_2_1</replica>  </macros></td></tr></tbody></table><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><pre class=" language-sql"><code class="language-sql">在clickhouse01上面去创建<span class="token keyword">create</span> <span class="token keyword">table</span> st_order_mt <span class="token keyword">on</span> cluster clickhouse <span class="token punctuation">(</span> id UInt32<span class="token punctuation">,</span> sku_id String<span class="token punctuation">,</span> total_amount <span class="token keyword">Decimal</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> create_time <span class="token keyword">Datetime</span><span class="token punctuation">)</span> <span class="token keyword">engine</span><span class="token operator">=</span>ReplicatedMergeTree<span class="token punctuation">(</span><span class="token string">'/clickhouse/tables/{shard}/st_order_mt'</span><span class="token punctuation">,</span><span class="token string">'{replica}'</span><span class="token punctuation">)</span> <span class="token keyword">partition</span> <span class="token keyword">by</span> toYYYYMMDD<span class="token punctuation">(</span>create_time<span class="token punctuation">)</span> <span class="token keyword">primary</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>id<span class="token punctuation">)</span> <span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token punctuation">(</span>id<span class="token punctuation">,</span>sku_id<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token keyword">create</span> <span class="token keyword">table</span> st_order_mt_all2 <span class="token keyword">on</span> cluster clickhouse<span class="token punctuation">(</span> id UInt32<span class="token punctuation">,</span> sku_id String<span class="token punctuation">,</span> total_amount <span class="token keyword">Decimal</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> create_time <span class="token keyword">Datetime</span><span class="token punctuation">)</span><span class="token keyword">engine</span> <span class="token operator">=</span> <span class="token keyword">Distributed</span><span class="token punctuation">(</span>clickhouse<span class="token punctuation">,</span><span class="token keyword">default</span><span class="token punctuation">,</span> st_order_mt<span class="token punctuation">,</span>hiveHash<span class="token punctuation">(</span>sku_id<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> st_order_mt_all2 <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">201</span><span class="token punctuation">,</span><span class="token string">'sku_001'</span><span class="token punctuation">,</span><span class="token number">1000.00</span><span class="token punctuation">,</span><span class="token string">'2020-06-01 12:00:00'</span><span class="token punctuation">)</span> <span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">202</span><span class="token punctuation">,</span><span class="token string">'sku_002'</span><span class="token punctuation">,</span><span class="token number">2000.00</span><span class="token punctuation">,</span><span class="token string">'2020-06-01 12:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">203</span><span class="token punctuation">,</span><span class="token string">'sku_004'</span><span class="token punctuation">,</span><span class="token number">2500.00</span><span class="token punctuation">,</span><span class="token string">'2020-06-01 12:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">204</span><span class="token punctuation">,</span><span class="token string">'sku_002'</span><span class="token punctuation">,</span><span class="token number">2000.00</span><span class="token punctuation">,</span><span class="token string">'2020-06-01 12:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">205</span><span class="token punctuation">,</span><span class="token string">'sku_003'</span><span class="token punctuation">,</span><span class="token number">600.00</span><span class="token punctuation">,</span><span class="token string">'2020-06-02 12:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>在clickhouse01去查询<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> st_order_mt_all2<span class="token punctuation">;</span>在clickhouse01<span class="token punctuation">,</span><span class="token number">02</span><span class="token punctuation">,</span><span class="token number">03</span>上面去查询<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> st_order_mt<span class="token punctuation">;</span></code></pre><p>在三台节点中去执行</p><p>重启: sudo systemctl restart clickhouse-server</p><p>启动: sudo systemctl start clickhouse-serve</p><p>关闭: sudo systemctl stop clickhouse-serve</p><p>启动客户端：clickhouse-client -m</p><p>可以使用副本机制和分片机制，在clickhouse中</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkCDC在sqlClient的使用</title>
      <link href="/2021/08/24/FlinkCDC%E5%9C%A8sqlClient%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/08/24/FlinkCDC%E5%9C%A8sqlClient%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="FlinkCDC在sqlClient的使用"><a href="#FlinkCDC在sqlClient的使用" class="headerlink" title="FlinkCDC在sqlClient的使用"></a>FlinkCDC在sqlClient的使用</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>其实在此之前，我也发表一篇关于Flink CDC的使用但是那个时候不是很懂的，很多东西都是懵懵懂懂的，但是今天我又重新写一篇关于自己做的一些的心得吧。</p><h3 id="1、首先环境准备"><a href="#1、首先环境准备" class="headerlink" title="1、首先环境准备"></a>1、首先环境准备</h3><ul><li>搭建Flink1.13.2的集群环境，启动sql-client的时候得启动这个</li><li>部署MySQL5.7单节点就行</li></ul><h3 id="2、配置项"><a href="#2、配置项" class="headerlink" title="2、配置项"></a>2、配置项</h3><h4 id="2-1-MySQL的配置项"><a href="#2-1-MySQL的配置项" class="headerlink" title="2.1 MySQL的配置项"></a>2.1 MySQL的配置项</h4><pre class=" language-xml"><code class="language-xml">server-id=1log-bin=mysql-binbinlog_format=rowbinlog-do-db=flinkInfo</code></pre><p>重启MySQL服务：systemctl restart mysqld</p><h4 id="2-2-Flink-lib目录下面的依赖需要准备的"><a href="#2-2-Flink-lib目录下面的依赖需要准备的" class="headerlink" title="2.2 Flink lib目录下面的依赖需要准备的"></a>2.2 Flink lib目录下面的依赖需要准备的</h4><pre class=" language-xml"><code class="language-xml">flink-connector-kafka_2.11-1.13.2.jarflink-format-changelog-json-2.0.0.jarflink-json-1.13.2.jarflink-sql-connector-mysql-cdc-2.0.0.jarflink-table-blink_2.11-1.13.2.jar</code></pre><p>这些核心的准备好，有些依赖是集群里面就自带的，但是我们分发给一个节点的时候，一定要分发给每一个节点，然后重启flink集群和flink sql client</p><h2 id="MySQL建表"><a href="#MySQL建表" class="headerlink" title="MySQL建表"></a>MySQL建表</h2><pre class=" language-sql"><code class="language-sql"><span class="token operator">-</span> MySQL<span class="token comment" spellcheck="true">/*Table structure for table `order_info` */</span><span class="token keyword">DROP</span> <span class="token keyword">TABLE</span> <span class="token keyword">IF</span> <span class="token keyword">EXISTS</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token punctuation">;</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span> <span class="token punctuation">(</span>  <span class="token punctuation">`</span>id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span> <span class="token keyword">COMMENT</span> <span class="token string">'编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>consignee<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'收货人'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>consignee_tel<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'收件人电话'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>total_amount<span class="token punctuation">`</span> <span class="token keyword">decimal</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'总金额'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_status<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单状态,1表示下单，2表示支付'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>user_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'用户id'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>payment_way<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'付款方式'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>delivery_address<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'送货地址'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_comment<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单备注'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>out_trade_no<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单交易编号（第三方支付用)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>trade_body<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单描述(第三方支付用)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>create_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>operate_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'操作时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>expire_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'失效时间'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>tracking_no<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'物流单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>parent_order_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'父订单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>img_url<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'图片路径'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>province_id<span class="token punctuation">`</span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'地区'</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token punctuation">`</span>id<span class="token punctuation">`</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token operator">=</span><span class="token number">1</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span><span class="token operator">=</span>utf8 <span class="token keyword">COMMENT</span><span class="token operator">=</span><span class="token string">'订单表'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token comment" spellcheck="true">-- Records of order_info</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">476</span><span class="token punctuation">,</span> <span class="token string">'lAXjcL'</span><span class="token punctuation">,</span> <span class="token string">'13408115089'</span><span class="token punctuation">,</span> <span class="token number">433.00</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token string">'OYyAdSdLxedceqovndCD'</span><span class="token punctuation">,</span> <span class="token string">'ihjAYsSjrgJMQVdFQnSy'</span><span class="token punctuation">,</span> <span class="token string">'8728720206'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 02:21:38'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">477</span><span class="token punctuation">,</span> <span class="token string">'QLiFDb'</span><span class="token punctuation">,</span> <span class="token string">'13415139984'</span><span class="token punctuation">,</span> <span class="token number">772.00</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token string">'OizYrQbKuWvrvdfpkeSZ'</span><span class="token punctuation">,</span> <span class="token string">'wiBhhqhMndCCgXwmWVQq'</span><span class="token punctuation">,</span> <span class="token string">'1679381473'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 09:12:25'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_info<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">478</span><span class="token punctuation">,</span> <span class="token string">'iwKjQD'</span><span class="token punctuation">,</span> <span class="token string">'13320383859'</span><span class="token punctuation">,</span> <span class="token number">88.00</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token number">107</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token string">'cbXLKtNHWOcWzJVBWdAs'</span><span class="token punctuation">,</span> <span class="token string">'njjsnknHxsxhuCCeNDDi'</span><span class="token punctuation">,</span> <span class="token string">'0937074290'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'2020-06-18 15:56:34'</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*Table structure for table `order_detail` */</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token punctuation">(</span>  <span class="token punctuation">`</span>id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span> <span class="token keyword">COMMENT</span> <span class="token string">'编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'订单编号'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_id<span class="token punctuation">`</span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'sku_id'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_name<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'sku名称（冗余)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>img_url<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'图片名称（冗余)'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>order_price<span class="token punctuation">`</span> <span class="token keyword">decimal</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'购买价格(下单时sku价格）'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>sku_num<span class="token punctuation">`</span> <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'购买个数'</span><span class="token punctuation">,</span>  <span class="token punctuation">`</span>create_time<span class="token punctuation">`</span> <span class="token keyword">datetime</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'创建时间'</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token punctuation">`</span>id<span class="token punctuation">`</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token operator">=</span><span class="token number">1</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span><span class="token operator">=</span>utf8 <span class="token keyword">COMMENT</span><span class="token operator">=</span><span class="token string">'订单明细表'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token comment" spellcheck="true">-- Records of order_detail</span><span class="token comment" spellcheck="true">-- ----------------------------</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1329</span><span class="token punctuation">,</span> <span class="token number">476</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//XLMByOyZDTJQYxphQHNTgYAFzJJCKTmCbzvEJIpz', 8900.00, '3', '2020-06-18 02:21:38');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1330</span><span class="token punctuation">,</span> <span class="token number">477</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token string">'荣耀10 GT游戏加速 AIS手持夜景 6GB+64GB 幻影蓝全网通 移动联通电信'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//ixOCtlYmlxEEgUfPLiLdjMftzrleOEIBKSjrhMne', 2452.00, '4', '2020-06-18 09:12:25');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span><span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1331</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'小米Play 流光渐变AI双摄 4GB+64GB 梦幻蓝 全网通4G 双卡双待 小水滴全面屏拍照游戏智能手机'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//RqfEFnAOqnqRnNZLFRvBuwXxwNBtptYJCILDKQYv', 1442.00, '1', '2020-06-18 15:56:34');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1332</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//IwhuCDlsiLenfKjPzbJrIoxswdfofKhJLMzlJAKV', 8900.00, '3', '2020-06-18 15:56:34');</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token punctuation">`</span>order_detail<span class="token punctuation">`</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token number">1333</span><span class="token punctuation">,</span> <span class="token number">478</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span><span class="token punctuation">,</span> 'http:<span class="token comment" spellcheck="true">//bbfwTbAzTWapywODzOtDJMJUEqNTeRTUQuCDkqXP', 8900.00, '1', '2020-06-18 15:56:34');</span></code></pre><h2 id="Flink-SQL建表"><a href="#Flink-SQL建表" class="headerlink" title="Flink SQL建表"></a>Flink SQL建表</h2><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 创建订单信息表</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> order_info<span class="token punctuation">(</span>    id <span class="token keyword">BIGINT</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span>    user_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    create_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    operate_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    province_id <span class="token keyword">INT</span><span class="token punctuation">,</span>    order_status STRING<span class="token punctuation">,</span>    total_amount <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>    <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'mysql-cdc'</span><span class="token punctuation">,</span>    <span class="token string">'hostname'</span> <span class="token operator">=</span> <span class="token string">'flink04'</span><span class="token punctuation">,</span>    <span class="token string">'port'</span> <span class="token operator">=</span> <span class="token string">'3306'</span><span class="token punctuation">,</span>    <span class="token string">'username'</span> <span class="token operator">=</span> <span class="token string">'root'</span><span class="token punctuation">,</span>    <span class="token string">'password'</span> <span class="token operator">=</span> <span class="token string">'000000'</span><span class="token punctuation">,</span>    <span class="token string">'database-name'</span> <span class="token operator">=</span> <span class="token string">'flinkInfo'</span><span class="token punctuation">,</span>    <span class="token string">'table-name'</span> <span class="token operator">=</span> <span class="token string">'order_info'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 创建订单详情表</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> order_detail<span class="token punctuation">(</span>    id <span class="token keyword">BIGINT</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span>    order_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    sku_id <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    sku_name STRING<span class="token punctuation">,</span>    sku_num <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>    order_price <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> create_time <span class="token keyword">TIMESTAMP</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>    <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'mysql-cdc'</span><span class="token punctuation">,</span>    <span class="token string">'hostname'</span> <span class="token operator">=</span> <span class="token string">'flink04'</span><span class="token punctuation">,</span>    <span class="token string">'port'</span> <span class="token operator">=</span> <span class="token string">'3306'</span><span class="token punctuation">,</span>    <span class="token string">'username'</span> <span class="token operator">=</span> <span class="token string">'root'</span><span class="token punctuation">,</span>    <span class="token string">'password'</span> <span class="token operator">=</span> <span class="token string">'000000'</span><span class="token punctuation">,</span>    <span class="token string">'database-name'</span> <span class="token operator">=</span> <span class="token string">'flinkInfo'</span><span class="token punctuation">,</span>    <span class="token string">'table-name'</span> <span class="token operator">=</span> <span class="token string">'order_detail'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h2 id="计算指标"><a href="#计算指标" class="headerlink" title="计算指标"></a>计算指标</h2><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span>    od<span class="token punctuation">.</span>id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>id order_id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>user_id<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>province_id<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_id<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_name<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>sku_num<span class="token punctuation">,</span>    od<span class="token punctuation">.</span>order_price<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>create_time<span class="token punctuation">,</span>    oi<span class="token punctuation">.</span>operate_time<span class="token keyword">FROM</span>   <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span> <span class="token operator">*</span>     <span class="token keyword">FROM</span> order_info    <span class="token keyword">WHERE</span>       order_status <span class="token operator">=</span> <span class="token string">'2'</span>   <span class="token punctuation">)</span> oi   <span class="token keyword">JOIN</span>  <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span> <span class="token operator">*</span>    <span class="token keyword">FROM</span> order_detail  <span class="token punctuation">)</span> od   <span class="token keyword">ON</span> oi<span class="token punctuation">.</span>id <span class="token operator">=</span> od<span class="token punctuation">.</span>order_id<span class="token punctuation">;</span></code></pre><p>最终的计算的结果和MySQL中计算的结果是一样的，说明这个实时导入是没有错的，但是这样的我也是可以在这个session中测试一下，这个几个表是否能进行一个动态的变化，测试结果如下</p><h2 id="测试订单信息和订单详情表是否能够动态的监听"><a href="#测试订单信息和订单详情表是否能够动态的监听" class="headerlink" title="测试订单信息和订单详情表是否能够动态的监听"></a>测试订单信息和订单详情表是否能够动态的监听</h2><h3 id="1、订单信息的原本数据"><a href="#1、订单信息的原本数据" class="headerlink" title="1、订单信息的原本数据"></a>1、订单信息的原本数据</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824173439075.png" alt="原始信息"></p><h3 id="2、修改MySQL中的数据为"><a href="#2、修改MySQL中的数据为" class="headerlink" title="2、修改MySQL中的数据为"></a>2、修改MySQL中的数据为</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824173545868.png" alt="修改数据"></p><p>通过观察，修改的数据在当前的这一次查询当中是不能够发现的，但是在下一次查询的过程当中是可以进行一个捕捉到的</p>]]></content>
      
      
      <categories>
          
          <category> 大数据采集系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据采集系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduce-the-upsert-kafka-Connector</title>
      <link href="/2021/08/24/Introduce-the-upsert-kafka-Connector/"/>
      <url>/2021/08/24/Introduce-the-upsert-kafka-Connector/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduce-the-upsert-kafka-Connector"><a href="#Introduce-the-upsert-kafka-Connector" class="headerlink" title="Introduce-the-upsert-kafka-Connector"></a>Introduce-the-upsert-kafka-Connector</h1><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-149%3A+Introduce+the+upsert-kafka+Connector" target="_blank" rel="noopener">参考文档</a></p><h2 id="一、动机"><a href="#一、动机" class="headerlink" title="一、动机"></a>一、动机</h2><p>通过邮件列表和社区问题，许多用户已经表达了他们对 upsert Kafka 的需求。在查看邮件列表后，我们认为背后有 3 个原因：</p><ul><li>将压缩的 kafka 主题解释为更改日志流，该流将带有键的记录解释为 upsert 事件 [1-3]； </li><li>作为实时管道的一部分，加入多个流以进行丰富并将结果存储到 Kafka 主题中以供以后进一步计算。但是，结果可能包含更新事件。</li><li>作为实时管道的一部分，聚合数据流并将结果存储到 Kafka 主题中以供以后进一步计算。但是，结果可能包含更新事件。</li></ul><h2 id="二、公共接口"><a href="#二、公共接口" class="headerlink" title="二、公共接口"></a>二、公共接口</h2><h3 id="1、Upsert-kafka-连接器定义"><a href="#1、Upsert-kafka-连接器定义" class="headerlink" title="1、Upsert-kafka 连接器定义"></a>1、Upsert-kafka 连接器定义</h3><p>Upsert-kafka 连接器产生一个变更日志流，其中每个数据记录代表一个更新或删除事件。更准确地说，数据记录中的值被解释为同一记录键的最后一个值的“更新”，如果有的话（如果相应的键不存在，则更新将被视为插入）。使用表类比，更改日志流中的数据记录被解释为 UPSERT 又名 INSERT/UPDATE，因为任何具有相同键的现有行都将被覆盖。此外，空值以特殊方式解释：具有空值的记录表示“删除”。</p><h3 id="2、upsert-kafka-上的主键约束"><a href="#2、upsert-kafka-上的主键约束" class="headerlink" title="2、upsert-kafka 上的主键约束"></a>2、upsert-kafka 上的主键约束</h3><p>当 upsert-kafka 连接器用作接收器时，它的工作方式类似于现有的 HBase 接收器。Upsert-kafka sink 不需要 planner 发送 UPDATE_BEFORE 消息（在某些情况下，planner 可能仍然会发送 UPDATE_BEFORE 消息），并且会将 INSERT/UPDATE_AFTER 消息写为带有关键部分的普通 Kafka 记录，并将 DELETE 消息写为带有 null 的 Kafka 记录值（指示键的墓碑）。Flink 将通过主键列的值对数据进行分区来保证主键上的消息排序。</p><p>Upsert-kafka 源是一种变更日志源。变更日志源上的主键语义意味着物化变更日志 (INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE) 在主键约束上是唯一的。Flink 假设所有消息在主键上都是有序的。</p><p>在 Flink 中创建 upsert-kafka 表需要在表上声明主键。主键定义还控制哪些字段应该在 Kafka 的键中结束。因此，我们不需要 upsert-kafka 连接器中的 ‘key.fields’ 选项。默认情况下，主键字段也将存储在 Kafka 的值中。但是用户仍然可以使用选项“value.fields-include”来控制这种行为。</p><h3 id="3、Upsert-kafka-源"><a href="#3、Upsert-kafka-源" class="headerlink" title="3、Upsert-kafka 源"></a>3、Upsert-kafka 源</h3><p>一般来说，upsert-kafka源码的底层topic必须是<a href="https://kafka.apache.org/documentation/#compaction" target="_blank" rel="noopener">compacted</a>。另外，底层topic必须在同一个partition中包含所有key相同的数据，否则结果会出错。</p><p>目前，upsert-kafka 连接器不提供开始读取位置选项。必须从最早的偏移量读取 upsert-kafka 连接器。这是对数据完整性的保护，否则很难解释当用户指定从中间位置开始偏移时的行为以及如何处理从未见过密钥的删除事件。因此，更新插入-kafka连接器不提供类似的选项<code>'``scan.startup.mode'</code>， “和” （仅用于“ ”启动模式）。<code>scan.startup.specific-offsets'``'scan.startup.timestamp-millis'``properties.group.id'``group-offsets</code></p><h3 id="4、Upsert-kafka-接收器"><a href="#4、Upsert-kafka-接收器" class="headerlink" title="4、Upsert-kafka 接收器"></a>4、Upsert-kafka 接收器</h3><p>为了保证消息排序，upsert-kafka sink 将始终在主键字段上以 HASH 分区器模式工作。因此，我们不需要<code>sink.partitioner</code>upsert-kafka 连接器中的 ‘ ‘ 选项。 </p><p>5、Upsert-kafka 连接器选项</p><p>upsert-kafka Connector 中的选项很像 Kafka Connector。</p><table><thead><tr><th>connector</th><th>required</th><th>(none)</th><th>String</th><th>Specify what connector to use, here should be ‘upsert-kafka’.</th></tr></thead><tbody><tr><td>properties.bootstrap.servers</td><td>required</td><td>(none)</td><td>String</td><td>Comma separated list of Kafka brokers.</td></tr><tr><td>key.format</td><td>required</td><td>(none)</td><td>String</td><td>The format used to deserialize and serialize the key of Kafka record. Only insert-only format is supported.</td></tr><tr><td>value.format</td><td>required</td><td>(none)</td><td>String</td><td>The format used to deserialize and serialize the value of Kafka records.</td></tr><tr><td>topic</td><td>optional</td><td>(none)</td><td>String</td><td>Topic name(s) to read data from when the table is used as source or to write data to when the table is used as sink. Note, only one of “<code>topic-pattern</code>“ and “<code>topic</code>“ can be specified for sources. When the table is used as sink, the topic name is the topic to write data to. Note topic list is not supported for sinks.</td></tr><tr><td>topic-pattern</td><td>optional</td><td>(none)</td><td>String</td><td>The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of “<code>topic-pattern</code>“ and “<code>topic</code>“ can be specified for sources.</td></tr><tr><td>value.fields-include</td><td>optional</td><td>‘ALL’</td><td>String</td><td>Controls which fields should end up in the value as well, possible values -  ALL (all fields of the schema, even if they are part of e.g. the key)-  EXCEPT_KEY (all fields of the schema - fields of the key)</td></tr></tbody></table><p><strong>注意：</strong>仅支持将仅插入格式用作<code>'key.format'</code>和 ‘ <code>value.format</code>‘。我们将使用<code>ChangelogMode</code>格式的 来区分格式是否为仅插入。</p><h2 id="三、背景介绍"><a href="#三、背景介绍" class="headerlink" title="三、背景介绍"></a>三、背景介绍</h2><p>在某些场景中，比如GROUP BY聚合之后的结果，需要去更新之前的结果值。这个时候，需要将 Kafka 消息记录的 key 当成主键处理，用来确定一条数据是应该作为插入、删除还是更新记录来处理。在Flink1.11中，可以通过 <strong>flink-cdc-connectors</strong> 项目提供的 <strong>changelog-json format</strong>来实现该功能。关于该功能的使用，见之前的分享<a href="https://mp.weixin.qq.com/s?__biz=MzU2ODQ3NjYyMA==&amp;mid=2247484891&amp;idx=2&amp;sn=5fcf4ec7c4519cf69528d66caccbdd95&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Flink1.11中的CDC Connectors操作实践</a>。</p><p>在Flink1.12版本中， 新增了一个 **upsert connector(upsert-kafka)**，该 connector 扩展自现有的 Kafka connector，工作在 upsert 模式（FLIP-149）下。新的 upsert-kafka connector 既可以作为 source 使用，也可以作为 sink 使用，并且提供了与现有的 kafka connector 相同的基本功能和持久性保证，因为两者之间复用了大部分代码。本文将以Flink1.13.2为例，介绍该功能的基本使用步骤，以下是全文，希望对你有所帮助。</p><h3 id="1-Upsert-Kafka-connector简介"><a href="#1-Upsert-Kafka-connector简介" class="headerlink" title="1 Upsert Kafka connector简介"></a>1 Upsert Kafka connector简介</h3><p><strong>Upsert Kafka Connector</strong>允许用户以upsert的方式从Kafka主题读取数据或将数据写入Kafka主题。</p><p><strong>当作为数据源时</strong>，upsert-kafka Connector会生产一个changelog流，其中每条数据记录都表示一个更新或删除事件。更准确地说，如果不存在对应的key，则视为<strong>INSERT</strong>操作。如果已经存在了相对应的key，则该key对应的value值为最后一次更新的值。</p><p>用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。</p><p><strong>当作为数据汇时</strong>，upsert-kafka Connector会消费一个changelog流。它将<strong>INSERT / UPDATE_AFTER</strong>数据作为正常的Kafka消息值写入(即INSERT和UPDATE操作，都会进行正常写入，如果是更新，则同一个key会存储多条数据，但在读取该表数据时，只保留最后一次更新的值)，并将 DELETE 数据以 value 为空的 Kafka 消息写入（key被打上墓碑标记，表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中</p><h3 id="2-依赖"><a href="#2-依赖" class="headerlink" title="2 依赖"></a>2 依赖</h3><p>为了使用Upsert Kafka连接器，需要添加下面的依赖</p><pre class=" language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-kafka_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.13.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><h3 id="3-使用方式"><a href="#3-使用方式" class="headerlink" title="3 使用方式"></a>3 使用方式</h3><p><strong>使用样例</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 创建一张kafka表，用户存储sink的数据</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> pageviews_per_region <span class="token punctuation">(</span>  user_region STRING<span class="token punctuation">,</span>  pv <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>  uv <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>user_region<span class="token punctuation">)</span> <span class="token operator">NOT</span> ENFORCED<span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>  <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'upsert-kafka'</span><span class="token punctuation">,</span>  <span class="token string">'topic'</span> <span class="token operator">=</span> <span class="token string">'pageviews_per_region'</span><span class="token punctuation">,</span>  <span class="token string">'properties.bootstrap.servers'</span> <span class="token operator">=</span> <span class="token string">'flink04:9092,flink05:9092,flink06:9092'</span><span class="token punctuation">,</span>  <span class="token string">'key.format'</span> <span class="token operator">=</span> <span class="token string">'avro'</span><span class="token punctuation">,</span>  <span class="token string">'value.format'</span> <span class="token operator">=</span> <span class="token string">'avro'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>创建表之后出现问题</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824161258282.png" alt="出现问题"></p><p>尖叫提示：</p><p>要使用 upsert-kafka connector，必须在创建表时使用<strong>PRIMARY KEY</strong>定义主键，并为键（key.format）和值（value.format）指定序列化反序列化格式。</p><p><strong>upsert-kafka connector参数</strong></p><ul><li><strong>connector</strong></li></ul><p><strong>必选</strong>。指定要使用的连接器，Upsert Kafka 连接器使用：<code>'upsert-kafka'</code>。</p><ul><li><strong>topic</strong></li></ul><p><strong>必选</strong>。用于读取和写入的 Kafka topic 名称。</p><ul><li><strong>properties.bootstrap.servers</strong></li></ul><p><strong>必选</strong>。以逗号分隔的 Kafka brokers 列表。</p><ul><li><strong>key.format</strong></li></ul><p><strong>必选</strong>。用于对 Kafka 消息中 key 部分序列化和反序列化的格式。key 字段由<strong>PRIMARY KEY</strong> 语法指定。支持的格式包括 <code>'csv'</code>、<code>'json'</code>、<code>'avro'</code>。</p><ul><li><strong>value.format</strong></li></ul><p><strong>必选</strong>。用于对 Kafka 消息中 value 部分序列化和反序列化的格式。支持的格式包括 <code>'csv'</code>、<code>'json'</code>、<code>'avro'</code>。</p><ul><li><strong>properties.</strong>*</li></ul><p><strong>可选</strong>。该选项可以传递任意的 Kafka 参数。选项的后缀名必须匹配定义在 Kafka 参数文档中的参数名。Flink 会自动移除 选项名中的 “properties.” 前缀，并将转换后的键名以及值传入 KafkaClient。例如，你可以通过 </p><p><code>'properties.allow.auto.create.topics' = 'false'</code> 来禁止自动创建 topic。但是，某些选项，例如<code>'key.deserializer'</code> 和 <code>'value.deserializer'</code> 是不允许通过该方式传递参数，因为 Flink 会重写这些参数的值。</p><ul><li><strong>value.fields-include</strong></li></ul><p><strong>可选</strong>，默认为<strong>ALL</strong>。控制key字段是否出现在 value 中。当取<strong>ALL</strong>时，表示<code>消息的 value 部分将包含 schema 中所有的字段，包括定义为主键的字段。</code>当取<strong>EXCEPT_KEY</strong>时，表示记录的 value 部分包含 schema 的所有字段，定义为主键的字段除外。</p><ul><li><strong>key.fields-prefix</strong></li></ul><p><strong>可选</strong>。为了避免与value字段命名冲突，为key字段添加一个自定义前缀。默认前缀为空。一旦指定了key字段的前缀，必须在DDL中指明前缀的名称，但是在构建key的序列化数据类型时，将移除该前缀。见下面的示例。在需要注意的是：使用该配置属性，<strong>value.fields-include</strong>的值必须为<strong>EXCEPT_KEY</strong>。</p><p>尖叫提示：</p><p>如果指定了key字段前缀，但在DDL中并没有添加该前缀字符串，那么在向该表写入数时，会抛出下面异常：</p><p>[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.api.ValidationException: All fields in ‘key.fields’ must be prefixed with ‘qwe’ when option ‘key.fields-prefix’ is set but field ‘do_date’ is not prefixed.</p><pre class=" language-sql"><code class="language-sql"> <span class="token comment" spellcheck="true">-- 创建一张upsert表，当指定了qwe前缀，涉及的key必须指定qwe前缀</span> <span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> result_total_pvuv_min_prefix <span class="token punctuation">(</span>     qwedo_date     STRING<span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 统计日期，必须包含qwe前缀</span>     qwedo_min      STRING<span class="token punctuation">,</span>      <span class="token comment" spellcheck="true">-- 统计分钟，必须包含qwe前缀</span>     pv          <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 点击量</span>     uv          <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true">-- 一天内同个访客多次访问仅计算一个UV</span>     currenttime <span class="token keyword">TIMESTAMP</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true">-- 当前时间</span>     <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>qwedo_date<span class="token punctuation">,</span> qwedo_min<span class="token punctuation">)</span> <span class="token operator">NOT</span> ENFORCED <span class="token comment" spellcheck="true">-- 必须包含qwe前缀</span> <span class="token punctuation">)</span> <span class="token keyword">WITH</span> <span class="token punctuation">(</span>   <span class="token string">'connector'</span> <span class="token operator">=</span> <span class="token string">'upsert-kafka'</span><span class="token punctuation">,</span>   <span class="token string">'topic'</span> <span class="token operator">=</span> <span class="token string">'result_total_pvuv_min_prefix'</span><span class="token punctuation">,</span>   <span class="token string">'properties.bootstrap.servers'</span> <span class="token operator">=</span> <span class="token string">'kms-2:9092,kms-3:9092,kms-4:9092'</span><span class="token punctuation">,</span>   <span class="token string">'key.json.ignore-parse-errors'</span> <span class="token operator">=</span> <span class="token string">'true'</span><span class="token punctuation">,</span>   <span class="token string">'value.json.fail-on-missing-field'</span> <span class="token operator">=</span> <span class="token string">'false'</span><span class="token punctuation">,</span>   <span class="token string">'key.format'</span> <span class="token operator">=</span> <span class="token string">'json'</span><span class="token punctuation">,</span>   <span class="token string">'value.format'</span> <span class="token operator">=</span> <span class="token string">'json'</span><span class="token punctuation">,</span>   <span class="token string">'key.fields-prefix'</span><span class="token operator">=</span><span class="token string">'qwe'</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">-- 指定前缀qwe</span>   <span class="token string">'value.fields-include'</span> <span class="token operator">=</span> <span class="token string">'EXCEPT_KEY'</span> <span class="token comment" spellcheck="true">-- key不出现kafka消息的value中</span> <span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 向该表中写入数据</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> result_total_pvuv_min_prefix <span class="token keyword">SELECT</span>   do_date<span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--  时间分区</span>   cast<span class="token punctuation">(</span>DATE_FORMAT <span class="token punctuation">(</span>access_time<span class="token punctuation">,</span><span class="token string">'HH:mm'</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> STRING<span class="token punctuation">)</span> <span class="token keyword">AS</span> do_min<span class="token punctuation">,</span><span class="token comment" spellcheck="true">-- 分钟级别的时间</span>   pv<span class="token punctuation">,</span>   uv<span class="token punctuation">,</span>   <span class="token keyword">CURRENT_TIMESTAMP</span> <span class="token keyword">AS</span> currenttime <span class="token comment" spellcheck="true">-- 当前时间</span> <span class="token keyword">from</span>   view_total_pvuv_min<span class="token punctuation">;</span></code></pre><ul><li><strong>sink.parallelism</strong></li></ul><p><strong>可选</strong>。定义 upsert-kafka sink 算子的并行度。默认情况下，由框架确定并行度，与上游链接算子的并行度保持一致。</p><h3 id="4-其他注意事项"><a href="#4-其他注意事项" class="headerlink" title="4 其他注意事项"></a>4 其他注意事项</h3><p><strong>Key和Value的序列化格式</strong></p><p>关于Key、value的序列化可以参考Kafka connector。值得注意的是，必须指定Key和Value的序列化格式，其中Key是通过<strong>PRIMARY KEY</strong>指定的。</p><p><strong>Primary Key约束</strong></p><p><strong>Upsert Kafka</strong> 工作在 upsert 模式（FLIP-149）下。当我们创建表时，需要在 DDL 中定义主键。具有相同key的数据，会存在相同的分区中。在 changlog source 上定义主键意味着在物化后的 changelog 上主键具有唯一性。定义的主键将决定哪些字段出现在 Kafka 消息的 key 中。</p><p><strong>一致性保障</strong></p><p>默认情况下，如果启用 checkpoint，Upsert Kafka sink 会保证至少一次将数据插入 Kafka topic。</p><p>这意味着，Flink 可以将具有相同 key 的重复记录写入 Kafka topic。但由于该连接器以 upsert 的模式工作，该连接器作为 source 读入时，可以确保具有相同主键值下仅最后一条消息会生效。因此，upsert-kafka 连接器可以像 HBase sink 一样实现幂等写入。</p><p><strong>分区水位线</strong></p><p>Flink 支持根据 Upsert Kafka 的 每个分区的数据特性发送相应的 watermark。当使用这个特性的时候，watermark 是在 Kafka consumer 内部生成的。合并每个分区生成的 watermark 的方式和 streaming shuffle 的方式是一致的(<strong>单个分区的输入取最大值，多个分区的输入取最小值</strong>)。数据源产生的 watermark 是取决于该 consumer 负责的所有分区中当前最小的 watermark。如果该 consumer 负责的部分分区是空闲的，那么整体的 watermark 并不会前进。在这种情况下，可以通过设置合适的 table.exec.source.idle-timeout 来缓解这个问题。</p><p><strong>数据类型</strong></p><p>Upsert Kafka 用字节bytes存储消息的 key 和 value，因此没有 schema 或数据类型。消息按格式进行序列化和反序列化，例如：csv、json、avro。不同的序列化格式所提供的数据类型有所不同，因此需要根据使用的序列化格式进行确定表字段的数据类型是否与该序列化类型提供的数据类型兼容。</p><h2 id="四、实施细则"><a href="#四、实施细则" class="headerlink" title="四、实施细则"></a>四、实施细则</h2><p>由于 upsert-kafka 连接器只产生不包含 UPDATE_BEFORE 消息的 upsert 流。但是，一些操作需要 UPDATE_BEFORE 消息才能正确处理，例如聚合。因此，我们需要一个物理节点来实现 upsert 流并生成带有完整更改消息的更改日志流。在物理运算符中，我们将使用状态来知道密钥是否是第一次被看到。该运算符将生成 INSERT 行，或额外为前一个图像生成 UPDATE_BEFORE 行，或生成所有列都填充有值的 DELETE 行。</p><p>规划者怎么知道要添加这样的物化算子呢？这取决于 FLIP-95 中引入的源的 ChangeMode。目前，我们仅支持仅插入或所有类型（例如 CDC 格式）的 ChangelogMode。在此 FLIP 中，我们将支持 [UPDATE_AFTER, DELETE] ChangelogMode，这表明源将在运行时仅发出 UPDATE_AFTER 和 DELETE 消息。当source的ChangelogMode为[UPDATE_AFTER, DELETE]时，planner会添加上面的物化算子。 </p><p>考虑到 Kafka 连接器和 upsert-kafka 连接器之间的相似性，我们应该重用引擎盖下的大部分代码，只引入不同的连接器工厂。</p><h2 id="五、兼容性、弃用和迁移计划"><a href="#五、兼容性、弃用和迁移计划" class="headerlink" title="五、兼容性、弃用和迁移计划"></a>五、兼容性、弃用和迁移计划</h2><p>此更改引入了一项新功能，但并不意味着任何兼容性问题。</p><h2 id="六、被拒绝的替代品"><a href="#六、被拒绝的替代品" class="headerlink" title="六、被拒绝的替代品"></a>六、被拒绝的替代品</h2><h3 id="1、在-Kafka-连接器中引入新属性-vs-引入-upsert-kafka-连接器"><a href="#1、在-Kafka-连接器中引入新属性-vs-引入-upsert-kafka-连接器" class="headerlink" title="1、在 Kafka 连接器中引入新属性 vs 引入 upsert-kafka 连接器"></a>1、在 Kafka 连接器中引入新属性 vs 引入 upsert-kafka 连接器</h3><ol><li>很难解释当用户指定从中间位置开始偏移时的行为（例如如何处理不存在的删除事件）。如果用户这样做是很危险的。所以我们目前没有在新的连接器中提供偏移选项。</li><li>这是对同一个 kafka 主题的不同视角/抽象（附加与更新插入）。如果我们可以将它们分开而不是将它们混合在一个连接器中会更容易理解。新连接器需要散列接收器分区器、主键声明、常规格式。如果我们将它们混合在一个连接器中，则可能会混淆如何正确使用这些选项。</li><li>upsert-kafka 连接器的语义与 Kafka Stream 中的 KTable 相同。所以对于Kafka Stream和KSQL用户来说非常方便。我们在邮件列表中看到了几个问题 [1][2]，询问如何对 KTable 进行建模以及如何在 Flink SQL 中加入 KTable。</li></ol><h3 id="2、使用-upsert-kafka-作为新的连接器名称-vs-使用-kafka-compacted-作为名称-vs-使用-ktable-作为名称"><a href="#2、使用-upsert-kafka-作为新的连接器名称-vs-使用-kafka-compacted-作为名称-vs-使用-ktable-作为名称" class="headerlink" title="2、使用 upsert-kafka 作为新的连接器名称 vs 使用 kafka- compacted 作为名称 vs 使用 ktable 作为名称"></a>2、使用 upsert-kafka 作为新的连接器名称 vs 使用 kafka- compacted 作为名称 vs 使用 ktable 作为名称</h3><p>考虑到 KTable 的隐含含义比预期的要多，并且 kafka-compacted 中的压缩含义与 topic 而不是 table 本身相关，因此更适合使用 upsert-kafka 作为新连接器的名称，这更直接。</p><h2 id="七、未来工作"><a href="#七、未来工作" class="headerlink" title="七、未来工作"></a>七、未来工作</h2><h3 id="1、支持有界-upsert-kafka-源"><a href="#1、支持有界-upsert-kafka-源" class="headerlink" title="1、支持有界 upsert-kafka 源"></a>1、支持有界 upsert-kafka 源</h3><p>用户也可能希望以批处理模式使用 upsert-kafka 连接器。但是我们需要更多地讨论这个特性。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Netflix全程无锁</title>
      <link href="/2021/08/24/Netflix%E5%85%A8%E7%A8%8B%E6%97%A0%E9%94%81/"/>
      <url>/2021/08/24/Netflix%E5%85%A8%E7%A8%8B%E6%97%A0%E9%94%81/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.12.0源码阅读</title>
      <link href="/2021/08/24/Flink1-12-0%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
      <url>/2021/08/24/Flink1-12-0%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink1-12-0源码阅读"><a href="#Flink1-12-0源码阅读" class="headerlink" title="Flink1.12.0源码阅读"></a>Flink1.12.0源码阅读</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="一、任务提交流程"><a href="#一、任务提交流程" class="headerlink" title="一、任务提交流程"></a>一、任务提交流程</h3><ol><li>命令行提交命令</li><li>20多个步骤、几十个类、数千行代码，关键源码加注释</li><li>16个小节</li><li>PPT动图</li></ol><h3 id="二、组件通信"><a href="#二、组件通信" class="headerlink" title="二、组件通信"></a>二、组件通信</h3><ol><li>actor模型，Akka基本原理和实现</li><li>5大关键角色</li><li>代理转发、处理细节</li><li>PPT动图</li></ol><h3 id="三、任务调度"><a href="#三、任务调度" class="headerlink" title="三、任务调度"></a>三、任务调度</h3><ol><li>图：流图、作业图、执行图、物理执行图</li><li>调用位置、如何转换</li><li>task调度：调度器、调度策略、调度模型</li><li>task执行、以map算子为例</li></ol><h3 id="四、内存管理"><a href="#四、内存管理" class="headerlink" title="四、内存管理"></a>四、内存管理</h3><ol><li>Flink 1.10之后的管理模型：jobmanager,taskmanager</li><li>模型有效避免JVM内存的不足</li><li>分配过程、内存管理的概念、数据结构，特有的管理器</li><li>网络传输的内存管理，数据传输</li><li>反压过程</li></ol><h2 id="一、任务提交流程-1"><a href="#一、任务提交流程-1" class="headerlink" title="一、任务提交流程"></a>一、任务提交流程</h2><h3 id="1、程序入口"><a href="#1、程序入口" class="headerlink" title="1、程序入口"></a>1、程序入口</h3><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">flink</span> <span class="token attr-value">run >>> 里面有很多的依赖环境，有脚本的获取</span>CliFrontendYarnJobClusterEntrypoint<span class="token attr-name">TaskExecutorRunner</span> <span class="token attr-value">>>> TaskManagerRunner(Standalone)</span>org.apache.flink.client.cli.CliFrontend<span class="token attr-name">config.sh</span> <span class="token attr-value">环境信息获取</span></code></pre><h3 id="2、提交流程参数解析"><a href="#2、提交流程参数解析" class="headerlink" title="2、提交流程参数解析"></a>2、提交流程参数解析</h3><p>Ctrl + N —&gt; CliFrontend</p><p>Ctrl + F12 可以快速的筛选，然后直接输入main就可以快速的找到了</p><pre class=" language-properties"><code class="language-properties">1、获取flink的conf目录的路径<span class="token attr-name">main</span> <span class="token attr-value">-> getConfigurationDirectoryFromEnv</span>2、根据conf路径，加载配置<span class="token attr-name">loadConfiguration，我们不是配置了Flink</span> <span class="token attr-value">yaml文件吗</span>3、封装命令行接口：按顺序Generic、Yarn、Default，如何区分是什么集群loadCustomCommandLines<span class="token attr-name">4、runSecured</span> <span class="token attr-value">// 对封装好的参数进行解析</span><span class="token attr-name">    ---></span> <span class="token attr-value">run ---> CliFrontendParser</span><span class="token attr-name">    对我们提交的参数进行解析</span> <span class="token attr-value">-t -s -d 具体的赋值，参数值</span></code></pre><h3 id="3、提交流程，选择哪种命令行客户端"><a href="#3、提交流程，选择哪种命令行客户端" class="headerlink" title="3、提交流程，选择哪种命令行客户端"></a>3、提交流程，选择哪种命令行客户端</h3>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQLbinlog日志学习</title>
      <link href="/2021/08/24/MySQLbinlog%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/24/MySQLbinlog%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink-CDC-2.0-正式发布-详解核心改进</title>
      <link href="/2021/08/24/Flink-CDC-2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83-%E8%AF%A6%E8%A7%A3%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B/"/>
      <url>/2021/08/24/Flink-CDC-2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83-%E8%AF%A6%E8%A7%A3%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-CDC-2-0-正式发布-详解核心改进"><a href="#Flink-CDC-2-0-正式发布-详解核心改进" class="headerlink" title="Flink-CDC-2.0-正式发布-详解核心改进"></a>Flink-CDC-2.0-正式发布-详解核心改进</h1><h2 id="一、CDC-概述"><a href="#一、CDC-概述" class="headerlink" title="一、CDC 概述"></a>一、CDC 概述</h2><p>CDC 的全称是 Change Data Capture ，在广义的概念上，只要是能捕获数据变更的技术，我们都可以称之为 CDC 。目前通常描述的 CDC 技术主要面向数据库的变更，是一种用于捕获数据库中数据变更的技术。CDC 技术的应用场景非常广泛：</p><ul><li><strong>数据同步：</strong>用于备份，容灾；</li><li><strong>数据分发：</strong>一个数据源分发给多个下游系统；</li><li><strong>数据采集：</strong>面向数据仓库 / 数据湖的 ETL 数据集成，是非常重要的数据源。</li></ul><p>CDC 的技术方案非常多，目前业界主流的实现机制可以分为两种：</p><ul><li><strong>基于查询的 CDC：</strong><ul><li>离线调度查询作业，批处理。把一张表同步到其他系统，每次通过查询去获取表中最新的数据；</li><li>无法保障数据一致性，查的过程中有可能数据已经发生了多次变更；</li><li>不保障实时性，基于离线调度存在天然的延迟。</li></ul></li><li><strong>基于日志的 CDC：</strong><ul><li>实时消费日志，流处理，例如 MySQL 的 binlog 日志完整记录了数据库中的变更，可以把 binlog 文件当作流的数据源；</li><li>保障数据一致性，因为 binlog 文件包含了所有历史变更明细；</li><li>保障实时性，因为类似 binlog 的日志文件是可以流式消费的，提供的是实时数据。</li></ul></li></ul><p>对比常见的开源 CDC 方案，我们可以发现：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824142945266.png" alt="常见开源CDC方案比较"></p><ul><li>对比增量同步能力，<ul><li>基于日志的方式，可以很好的做到增量同步；</li><li>而基于查询的方式是很难做到增量同步的。</li></ul></li><li>对比全量同步能力，基于查询或者日志的 CDC 方案基本都支持，除了 Canal。</li><li>而对比全量 + 增量同步的能力，只有 Flink CDC、Debezium、Oracle Goldengate 支持较好。</li><li>从架构角度去看，该表将架构分为单机和分布式，这里的分布式架构不单纯体现在数据读取能力的水平扩展上，更重要的是在大数据场景下分布式系统接入能力。例如 Flink CDC 的数据入湖或者入仓的时候，下游通常是分布式的系统，如 Hive、HDFS、Iceberg、Hudi 等，那么从对接入分布式系统能力上看，Flink CDC 的架构能够很好地接入此类系统。</li><li>在数据转换 / 数据清洗能力上，当数据进入到 CDC 工具的时候是否能较方便的对数据做一些过滤或者清洗，甚至聚合？<ul><li>在 Flink CDC 上操作相当简单，可以通过 Flink SQL 去操作这些数据；</li><li>但是像 DataX、Debezium 等则需要通过脚本或者模板去做，所以用户的使用门槛会比较高。</li></ul></li><li>另外，在生态方面，这里指的是下游的一些数据库或者数据源的支持。Flink CDC 下游有丰富的 Connector，例如写入到 TiDB、MySQL、Pg、HBase、Kafka、ClickHouse 等常见的一些系统，也支持各种自定义 connector。</li></ul><h2 id="二、Flink-CDC-项目"><a href="#二、Flink-CDC-项目" class="headerlink" title="二、Flink CDC 项目"></a>二、Flink CDC 项目</h2><p>讲到这里，先带大家回顾下开发 Flink CDC 项目的动机。</p><h3 id="1-Dynamic-Table-amp-ChangeLog-Stream"><a href="#1-Dynamic-Table-amp-ChangeLog-Stream" class="headerlink" title="1. Dynamic Table &amp; ChangeLog Stream"></a>1. Dynamic Table &amp; ChangeLog Stream</h3><p>大家都知道 Flink 有两个基础概念：Dynamic Table 和 Changelog Stream。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143237941.png" alt="Flink变更日志流"></p><ul><li>Dynamic Table 就是 Flink SQL 定义的动态表，动态表和流的概念是对等的。参照上图，流可以转换成动态表，动态表也可以转换成流。</li><li>在 Flink SQL中，数据在从一个算子流向另外一个算子时都是以 Changelog Stream 的形式，任意时刻的 Changelog Stream 可以翻译为一个表，也可以翻译为一个流。</li></ul><p>联想下 MySQL 中的表和 binlog 日志，就会发现：MySQL 数据库的一张表所有的变更都记录在 binlog 日志中，如果一直对表进行更新，binlog 日志流也一直会追加，数据库中的表就相当于 binlog 日志流在某个时刻点物化的结果；日志流就是将表的变更数据持续捕获的结果。这说明 Flink SQL 的 Dynamic Table 是可以非常自然地表示一张不断变化的 MySQL 数据库表。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143525924.png" alt="数据格式"></p><p>在此基础上，我们调研了一些 CDC 技术，最终选择了 Debezium 作为 Flink CDC 的底层采集工具。Debezium 支持全量同步，也支持增量同步，也支持全量 + 增量的同步，非常灵活，同时基于日志的 CDC 技术使得提供 Exactly-Once 成为可能。</p><p>将 Flink SQL 的内部数据结构 RowData 和 Debezium 的数据结构进行对比，可以发现两者是非常相似的。</p><ul><li>每条 RowData 都有一个元数据 RowKind，包括 4 种类型， 分别是插入 (INSERT)、更新前镜像 (UPDATE_BEFORE)、更新后镜像 (UPDATE_AFTER)、删除 (DELETE)，这四种类型和数据库里面的 binlog 概念保持一致。</li><li>而 Debezium 的数据结构，也有一个类似的元数据 op 字段， op 字段的取值也有四种，分别是 c、u、d、r，各自对应 create、update、delete、read。对于代表更新操作的 u，其数据部分同时包含了前镜像 (before) 和后镜像 (after)。</li></ul><p>通过分析两种数据结构，Flink 和 Debezium 两者的底层数据是可以非常方便地对接起来的，大家可以发现 Flink 做 CDC 从技术上是非常合适的。</p><h3 id="2-传统-CDC-ETL-分析"><a href="#2-传统-CDC-ETL-分析" class="headerlink" title="2. 传统 CDC ETL 分析"></a>2. 传统 CDC ETL 分析</h3><p>我们来看下传统 CDC 的 ETL 分析链路，如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824143809866.png" alt="传统的CDC ETL分析"></p><p>目前我们公司就是在使用的是传统的ETL技术来进行实现的</p><p>传统的基于 CDC 的 ETL 分析中，数据采集工具是必须的，国外用户常用 Debezium，国内用户常用阿里开源的 Canal，采集工具负责采集数据库的增量数据，一些采集工具也支持同步全量数据。采集到的数据一般输出到消息中间件如 Kafka，然后 Flink 计算引擎再去消费这一部分数据写入到目的端，目的端可以是各种 DB，数据湖，实时数仓和离线数仓。</p><p>注意，Flink 提供了 changelog-json format，可以将 changelog 数据写入离线数仓如 Hive / HDFS；对于实时数仓，Flink 支持将 changelog 通过 upsert-kafka connector 直接写入 Kafka。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144049616.png" alt="image-20210824144049616"></p><p>我们一直在思考是否可以使用 Flink CDC 去替换上图中虚线框内的采集组件和消息队列，从而简化分析链路，降低维护成本。同时更少的组件也意味着数据时效性能够进一步提高。答案是可以的，于是就有了我们基于 Flink CDC 的 ETL 分析流程。</p><h3 id="3-基于-Flink-CDC-的-ETL-分析"><a href="#3-基于-Flink-CDC-的-ETL-分析" class="headerlink" title="3. 基于 Flink CDC 的 ETL 分析"></a>3. 基于 Flink CDC 的 ETL 分析</h3><p>在使用了 Flink CDC 之后，除了组件更少，维护更方便外，另一个优势是通过 Flink SQL 极大地降低了用户使用门槛，可以看下面的例子：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144211235.png" alt="Flink CDC ETL分析"></p><p>该例子是通过 Flink CDC 去同步数据库数据并写入到 TiDB，用户直接使用 Flink SQL 创建了产品和订单的 MySQL-CDC 表，然后对数据流进行 JOIN 加工，加工后直接写入到下游数据库。通过一个 Flink SQL 作业就完成了 CDC 的数据分析，加工和同步。</p><p>大家会发现这是一个纯 SQL 作业，这意味着只要会 SQL 的 BI，业务线同学都可以完成此类工作。与此同时，用户也可以利用 Flink SQL 提供的丰富语法进行数据清洗、分析、聚合。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144319174.png" alt="Flink CDC的聚合分析"></p><p>而这些能力，对于现有的 CDC 方案来说，进行数据的清洗，分析和聚合是非常困难的。</p><p>此外，利用 <strong>Flink SQL 双流 JOIN、维表 JOIN、UDTF</strong> 语法可以非常容易地完成数据打宽，以及各种业务逻辑加工。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144524752.png" alt="Flink如何实现数据打宽"></p><h3 id="4-Flink-CDC-项目发展"><a href="#4-Flink-CDC-项目发展" class="headerlink" title="4. Flink CDC 项目发展"></a>4. Flink CDC 项目发展</h3><ul><li><p>2020 年 7 月由云邪提交了第一个 commit，这是基于个人兴趣孵化的项目；</p></li><li><p>2020 年 7 中旬支持了 MySQL-CDC；</p></li><li><p>2020 年 7 月末支持了 Postgres-CDC；</p></li><li><p>一年的时间，该项目在 GitHub 上的 star 数已经超过 800。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144641494.png" alt="image-20210824144641494"></p></li></ul><h2 id="三、Flink-CDC-2-0-详解"><a href="#三、Flink-CDC-2-0-详解" class="headerlink" title="三、Flink CDC 2.0 详解"></a>三、Flink CDC 2.0 详解</h2><h3 id="1-Flink-CDC-痛点"><a href="#1-Flink-CDC-痛点" class="headerlink" title="1. Flink CDC 痛点"></a>1. Flink CDC 痛点</h3><p>MySQL CDC 是 Flink CDC 中使用最多也是最重要的 Connector，本文下述章节描述 Flink CDC Connector 均为 MySQL CDC Connector。</p><p>随着 Flink CDC 项目的发展，得到了很多用户在社区的反馈，主要归纳为三个：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824144838126.png" alt="Flink CDC痛点"></p><ul><li>全量 + 增量读取的过程需要保证所有数据的一致性，因此需要通过加锁保证，但是加锁在数据库层面上是一个十分高危的操作。底层 Debezium 在保证数据一致性时，需要对读取的库或表加锁，全局锁可能导致数据库锁住，表级锁会锁住表的读，DBA 一般不给锁权限。</li><li>不支持水平扩展，因为 Flink CDC 底层是基于 Debezium，起架构是单节点，所以Flink CDC 只支持单并发。在全量阶段读取阶段，如果表非常大 (亿级别)，读取时间在小时甚至天级别，用户不能通过增加资源去提升作业速度。</li><li>全量读取阶段不支持 checkpoint：CDC 读取分为两个阶段，全量读取和增量读取，目前全量读取阶段是不支持 checkpoint 的，因此会存在一个问题：当我们同步全量数据时，假设需要 5 个小时，当我们同步了 4 小时的时候作业失败，这时候就需要重新开始，再读取 5 个小时。</li></ul><h3 id="2-Debezium-锁分析"><a href="#2-Debezium-锁分析" class="headerlink" title="2. Debezium 锁分析"></a>2. Debezium 锁分析</h3><p>Flink CDC 底层封装了 Debezium， Debezium 同步一张表分为两个阶段：</p><ul><li><strong>全量阶段：</strong>查询当前表中所有记录；</li><li><strong>增量阶段：</strong>从 binlog 消费变更数据。</li></ul><p>大部分用户使用的场景都是全量 + 增量同步，加锁是发生在全量阶段，目的是为了确定全量阶段的初始位点，保证增量 + 全量实现一条不多，一条不少，从而保证数据一致性。从下图中我们可以分析全局锁和表锁的一些加锁流程，左边红色线条是锁的生命周期，右边是 MySQL 开启可重复读事务的生命周期。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145148162.png" alt="image-20210824145148162"></p><p>以全局锁为例，首先是获取一个锁，然后再去开启可重复读的事务。这里锁住操作是读取 binlog 的起始位置和当前表的 schema。这样做的目的是保证 binlog 的起始位置和读取到的当前 schema 是可以对应上的，因为表的 schema 是会改变的，比如如删除列或者增加列。在读取这两个信息后，SnapshotReader 会在可重复读事务里读取全量数据，在全量数据读取完成后，会启动 BinlogReader 从读取的 binlog 起始位置开始增量读取，从而保证全量数据 + 增量数据的无缝衔接。</p><p>表锁是全局锁的退化版，因为全局锁的权限会比较高，因此在某些场景，用户只有表锁。表锁锁的时间会更长，因为表锁有个特征：锁提前释放了可重复读的事务默认会提交，所以锁需要等到全量数据读完后才能释放。</p><p>经过上面分析，接下来看看这些锁到底会造成怎样严重的后果</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145357151.png" alt="image-20210824145357151"></p><p>Flink CDC 1.x 可以不加锁，能够满足大部分场景，但牺牲了一定的数据准确性。Flink CDC 1.x 默认加全局锁，虽然能保证数据一致性，但存在上述 hang 住数据的风险。</p><h3 id="3-Flink-CDC-2-0-设计-以-MySQL-为例"><a href="#3-Flink-CDC-2-0-设计-以-MySQL-为例" class="headerlink" title="3. Flink CDC 2.0 设计 ( 以 MySQL 为例)"></a>3. Flink CDC 2.0 设计 ( 以 MySQL 为例)</h3><p>通过上面的分析，可以知道 2.0 的设计方案，核心要解决上述的三个问题，即支持无锁、水平扩展、checkpoint。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824145543291.png" alt="image-20210824145543291"></p><p>DBlog 这篇论文里描述的无锁算法如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150134772.png" alt="image-20210824150134772"></p><p>左边是 Chunk 的切分算法描述，Chunk 的切分算法其实和很多数据库的分库分表原理类似，通过表的主键对表中的数据进行分片。假设每个 Chunk 的步长为 10，按照这个规则进行切分，只需要把这些 Chunk 的区间做成左开右闭或者左闭右开的区间，保证衔接后的区间能够等于表的主键区间即可。</p><p>右边是每个 Chunk 的无锁读算法描述，该算法的核心思想是在划分了 Chunk 后，对于每个 Chunk 的全量读取和增量读取，在不用锁的条件下完成一致性的合并。Chunk 的切分如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150249154.png" alt="image-20210824150249154"></p><p>因为每个 chunk 只负责自己主键范围内的数据，不难推导，只要能够保证每个 Chunk 读取的一致性，就能保证整张表读取的一致性，这便是无锁算法的基本原理。</p><p>Netflix 的 DBLog 论文中 Chunk 读取算法是通过在 DB 维护一张信号表，再通过信号表在 binlog 文件中打点，记录每个 chunk 读取前的 Low Position (低位点) 和读取结束之后 High Position (高位点) ，在低位点和高位点之间去查询该 Chunk 的全量数据。在读取出这一部分 Chunk 的数据之后，再将这 2 个位点之间的 binlog 增量数据合并到 chunk 所属的全量数据，从而得到高位点时刻，该 chunk 对应的全量数据。</p><p>Flink CDC 结合自身的情况，在 Chunk 读取算法上做了去信号表的改进，不需要额外维护信号表，通过直接读取 binlog 位点替代在 binlog 中做标记的功能，整体的 chunk 读算法描述如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150514617.png" alt="image-20210824150514617"></p><p>比如正在读取 Chunk-1，Chunk 的区间是 [K1, K10]，首先直接将该区间内的数据 select 出来并把它存在 buffer 中，在 select 之前记录 binlog 的一个位点 (低位点)，select 完成后记录 binlog 的一个位点 (高位点)。然后开始增量部分，消费从低位点到高位点的 binlog。</p><ul><li>图中的 - ( k2,100 ) + ( k2,108 ) 记录表示这条数据的值从 100 更新到 108；</li><li>第二条记录是删除 k3；</li><li>第三条记录是更新 k2 为 119；</li><li>第四条记录是 k5 的数据由原来的 77 变更为 100。</li></ul><p>观察图片中右下角最终的输出，会发现在消费该 chunk 的 binlog 时，出现的 key 是k2、k3、k5，我们前往 buffer 将这些 key 做标记。</p><ul><li>对于 k1、k4、k6、k7 来说，在高位点读取完毕之后，这些记录没有变化过，所以这些数据是可以直接输出的；</li><li>对于改变过的数据，则需要将增量的数据合并到全量的数据中，只保留合并后的最终数据。例如，k2 最终的结果是 119 ，那么只需要输出 +(k2,119)，而不需要中间发生过改变的数据。</li></ul><p>通过这种方式，Chunk 最终的输出就是在高位点是 chunk 中最新的数据。</p><p>上图描述的是单个 Chunk 的一致性读，但是如果有多个表分了很多不同的 Chunk，且这些 Chunk 分发到了不同的 task 中，那么如何分发 Chunk 并保证全局一致性读呢？</p><p>这个就是基于 FLIP-27 来优雅地实现的，通过下图可以看到有 SourceEnumerator 的组件，这个组件主要用于 Chunk 的划分，划分好的 Chunk 会提供给下游的 SourceReader 去读取，通过把 chunk 分发给不同的 SourceReader 便实现了并发读取 Snapshot Chunk 的过程，同时基于 FLIP-27 我们能较为方便地做到 chunk 粒度的 checkpoint。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824150831713.png" alt="image-20210824150831713"></p><p>当 Snapshot Chunk 读取完成之后，需要有一个汇报的流程，如下图中橘色的汇报信息，将 Snapshot Chunk 完成信息汇报给 SourceEnumerator。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151035349.png" alt="image-20210824151035349"></p><p>汇报的主要目的是为了后续分发 binlog chunk (如下图)。因为 Flink CDC 支持全量 + 增量同步，所以当所有 Snapshot Chunk 读取完成之后，还需要消费增量的 binlog，这是通过下发一个 binlog chunk 给任意一个 Source Reader 进行单并发读取实现的。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151132414.png" alt="image-20210824151132414"></p><p>对于大部分用户来讲，其实无需过于关注如何无锁算法和分片的细节，了解整体的流程就好。</p><p>整体流程可以概括为，首先通过主键对表进行 Snapshot Chunk 划分，再将 Snapshot Chunk 分发给多个 SourceReader，每个 Snapshot Chunk 读取时通过算法实现无锁条件下的一致性读，SourceReader 读取时支持 chunk 粒度的 checkpoint，在所有 Snapshot Chunk 读取完成后，下发一个 binlog chunk 进行增量部分的 binlog 读取，这便是 Flink CDC 2.0 的整体流程，如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824151250393.png" alt="image-20210824151250393"></p><p>Flink CDC 是一个完全开源的项目，项目所有设计和源码目前都已贡献到开源社区，<a href="https://github.com/ververica/flink-cdc-connectors/releases/tag/release-2.0.0">Flink CDC 2.0</a> 也已经正式发布，此次的核心改进和提升包括：</p><ul><li>提供 MySQL CDC 2.0，核心feature 包括<ul><li>并发读取，全量数据的读取性能可以水平扩展；</li><li>全程无锁，不对线上业务产生锁的风险；</li><li>断点续传，支持全量阶段的 checkpoint。</li></ul></li><li>搭建文档网站，提供多版本文档支持，文档支持关键词搜索</li></ul><p>笔者用 TPC-DS 数据集中的 customer 表进行了测试，Flink 版本是 1.13.1，customer 表的数据量是 6500 万条，Source 并发为 8，全量读取阶段:</p><ul><li>MySQL CDC 2.0 用时 <strong>13</strong> 分钟；</li><li>MySQL CDC 1.4 用时 <strong>89</strong> 分钟；</li><li>读取性能提升 <strong>6.8</strong> 倍。</li></ul><p>为了提供更好的文档支持，Flink CDC 社区搭建了文档网站，网站支持对文档的版本管理：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152059965.png" alt="image-20210824152059965"></p><p>文档网站支持关键字搜索功能，非常实用：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152117345.png" alt="image-20210824152117345"></p><h2 id="四、未来规划"><a href="#四、未来规划" class="headerlink" title="四、未来规划"></a>四、未来规划</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824152546448.png" alt="image-20210824152546448"></p><p>关于 CDC 项目的未来规划，我们希望围绕稳定性，进阶 feature 和生态集成三个方面展开。</p><ul><li><strong>稳定性</strong><ul><li>通过社区的方式吸引更多的开发者，公司的开源力量提升 Flink CDC 的成熟度；</li><li>支持 Lazy Assigning。Lazy Assigning 的思路是将 chunk 先划分一批，而不是一次性进行全部划分。当前 Source Reader 对数据读取进行分片是一次性全部划分好所有 chunk，例如有 1 万个 chunk，可以先划分 1 千个 chunk，而不是一次性全部划分，在 SourceReader 读取完 1 千 chunk 后再继续划分，节约划分 chunk 的时间。</li></ul></li><li><strong>进阶 Feature</strong><ul><li>支持 Schema Evolution。这个场景是：当同步数据库的过程中，突然在表中添加了一个字段，并且希望后续同步下游系统的时候能够自动加入这个字段；</li><li>支持 Watermark Pushdown 通过 CDC 的 binlog 获取到一些心跳信息，这些心跳的信息可以作为一个 Watermark，通过这个心跳信息可以知道到这个流当前消费的一些进度；</li><li>支持 META 数据，分库分表的场景下，有可能需要元数据知道这条数据来源哪个库哪个表，在下游系统入湖入仓可以有更多的灵活操作；</li><li>整库同步：用户要同步整个数据库只需一行 SQL 语法即可完成，而不用每张表定义一个 DDL 和 query。</li></ul></li><li><strong>生态集成</strong><ul><li>集成更多上游数据库，如 Oracle，MS SqlServer。Cloudera 目前正在积极贡献 oracle-cdc connector；</li><li>在入湖层面，Hudi 和 Iceberg 写入上有一定的优化空间，例如在高 QPS 入湖的时候，数据分布有比较大的性能影响，这一点可以通过与生态打通和集成继续优化。</li></ul></li></ul><p>最后，欢迎大家加入 Flink CDC 用户群一起交流。</p><h2 id="五、附录"><a href="#五、附录" class="headerlink" title="五、附录"></a>五、附录</h2><p><a href="https://github.com/ververica/flink-cdc-connectors?spm=a2csy.flink.0.0.49493bdcI4Avjy"><font color="red">[1]&nbsp; &nbsp; Flink CDC 项目地址</font></a></p><p><a href="https://ververica.github.io/flink-cdc-connectors/master/" target="_blank" rel="noopener"><font color="red">[2]&nbsp; &nbsp; Flink CDC 文档网站</font></a></p><p><a href="https://www.percona.com/blog/2014/03/11/introducing-backup-locks-percona-server-2/?spm=a2csy..0.0.55796c2e2qSuAf" target="_blank" rel="noopener"><font color="red">[3]&nbsp; &nbsp; Percona MySQL 全局锁时间分析</font></a></p><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface?spm=a2csy..0.0.55796c2e2qSuAf" target="_blank" rel="noopener"><font color="red">[4]&nbsp; &nbsp; Flink FLIP-27设计文档</font></a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据采集系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据采集系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入解读FlinkSQL1.13</title>
      <link href="/2021/08/24/%E6%B7%B1%E5%85%A5%E8%A7%A3%E8%AF%BBFlinkSQL1-13/"/>
      <url>/2021/08/24/%E6%B7%B1%E5%85%A5%E8%A7%A3%E8%AF%BBFlinkSQL1-13/</url>
      
        <content type="html"><![CDATA[<h1 id="深入解读FlinkSQL1-13"><a href="#深入解读FlinkSQL1-13" class="headerlink" title="深入解读FlinkSQL1.13"></a>深入解读FlinkSQL1.13</h1><p>参考网址：<a href="https://flink-learning.org.cn/article/detail/a8b0895d4271bf6b770927eea214612d" target="_blank" rel="noopener">https://flink-learning.org.cn/article/detail/a8b0895d4271bf6b770927eea214612d</a></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113303217.png" alt="Flink SQL核心功能"></p><ol><li>Flink SQL 1.13 概览</li><li>核心 feature 解读</li><li>重要改进解读</li><li>Flink SQL 1.14 未来规划</li><li>总结</li></ol><h2 id="一、Flink-SQL-1-13-概览"><a href="#一、Flink-SQL-1-13-概览" class="headerlink" title="一、Flink SQL 1.13 概览"></a>一、Flink SQL 1.13 概览</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113358728.png" alt="Flink SQL概览"></p><p>Flink 1.13 是一个社区大版本，解决的 issue 在 1000 个以上，通过上图我们可以看到，解决的问题大部分是关于 Table/SQL 模块，一共 400 多个 issue 占了总体的 37% 左右。这些 issue 主要围绕了 5 个 FLIP 展开，在本文中我们也会根据这 5 个方面进行介绍，它们分别是：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113444978.png" alt="Table SQL"></p><p>下面我们对这些 FLIP 进行详细解读。</p><h2 id="二、-核心-feature-解读"><a href="#二、-核心-feature-解读" class="headerlink" title="二、 核心 feature 解读"></a>二、 核心 feature 解读</h2><h3 id="1-FLIP-145：支持-Window-TVF"><a href="#1-FLIP-145：支持-Window-TVF" class="headerlink" title="1. FLIP-145：支持 Window TVF"></a>1. FLIP-145：支持 Window TVF</h3><p>社区的小伙伴应该了解，在腾讯、阿里巴巴、字节跳动等公司的内部分支已经开发了这个功能的基础版本。这次 Flink 社区也在 Flink 1.13 推出了 TVF 的相关支持和优化。下面将从 Window TVF 语法、近实时累计计算场景、 Window 性能优化、多维数据分析，来分析这个新功能。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113601029.png" alt="Window TVF"></p><h4 id="1-1-Window-TVF-语法"><a href="#1-1-Window-TVF-语法" class="headerlink" title="1.1 Window TVF 语法"></a>1.1 Window TVF 语法</h4><p>在 1.13 版本前，window 的实现是通过一个特殊的 SqlGroupedWindowFunction：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span>   TUMBLE_START<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  TUMBLE_END<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  TUMBLE_ROWTIME<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token function">SUM</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span><span class="token keyword">FROM</span> MyTable<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> TUMBLE<span class="token punctuation">(</span>bidtime<span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span></code></pre><p>在 1.13 版本中，我们对它进行了 Table-Valued Function 的语法标准化：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end<span class="token punctuation">,</span>WINDOW_time<span class="token punctuation">,</span><span class="token function">SUM</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span> <span class="token keyword">FROM</span> <span class="token keyword">Table</span><span class="token punctuation">(</span>TUMBLE<span class="token punctuation">(</span><span class="token keyword">Table</span> myTable<span class="token punctuation">,</span>DESCRIPTOR<span class="token punctuation">(</span>biztime<span class="token punctuation">)</span><span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTE<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end</code></pre><p>通过对比两种语法，我们可以发现：TVF 语法更加灵活，不需要必须跟在 GROUP BY 关键字后面，同时 Window TVF 基于关系代数，使得其更加标准。在只需要划分窗口场景时，可以只用 TVF，无需用 GROUP BY 做聚合，这使得 TVF 扩展性和表达能力更强，支持自定义 TVF（例如实现 TOP-N 的 TVF）。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113822022.png" alt="TOP-N TVF"></p><p>上图中的示例就是利用 TVF 做的滚动窗口的划分，只需要把数据划分到窗口，无需聚合；如果后续需要聚合，再进行 GROP BY 即可。同时，对于熟悉批 SQL 的用户来说，这种操作是非常自然的，我们不再需要像 1.13 版本之前那样必须要用特殊的 SqlGroupedWindowFunction 将窗口划分和聚合绑定在一起。</p><p>目前 Window TVF 支持 tumble window，hop window，新增了 cumulate window；session window 预计在 1.14 版本也会支持。</p><h4 id="1-2-Cumulate-Window"><a href="#1-2-Cumulate-Window" class="headerlink" title="1.2 Cumulate Window"></a>1.2 Cumulate Window</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824113914700.png" alt="cumulate window"></p><p>Cumulate window 就是累计窗口，简单来说，以上图里面时间轴上的一个区间为窗口步长。</p><ul><li>第一个 window 统计的是一个区间的数据；</li><li>第二个 window 统计的是第一区间和第二个区间的数据；</li><li>第三个 window 统计的是第一区间，第二个区间和第三个区间的数据。</li></ul><p>累积计算在业务场景中非常常见，如累积 UV 场景。在 UV 大盘曲线中：我们每隔 10 分钟统计一次当天累积用户 UV。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114137054.png" alt="近实时累计计算"></p><p>在 1.13 版本之前，当需要做这种计算时，我们一般的 SQL 写法如下：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> cumulative_UV<span class="token keyword">SELECT</span> date_str<span class="token punctuation">,</span><span class="token function">MAX</span><span class="token punctuation">(</span>time_str<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token keyword">DISTINCT</span> user_id<span class="token punctuation">)</span> <span class="token keyword">as</span> UV<span class="token keyword">FROM</span> <span class="token punctuation">(</span>    <span class="token keyword">SELECT</span>      DATE_FORMAT<span class="token punctuation">(</span>ts<span class="token punctuation">,</span><span class="token string">'yyyy-MM-dd'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> date_str<span class="token punctuation">,</span>      SUBSTR<span class="token punctuation">(</span>DATE_FORMAT<span class="token punctuation">(</span>ts<span class="token punctuation">,</span><span class="token string">'HH:mm'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token operator">||</span> <span class="token string">'0'</span> <span class="token keyword">as</span> time_str<span class="token punctuation">,</span>      user_id  <span class="token keyword">FROM</span> user_behavior<span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> date_str</code></pre><p>先将每条记录所属的时间窗口字段拼接好，然后再对所有记录按照拼接好的时间窗口字段，通过 GROUP BY 做聚合，从而达到近似累积计算的效果。</p><ul><li>1.13 版本前的写法有很多缺点，首先这个聚合操作是每条记录都会计算一次。其次，在追逆数据的时候，消费堆积的数据时，UV 大盘的曲线就会跳变。</li><li>在 1.13 版本支持了 TVF 写法，基于 cumulate window，我们可以修改为下面的写法，将每条数据按照 Event Time 精确地分到每个 Window 里面, 每个窗口的计算通过 watermark 触发，即使在追数据场景中也不会跳变。</li></ul><pre class=" language-sql"><code class="language-sql"><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> cumulative_UV<span class="token keyword">SELECT</span> WINDOW_end<span class="token punctuation">,</span><span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token keyword">DISTINCT</span> user_id<span class="token punctuation">)</span> <span class="token keyword">as</span> UV<span class="token keyword">FROM</span> <span class="token keyword">Table</span><span class="token punctuation">(</span>    CUMULATE<span class="token punctuation">(</span><span class="token keyword">Table</span> user_behavior<span class="token punctuation">,</span>DESCRIPTOR<span class="token punctuation">(</span>ts<span class="token punctuation">)</span><span class="token punctuation">,</span>INTERVAL <span class="token string">'10'</span> MINUTES<span class="token punctuation">,</span>INTERVAL <span class="token string">'1'</span> DAY<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> WINDOW_start<span class="token punctuation">,</span>WINDOW_end</code></pre><p>UV 大盘曲线效果如下图所示：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114419320.png" alt="UV大盘曲线效果下图所示"></p><h4 id="1-3-Window-性能优化"><a href="#1-3-Window-性能优化" class="headerlink" title="1.3 Window 性能优化"></a>1.3 Window 性能优化</h4><p>Flink 1.13 社区开发者们对 Window TVF 进行了一系列的性能优化，包括：</p><ul><li><strong>内存优化：</strong>通过内存预分配，缓存 window 的数据，通过 window watermark 触发计算，通过申请一些内存 buffer 避免高频的访问 state；</li><li><strong>切片优化：</strong>将 window 切片，尽可能复用已计算结果，如 hop window，cumulate window。计算过的分片数据无需再次计算，只需对切片的计算结果进行复用；</li><li><strong>算子优化：</strong>window 算子支持 local-global 优化；同时支持 count(distinct) 自动解热点优化；</li><li><strong>迟到数据：</strong>支持将迟到数据计算到后续分片，保证数据准确性。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114505810.png" alt="Window性能优化"></p><p>基于这些优化，我们通过开源 Benchmark (Nexmark) 进行性能测试。结果显示 window 的普适性能有 2x 提升，且在 count(distinct) 场景会有更好的性能提升。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114537906.png" alt="Window性能优化"></p><h4 id="1-4-多维数据分析"><a href="#1-4-多维数据分析" class="headerlink" title="1.4 多维数据分析"></a>1.4 多维数据分析</h4><p>语法的标准化带来了更多的灵活性和扩展性，用户可以直接在 window 窗口函数上进行多维分析。如下图所示，可以直接进行 GROUPING SETS、ROLLUP、CUBE 的分析计算。如果是在 1.13 之前的版本，我们可能需要对这些分组进行单独的 SQL 聚合，再对聚合结果做 union 操作才能达到类似的效果。而现在，类似这种多维分析的场景，可以直接在 window TVF 上支持。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114730789.png" alt="多维数据分析"></p><h5 id="1-4-1-支持-Window-Top-N"><a href="#1-4-1-支持-Window-Top-N" class="headerlink" title="1.4.1 支持 Window Top-N"></a>1.4.1 <strong>支持 Window Top-N</strong></h5><p>除了多维分析，Window TVF 也支持 Top-N 语法，使得在 Window 上取 Top-N 的写法更加简单。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114826337.png" alt="多维数据分析SQL"></p><h3 id="2-FLIP-162：时区和时间函数"><a href="#2-FLIP-162：时区和时间函数" class="headerlink" title="2. FLIP-162：时区和时间函数"></a>2. FLIP-162：时区和时间函数</h3><h4 id="2-1-时区问题分析"><a href="#2-1-时区问题分析" class="headerlink" title="2.1 时区问题分析"></a>2.1 时区问题分析</h4><p>大家在使用 Flink SQL 时反馈了很多时区相关的问题，造成时区问题的原因可以归纳为 3 个：</p><ul><li>PROCTIME() 函数应该考虑时区，但未考虑时区；</li><li>CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/NOW() 函数未考虑时区；</li><li>Flink 的时间属性，只支持定义在 TIMESTAMP 这种数据类型上面，这个类型是无时区的，TIMESTAMP 类型不考虑时区，但用户希望是本地时区的时间。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824114929478.png" alt="时区问题分析"></p><p>针对 TIMESTAMP 类型没有考虑时区的问题，我们提议通过TIMESTAMP_LTZ类型支持 (TIMESTAMP_LTZ 是 timestamp with local time zone 的缩写)。可以通过下面的表格来进行和 TIMESTAMP 的对比：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115024280.png" alt="时区问题分析"></p><p>TIMESTAMP_LTZ 区别于之前我们使用的 TIMESTAMP，它表示绝对时间的含义。通过对比我们可以发现：</p><ul><li>如果我们配置使用 TIMESTAMP，它可以是字符串类型的。用户不管是从英国还是中国时区来观察，这个值都是一样的；</li><li>但是对于 TIMSTAMP_TLZ 来说，它的来源就是一个 Long 值，表示从时间原点流逝过的时间。同一时刻，从时间原点流逝的时间在所有时区都是相同的，所以这个 Long 值是绝对时间的概念。当我们在不同的时区去观察这个值，我们会用本地的时区去解释成 “年-月-日-时-分-秒” 的可读格式，这就是 TIMSTAMP_TLZ 类型，TIMESTAMP_LTZ 类型也更加符合用户在不同时区下的使用习惯。</li></ul><p>下面的例子展示了 TIMESTAMP 和 TIMESTAMP_LTZ 两个类型的区别。</p><h4 id="2-2-时间函数纠正"><a href="#2-2-时间函数纠正" class="headerlink" title="2.2 时间函数纠正"></a>2.2 时间函数纠正</h4><p><strong>订正 PROCTIME() 函数</strong></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115208530.png" alt="时间函数纠正"></p><p>当我们有了 TIMESTAMP_LTZ 这个类型的时候，我们对 PROCTIME() 类型做了纠正：在 1.13 版本之前，它总是返回 UTC 的 TIMESTAMP；而现在，我们把返回类型变为了 TIMESTAMP_LTZ。PROCTIME 除了表示函数之外，也可以表示时间属性的标记。</p><p><strong>订正 CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/NOW() 函数</strong></p><p>这些函数在不同时区下出来的值是会发生变化的。例如在英国 UTC 时区时候是凌晨 2 点；但是如果你设置了时区是 UTC+8，时间就是在早上的 10 点。不同时区的实际时间会发生变化，效果如下图：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824115254981.png" alt="时间函数的纠正"><strong>解决 processing time Window 时区问题</strong></p><p>大家都知道 proctime 可以表示一个时间属性，对 proctime 的 window 操作：</p><ul><li>在 1.13 版本之前，如果我们需要做按天的 window 操作，你需要手动解决时区问题，去做一些 8 小时的偏移然后再减回去；</li><li>在 FLIP-162 中我们解决了这个问题，现在用户使用的时候十分简单，只需要声明 proctime 属性，因为 PROCTIME() 函数的返回值是TIMESTAMP_LTZ，所以结果是会考虑本地的时区。下图的例子显示了在不同的时区下，proctime 属性的 window 的聚合是按照本地时区进行的。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121417679.png" alt="image-20210824121417679"></p><p><strong>订正 Streaming 和 Batch 模式下函数取值方式</strong></p><p>时间函数其实在流和批上面的表现形式会有所区别，这次修正主要是让其更加符合用户实际的使用习惯。例如以下函数：</p><ul><li>在流模式中是 per-record 计算，即每条数据都计算一次；</li><li>在 Batch 模式是 query-start 计算，即在作业开始前计算一次。例如我们常用的一些 Batch 计算引擎，如 Hive 也是在每一个批开始前计算一次。</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121453936.png" alt="时间函数的纠正"></p><h4 id="2-3-时间类型使用"><a href="#2-3-时间类型使用" class="headerlink" title="2.3 时间类型使用"></a>2.3 时间类型使用</h4><p>在 1.13 版本也支持了在 TIMESTAMP 列上定义 Event time，也就是说Event time 现在既支持定义在 TIMESTAMP 列上，也支持定义在 TIMESTAMP_ LTZ 列上。那么作为用户，具体什么场景用什么类型呢？</p><ul><li>当作业的上游源数据包含了字符串的时间（如：2021-4-15 14:00:00）这样的场景，直接声明为 TIMESTAMP 然后把 Event time 定义在上面即可，窗口在计算的时候会基于时间字符串进行切分，最终会计算出符合你实际想要的预想结果；</li></ul><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121528721.png" alt="时间类型的使用"></p><p>当上游数据源的打点时间属于 long 值，表示的是一个绝对时间的含义。在 1.13 版本你可以把 Event time 定义在 TIMESTAMP_LTZ 上面。此时定义在 TIMESTAMP_LTZ 类型上的各种 WINDOW 聚合，都能够自动的解决 8 小时的时区偏移问题，无需按照之前的 SQL 写法额外做时区的修改和订正。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121559194.png" alt="时间类型使用"></p><p>小提示：Flink SQL 中关于时间函数，时区支持的这些提升，是版本不兼容的。用户在进行版本更新的时候需要留意作业逻辑中是否包含此类函数，避免升级后业务受到影响。</p><h4 id="2-4-夏令时支持"><a href="#2-4-夏令时支持" class="headerlink" title="2.4 夏令时支持"></a>2.4 夏令时支持</h4><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121631915.png" alt="夏令时支持"></p><p>在 Flink 1.13 以前，对于国外夏令时时区的用户，做窗口相关的计算操作是十分困难的一件事，因为存在夏令时和冬令时切换的跳变。</p><p>Flink 1.13 通过支持在 TIMESTAMP_LTZ 列上定义时间属性，同时 Flink SQL 在 WINDOW 处理时巧妙地结合 TIMESTAMP 和 TIMESTAMP_LTZ 类型，优雅地支持了夏令时。这对国外夏令时时区用户，以及有海外业务场景的公司比较有用。</p><h2 id="三、重要改进解读"><a href="#三、重要改进解读" class="headerlink" title="三、重要改进解读"></a>三、重要改进解读</h2><h3 id="1-FLIP-152：提升-Hive-语法兼容性"><a href="#1-FLIP-152：提升-Hive-语法兼容性" class="headerlink" title="1. FLIP-152：提升 Hive 语法兼容性"></a>1. FLIP-152：提升 Hive 语法兼容性</h3><p>FLIP-152 主要是做了 Hive 语法的兼容性增强，支持了 Hive 的一些常用 DML 和 DQL 语法，包括：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121758967.png" alt="提升hive语法兼容性问题"></p><p>通过 Hive dialect 支持 Hive 常用语法。Hive 有很多的内置函数，Hive dialect 需要配合 HiveCatalog 和 Hive Module 一起使用，Hive Module 提供了 Hive 所有内置函数，加载后可以直接访问。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121826256.png" alt="Hivedialect支持hive 常用语法"></p><p>与此同时，我们还可以通过 Hive dialect 创建/删除 Catalog 函数以及一些自定义的函数，这样使得 Flink SQL 与 Hive 的兼容性得到了极大的提升，让熟悉 Hive 的用户使用起来会更加方便。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121903273.png" alt="删除函数"></p><h3 id="2-FLIP-163：改进-SQL-Client"><a href="#2-FLIP-163：改进-SQL-Client" class="headerlink" title="2. FLIP-163：改进 SQL Client"></a>2. FLIP-163：改进 SQL Client</h3><p>在 1.13 版本之前，大家觉得 Flink SQL Client 就是周边的一个小工具。但是，FLIP-163 在 1.13 版本进行了重要改进：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121932578.png" alt="改进SQL Client"></p><ol><li>通过 -i 的参数，提前把 DDL 一次性加载初始化，方便初始化表的多个 DDL 语句，不需要多次执行命令创建表，替代了之前用 yaml 文件方式创建表；</li><li>支持 -f 参数，其中 SQL 文件支持 DML（insert into）语句；</li><li>支持更多实用的配置：</li></ol><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824121955707.png" alt="SQL client配置"></p><ol><li><ul><li>通过 <strong>SET SQL-client.verbose = true</strong> , 开启 verbose，通过开启 verbose 打印整个信息，相对以前只输出一句话更加容易追踪错误信息；</li><li>通过 <strong>SET execution.runtime-mode=streaming / batch</strong> 支持设置批/流作业模式；</li><li>通过 <strong>SET pipline.name=my_Flink_job</strong> 设置作业名称；</li><li>通过 <strong>SET execution.savepoint.path=/tmp/Flink-savepoints/savepoint-bb0dab</strong> 设置作业 savepoint 路径；</li><li>对于有依赖的多个作业，通过 <strong>SET Table.dml-sync=true</strong> 去选择是否异步执行，例如离线作业，作业 a 跑完才能跑作业 b ，通过设置为 true 实现执行有依赖关系的 pipeline 调度。</li></ul></li></ol><p>4、同时支持 STATEMENT SET语法：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824122057735.png" alt="sql client"></p><p>有可能我们的一个查询不止写到一个 sink 里面，而是需要输出到多个 sink，比如一个 sink 写到 jdbc，一个 sink 写到 HBase。</p><ul><li>在 1.13 版本之前需要启动 2 个 query 去完成这个作业；</li><li>在 1.13 版本，我们可以把这些放到一个 statement 里面，以一个作业的方式去执行，能够实现节点的复用，节约资源。</li></ul><h3 id="3-FLIP-136：增强-DataStream-和-Table-的转换"><a href="#3-FLIP-136：增强-DataStream-和-Table-的转换" class="headerlink" title="3. FLIP-136：增强 DataStream 和 Table 的转换"></a>3. FLIP-136：增强 DataStream 和 Table 的转换</h3><p>虽然 Flink SQL 大大降低了我们使用实时计算的一些使用门槛，但 Table/SQL 这种高级封装也屏蔽了一些底层实现，如 timer，state 等。不少高级用户希望能够直接操作 DataStream 获得更多的灵活性，这就需要在 Table 和 DataStream 之间进行转换。FLIP-136 增强了 Table 和 DataStream 间的转换，使得用户在两者之间的转换更加容易。</p><ul><li>支持 DataStream 和 Table 转换时传递 EVENT TIME 和 WATERMARK；</li></ul><pre class=" language-java"><code class="language-java">Table Table <span class="token operator">=</span> TableEnv<span class="token punctuation">.</span><span class="token function">fromDataStream</span><span class="token punctuation">(</span>    dataStream<span class="token punctuation">,</span>  Schema<span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">columnByMetadata</span><span class="token punctuation">(</span><span class="token string">"rowtime"</span><span class="token punctuation">,</span><span class="token string">"TIMESTMP(3)"</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">watermark</span><span class="token punctuation">(</span><span class="token string">"rowtime"</span><span class="token punctuation">,</span><span class="token string">"SOURCE_WATERMARK()"</span><span class="token punctuation">)</span>  <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">)</span></code></pre><ul><li>支持 Changelog 数据流在 Table 和 DataStream 间相互转换。</li></ul><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">//DATASTREAM 转 Table</span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">fromChangelogStream</span><span class="token punctuation">(</span>DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span><span class="token punctuation">)</span><span class="token operator">:</span> TableStreamTableEnvironment<span class="token punctuation">.</span><span class="token function">fromChangelogStream</span><span class="token punctuation">(</span>DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span><span class="token punctuation">,</span>Schema<span class="token punctuation">)</span><span class="token operator">:</span> Table<span class="token comment" spellcheck="true">//Table 转 DATASTREAM</span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">toChangelogStream</span><span class="token punctuation">(</span>Table<span class="token punctuation">)</span><span class="token operator">:</span> DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span>StreamTableEnvironment<span class="token punctuation">.</span><span class="token function">toChangelogStream</span><span class="token punctuation">(</span>Table<span class="token punctuation">,</span>Schema<span class="token punctuation">)</span><span class="token operator">:</span> DataStream<span class="token operator">&lt;</span>ROW<span class="token operator">></span>  </code></pre><h2 id="四、Flink-SQL-1-14-未来规划"><a href="#四、Flink-SQL-1-14-未来规划" class="headerlink" title="四、Flink SQL 1.14 未来规划"></a>四、Flink SQL 1.14 未来规划</h2><p>1.14 版本主要有以下几点规划：</p><ul><li><strong>删除 Legacy Planner</strong>：从 Flink 1.9 开始，在阿里贡献了 Blink-Planner 之后，很多一些新的 Feature 已经基于此 Blink Planner 进行开发，以前旧的 Legacy Planner 会彻底删除；</li><li><strong>完善 Window TVF</strong>：支持 session window，支持 window TVF 的 allow -lateness 等；</li><li><strong>提升 Schema Handling</strong>：全链路的 Schema 处理能力以及关键校验的提升；</li><li><strong>增强 Flink CDC 支持</strong>：增强对上游 CDC 系统的集成能力，Flink SQL 内更多的算子支持 CDC 数据流。</li></ul><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>本文详细解读了 Flink SQL 1.13 的核心功能和重要改进。</p><ul><li>支持 Window TVF；</li><li>系统地解决时区和时间函数问题；</li><li>提升 Hive 和 Flink 的兼容性；</li><li>改进 SQL Client；</li><li>增强 DataStream 和 Table 的转换。</li></ul><p>同时还分享了社区关于 Flink SQL 1.14 的未来规划，相信看完文章的同学可以对 Flink SQL 在这个版本中的变化有更多的了解，在实践过程中大家可以多多关注这些新的改动和变化，感受它们所带来的业务层面上的便捷。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ApacheFlink实战教程CEP实战</title>
      <link href="/2021/08/24/ApacheFlink%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8BCEP%E5%AE%9E%E6%88%98/"/>
      <url>/2021/08/24/ApacheFlink%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8BCEP%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h1 id="ApacheFlink实战教程CEP实战"><a href="#ApacheFlink实战教程CEP实战" class="headerlink" title="ApacheFlink实战教程CEP实战"></a>ApacheFlink实战教程CEP实战</h1><ol><li>Flink CEP 概念以及使用场景。</li><li>如何使用 Flink CEP。</li><li>如何扩展 Flink CEP。</li></ol><h2 id="Flink-CEP-概念以及使用场景"><a href="#Flink-CEP-概念以及使用场景" class="headerlink" title="Flink CEP 概念以及使用场景"></a>Flink CEP 概念以及使用场景</h2><h3 id="1-什么是-CEP"><a href="#1-什么是-CEP" class="headerlink" title="1.什么是 CEP"></a>1.什么是 CEP</h3><p>CEP 的意思是复杂事件处理，例如：起床–&gt;洗漱–&gt;吃饭–&gt;上班等一系列串联起来的事件流形成的模式称为 CEP。如果发现某一次起床后没有刷牙洗脸亦或是吃饭就直接上班，就可以把这种非正常的事件流匹配出来进行分析，看看今天是不是起晚了。</p><p>下图中列出了几个例子：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824104216837.png" alt="Flink CEP Example"></p><ul><li><strong>第一个是异常行为检测的例子：</strong>假设车辆维修的场景中，当一辆车出现故障时，这辆车会被送往维修点维修，然后被重新投放到市场运行。如果这辆车被投放到市场之后还未被使用就又被报障了，那么就有可能之前的维修是无效的。</li><li><strong>第二个是策略营销的例子：</strong>假设打车的场景中，用户在 APP 上规划了一个行程订单，如果这个行程在下单之后超过一定的时间还没有被司机接单的话，那么就需要将这个订单输出到下游做相关的策略调整。</li><li><strong>第三个是运维监控的例子：</strong>通常运维会监控服务器的 CPU、网络 IO 等指标超过阈值时产生相应的告警。但是在实际使用中，后台服务的重启、网络抖动等情况都会造成瞬间的流量毛刺，对非关键链路可以忽略这些毛刺而只对频繁发生的异常进行告警以减少误报。</li></ul><h3 id="2-Flink-CEP-应用场景"><a href="#2-Flink-CEP-应用场景" class="headerlink" title="2.Flink CEP 应用场景"></a>2.Flink CEP 应用场景</h3><ul><li><strong>风险控制：</strong>对用户异常行为模式进行实时检测，当一个用户发生了不该发生的行为，判定这个用户是不是有违规操作的嫌疑。</li><li><strong>策略营销：</strong>用预先定义好的规则对用户的行为轨迹进行实时跟踪，对行为轨迹匹配预定义规则的用户实时发送相应策略的推广。</li><li><strong>运维监控：</strong>灵活配置多指标、多依赖来实现更复杂的监控模式。</li></ul><h3 id="3-Flink-CEP-原理"><a href="#3-Flink-CEP-原理" class="headerlink" title="3.Flink CEP 原理"></a>3.Flink CEP 原理</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824104701307.png" alt="Flink CEP原理图"></p><p>Flink CEP 内部是用 NFA（非确定有限自动机）来实现的，由点和边组成的一个状态图，以一个初始状态作为起点，经过一系列的中间状态，达到终态。点分为起始状态、中间状态、最终状态三种，边分为 take、ignore、proceed 三种。</p><ul><li><strong>take</strong>：必须存在一个条件判断，当到来的消息满足 take 边条件判断时，把这个消息放入结果集，将状态转移到下一状态。</li><li><strong>ignore</strong>：当消息到来时，可以忽略这个消息，将状态自旋在当前不变，是一个自己到自己的状态转移。</li><li><strong>proceed</strong>：又叫做状态的空转移，当前状态可以不依赖于消息到来而直接转移到下一状态。举个例子，当用户购买商品时，如果购买前有一个咨询客服的行为，需要把咨询客服行为和购买行为两个消息一起放到结果集中向下游输出；如果购买前没有咨询客服的行为，只需把购买行为放到结果集中向下游输出就可以了。 也就是说，如果有咨询客服的行为，就存在咨询客服状态的上的消息保存，如果没有咨询客服的行为，就不存在咨询客服状态的上的消息保存，咨询客服状态是由一条 proceed 边和下游的购买状态相连。</li></ul><p>下面以一个打车的例子来展示状态是如何流转的，规则见下图所示。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105008651.png" alt="打车案例"></p><p>以乘客制定行程作为开始，匹配乘客的下单事件，如果这个订单超时还没有被司机接单的话，就把行程事件和下单事件作为结果集往下游输出。</p><p>假如消息到来顺序为：行程–&gt;其他–&gt;下单–&gt;其他。</p><p>状态流转如下：</p><p>1.开始时状态处于行程状态，即等待用户制定行程。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105557056.png" alt="image-20210824105557056"></p><p>2.当收到行程事件时，匹配行程状态的条件，把行程事件放到结果集中，通过 take 边将状态往下转移到下单状态。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105707540.png" alt="image-20210824105707540"></p><p>3.由于下单状态上有一条 ignore 边，所以可以忽略收到的其他事件，直到收到下单事件时将其匹配，放入结果集中，并且将当前状态往下转移到超时未接单状态。这时候结果集当中有两个事件：制定行程事件和下单事件。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105843389.png" alt="image-20210824105843389"></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105851550.png" alt="image-20210824105851550"></p><p>4.超时未接单状态时，如果来了一些其他事件，同样可以被 ignore 边忽略，直到超时事件的触发，将状态往下转移到最终状态，这时候整个模式匹配成功，最终将结果集中的制定行程事件和下单事件输出到下游。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824105952153.png" alt="image-20210824105952153"></p><p>上面是一个匹配成功的例子，如果是不成功的例子会怎么样？</p><p>假如当状态处于超时未接单状态时，收到了一个接单事件，那么就不符合超时未被接单的触发条件，此时整个模式匹配失败，之前放入结果集中的行程事件和下单事件会被清理。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110020762.png" alt="image-20210824110020762"></p><h2 id="Flink-CEP-程序开发"><a href="#Flink-CEP-程序开发" class="headerlink" title="Flink CEP 程序开发"></a>Flink CEP 程序开发</h2><p>本节将详细介绍 Flink CEP 的程序结构以及 API。</p><h3 id="1-Flink-CEP-程序结构"><a href="#1-Flink-CEP-程序结构" class="headerlink" title="1.Flink CEP 程序结构"></a>1.Flink CEP 程序结构</h3><p>主要分为两部分：定义事件模式和匹配结果处理。</p><p>官方示例如下：</p><pre class=" language-java"><code class="language-java">DataStream<span class="token operator">&lt;</span>Event<span class="token operator">></span> input <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>Pattern<span class="token operator">&lt;</span>Event<span class="token punctuation">,</span> <span class="token operator">?</span><span class="token operator">></span> pattern <span class="token operator">=</span> Pattern<span class="token punctuation">.</span>&lt;Event<span class="token operator">></span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getId</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">42</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token string">"middle"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">subtype</span><span class="token punctuation">(</span>SubEvent<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>SubEvent<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>SubEvent subEvent<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> subEvent<span class="token punctuation">.</span><span class="token function">getVolume</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">10.0</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">followedBy</span><span class="token punctuation">(</span><span class="token string">"end"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>         <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"end"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>         <span class="token punctuation">}</span>    <span class="token punctuation">)</span><span class="token punctuation">;</span>PatternStream<span class="token operator">&lt;</span>Event<span class="token operator">></span> patternStream <span class="token operator">=</span> CEP<span class="token punctuation">.</span><span class="token function">pattern</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> pattern<span class="token punctuation">)</span><span class="token punctuation">;</span>DataStream<span class="token operator">&lt;</span>Alert<span class="token operator">></span> result <span class="token operator">=</span> patternStream<span class="token punctuation">.</span><span class="token function">select</span><span class="token punctuation">(</span>    <span class="token keyword">new</span> <span class="token class-name">PatternProcessFunction</span><span class="token operator">&lt;</span>Event<span class="token punctuation">,</span> Alert<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">select</span><span class="token punctuation">(</span>                Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> List<span class="token operator">&lt;</span>Event<span class="token operator">>></span> pattern<span class="token punctuation">,</span>                Context ctx<span class="token punctuation">,</span>                Collector<span class="token operator">&lt;</span>Alert<span class="token operator">></span> out<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>            out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token function">createAlertFrom</span><span class="token punctuation">(</span>pattern<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong>程序结构分为三部分：</strong>首先需要定义一个模式(<strong>Pattern</strong>)，即第 2 行代码所示，接着把定义好的模式绑定在 DataStream 上（<strong>第 25 行</strong>），最后就可以在具有 CEP 功能的 DataStream 上将匹配的结果进行处理（<strong>第 27 行</strong>）。下面对关键部分做详细讲解：</p><ul><li><strong>定义模式：</strong>上面示例中，分为了三步，首先匹配一个 ID 为 42 的事件，接着匹配一个体积大于等于 10 的事件，最后等待收到一个 name 等于 end 的事件。</li><li><strong>匹配结果输出：</strong>此部分，需要重点注意 select 函数（第 30 行，注：本文基于 Flink 1.7 版本）里边的 Map 类型的 pattern 参数，Key 是一个 pattern 的 name，它的取值是模式定义中的 Begin 节点 start，或者是接下来 next 里面的 middle，或者是第三个步骤的 end。后面的 map 中的 value 是每一步发生的匹配事件。因在每一步中是可以使用循环属性的，可以匹配发生多次，所以 map 中的 value 是匹配发生多次的所有事件的一个集合。</li></ul><h3 id="2-Flink-CEP-构成"><a href="#2-Flink-CEP-构成" class="headerlink" title="2.Flink CEP 构成"></a>2.Flink CEP 构成</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110422728.png" alt="image-20210824110422728"></p><p>上图中，蓝色方框代表的是一个个单独的模式；浅黄色的椭圆代表的是这个模式上可以添加的属性，包括模式可以发生的循环次数，或者这个模式是贪婪的还是可选的；橘色的椭圆代表的是模式间的关系，定义了多个模式之间是怎么样串联起来的。通过定义模式，添加相应的属性，将多个模式串联起来三步，就可以构成了一个完整的 Flink CEP 程序。</p><h4 id="2-1-定义模式"><a href="#2-1-定义模式" class="headerlink" title="2.1 定义模式"></a>2.1 定义模式</h4><p>下面是示例代码：</p><pre class=" language-java"><code class="language-java">pattern<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>        <span class="token keyword">new</span> <span class="token class-name">SimpleCondition</span><span class="token operator">&lt;</span>Event<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token annotation punctuation">@Override</span>            <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">filter</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> event<span class="token punctuation">.</span><span class="token function">getId</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">42</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>定义模式主要有如下 5 个部分组成：</p><ul><li><strong>pattern</strong>：前一个模式</li><li><strong>next/followedBy/…</strong>：开始一个新的模式</li><li><strong>start</strong>：模式名称</li><li><strong>where</strong>：模式的内容</li><li><strong>filter</strong>：核心处理逻辑</li></ul><h4 id="2-2-模式的属性"><a href="#2-2-模式的属性" class="headerlink" title="2.2 模式的属性"></a>2.2 模式的属性</h4><p>接下来介绍一下怎样设置模式的属性。模式的属性主要分为循环属性和可选属性。</p><ul><li>循环属性可以定义模式匹配发生固定次数（<strong>times</strong>），匹配发生一次以上（<strong>oneOrMore</strong>），匹配发生多次以上(<strong>timesOrMore</strong>)。</li><li>可选属性可以设置模式是贪婪的（<strong>greedy</strong>），即匹配最长的串，或设置为可选的（<strong>optional</strong>），有则匹配，无则忽略。</li></ul><h4 id="2-3-模式的有效期"><a href="#2-3-模式的有效期" class="headerlink" title="2.3 模式的有效期"></a>2.3 模式的有效期</h4><p>由于模式的匹配事件存放在状态中进行管理，所以需要设置一个全局的有效期（within）。若不指定有效期，匹配事件会一直保存在状态中不会被清除。至于有效期能开多大，要依据具体使用场景和数据量来衡量，关键要看匹配的事件有多少，随着匹配的事件增多，新到达的消息遍历之前的匹配事件会增加 CPU、内存的消耗，并且随着状态变大，数据倾斜也会越来越严重。</p><h4 id="2-4-模式间的联系"><a href="#2-4-模式间的联系" class="headerlink" title="2.4 模式间的联系"></a>2.4 模式间的联系</h4><p>主要分为三种：严格连续性（next/notNext），宽松连续性（followedBy/notFollowedBy），和非确定宽松连续性（followedByAny）。</p><p>三种模式匹配的差别见下表所示：</p><table><thead><tr><th align="left"><strong>模式&amp;数据流</strong></th><th align="left"><strong>严格连续性</strong></th><th align="left"><strong>宽松连续性</strong></th><th align="left"><strong>非确定宽松连续性</strong></th></tr></thead><tbody><tr><td align="left">Pattern(A B) Streaming(‘a’,’c’,’b1′,’b2′)</td><td align="left">不匹配</td><td align="left">匹配 输出：a,b1</td><td align="left">匹配 输出：a,b1 a,b2</td></tr></tbody></table><p>总结如下：</p><ul><li><strong>严格连续性</strong>：需要消息的顺序到达与模式完全一致。</li><li><strong>宽松连续性</strong>：允许忽略不匹配的事件。</li><li><strong>非确定宽松连性</strong>：不仅可以忽略不匹配的事件，也可以忽略已经匹配的事件。</li></ul><h4 id="2-5-多模式组合"><a href="#2-5-多模式组合" class="headerlink" title="2.5 多模式组合"></a>2.5 多模式组合</h4><p>除了前面提到的模式定义和模式间的联系，还可以把相连的多个模式组合在一起看成一个模式组，类似于视图，可以在这个模式视图上进行相关操作。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824110937444.png" alt="image-20210824110937444"></p><p>上图这个例子里面，首先匹配了一个登录事件，然后接下来匹配浏览，下单，购买这三个事件反复发生三次的用户。</p><p>如果没有模式组的话，代码里面浏览，下单，购买要写三次。有了模式组，只需把浏览，下单，购买这三个事件当做一个模式组，把相应的属性加上 times(3) 就可以了。</p><h4 id="2-6-处理结果"><a href="#2-6-处理结果" class="headerlink" title="2.6 处理结果"></a>2.6 处理结果</h4><p>处理匹配的结果主要有四个接口：PatternFlatSelectFunction，PatternSelectFunction，PatternFlatTimeoutFunction 和 PatternTimeoutFunction。</p><p>从名字上可以看出，输出可以分为两类：select 和 flatSelect 指定输出一条还是多条，timeoutFunction 和不带 timeout 的 Function 指定可不可以对超时事件进行旁路输出。</p><p>下图是输出的综合示例代码：</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111048098.png" alt="image-20210824111048098"></p><h4 id="2-7-状态存储优化"><a href="#2-7-状态存储优化" class="headerlink" title="2.7 状态存储优化"></a>2.7 状态存储优化</h4><p>当一个事件到来时，如果这个事件同时符合多个输出的结果集，那么这个事件是如何保存的？</p><p>Flink CEP 通过 Dewey 计数法在多个结果集中共享同一个事件副本，以实现对事件副本进行资源共享。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111200461.png" alt="image-20210824111200461"></p><h2 id="Flink-CEP-的扩展"><a href="#Flink-CEP-的扩展" class="headerlink" title="Flink CEP 的扩展"></a>Flink CEP 的扩展</h2><p>本章主要介绍一些 Flink CEP 的扩展，讲述如何做到超时机制的精确管理，以及规则的动态加载与更新。</p><h3 id="1-超时触发机制扩展"><a href="#1-超时触发机制扩展" class="headerlink" title="1.超时触发机制扩展"></a>1.超时触发机制扩展</h3><p>原生 Flink CEP 中超时触发的功能可以通过 within+outputtag 结合来实现，但是在复杂的场景下处理存在问题，如下图所示，在下单事件后还有一个预付款事件，想要得到下单并且预付款后超时未被接单的订单，该如何表示呢？</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111320576.png" alt="image-20210824111320576"></p><p>参照下单后超时未被接单的做法，把下单并且预付款后超时未被接单规则表示为下单**.followedBy(预付款).followedBy(接单).within(time)**，那么这样实现会存在问题吗？</p><p>这种做法的计算结果是会存在脏数据的，因为这个规则不仅匹配到了下单并且预付款后超时未被接单的订单（想要的结果），同样还匹配到了只有下单行为后超时未被接单的订单（脏数据，没有预付款）。原因是因为超时 within 是控制在整个规则上，而不是某一个状态节点上，所以不论当前的状态是处在哪个状态节点，超时后都会被旁路输出。</p><p>那么就需要考虑能否通过时间来直接对状态转移做到精确的控制，而不是通过规则超时这种曲线救国的方式。于是乎，在通过消息触发状态的转移之外，需要增加通过时间触发状态的转移支持。要实现此功能，需要在原来的状态以及状态转移中，增加时间属性的概念。如下图所示，通过 wait 算子来得到 waiting 状态，然后在 waiting 状态上设置一个十秒的时间属性以定义一个十秒的时间窗口。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111450980.png" alt="image-20210824111450980"></p><p>wait 算子对应 NFA 中的 ignore 状态，将在没有到达时间窗口结束时间时自旋，在 ComputationState 中记录 wait 的开始时间，在 NFA 的 doProcess 中，将到来的数据与waiting 状态处理，如果到了 waiting 的结束时间，则进行状态转移。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111558511.png" alt="image-20210824111558511"></p><p>上图中红色方框中为 waiting 状态设置了两条 ignore 边：</p><ol><li>waitingStatus.addIgnore(lastSink,waitingCondition)，waitingCondition 中的逻辑是获取当前的时间（支持事件时间），判断有没有超过设置的 waiting 阈值，如果超过就把状态向后转移。</li><li>waitingStatus.addIgnore(waitingCondition)，waitingCondition 中如果未达到设置的 waiting 阈值，就会自旋在当前的 waiting 状态不变。</li></ol><h3 id="2-规则动态注入"><a href="#2-规则动态注入" class="headerlink" title="2.规则动态注入"></a>2.规则动态注入</h3><p>线上运行的 CEP 中肯定经常遇到规则变更的情况，如果每次变更时都将任务重启、重新发布是非常不优雅的。尤其在营销或者风控这种对实时性要求比较高的场景，如果规则窗口过长（一两个星期），状态过大，就会导致重启时间延长，期间就会造成一些想要处理的异常行为不能及时发现。</p><p>那么要怎么样做到规则的动态更新和加载呢？</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111719312.png" alt="image-20210824111719312"></p><p>梳理一下整体架构，Flink CEP 是运行在 Flink Job 里的，而规则库是放在外部存储中的。首先，需要在运行的 Job 中能及时发现外部存储中规则的变化，即需要在 Job 中提供访问外部库的能力。其次，需要将规则库中变更的规则动态加载到 CEP 中，即把外部规则的描述解析成 Flink CEP 所能识别的 pattern 结构体。最后，把生成的 pattern 转化成 NFA，替换历史 NFA，这样对新到来的消息，就会使用新的规则进行匹配。</p><p>下图就是一个支持将外部规则动态注入、更新的接口。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111806820.png" alt="image-20210824111806820"></p><p>这个接口里面主要实现了四个方法：</p><ul><li><strong>initialize</strong>：初始化方法，进行外部库连接的初始化。</li><li><strong>inject</strong>：和外部数据库交互的主要方法，监听外部库变化，获取最新的规则并通过 Groovy 动态加载，返回 pattern。</li><li><strong>getPeriod</strong>：设置轮巡周期，在一些比较简单的实时性要求不高的场景，可以采用轮巡的方式，定期对外部数据库进行检测。</li><li><strong>getNfaKeySelector</strong>：和动态更新无关，用来支持一个流对应多个规则组。</li></ul><h3 id="3-历史匹配结果清理"><a href="#3-历史匹配结果清理" class="headerlink" title="3.历史匹配结果清理"></a>3.历史匹配结果清理</h3><p>新规则动态加载到 Flink CEP 的 Job 中，替换掉原来的 NFA 之后，还需要对历史匹配的结果集进行清理。在 AbstractKeyedCEPPatternOperator 中实现刷新 NFA，注意，历史状态是否需要清理和业务相关：</p><ol><li>修改的逻辑对规则中事件的匹配没有影响，保留历史结果集中的状态。</li><li>修改的逻辑影响到了之前匹配的部分，需要将之前匹配的结果集中的状态数据清除，防止错误的输出。</li></ol><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111921774.png" alt="image-20210824111921774"></p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824111953602.png" alt="image-20210824111953602"></p><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>使用 Flink CEP，熟知其原理是很重要的，特别是 NFA 的状态转移流程，然后再去看源码中的状态图的构建就会很清晰了。</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语日常学习</title>
      <link href="/2021/08/23/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/23/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="英语日常学习"><a href="#英语日常学习" class="headerlink" title="英语日常学习"></a>英语日常学习</h1><h2 id="生活最常用英语口语"><a href="#生活最常用英语口语" class="headerlink" title="生活最常用英语口语"></a>生活最常用英语口语</h2><p><a href="https://www.youtube.com/watch?v=EXMTbXdQ63Q" target="_blank" rel="noopener">https://www.youtube.com/watch?v=EXMTbXdQ63Q</a></p><pre class=" language-properties"><code class="language-properties">1、今天的天气如何<span class="token attr-name">The</span> <span class="token attr-value">Weather is very good.</span>2、今天的天气很糟糕<span class="token attr-name">The</span> <span class="token attr-value">Weather is very bad.</span>3、今天的天气如何<span class="token attr-name">How</span> <span class="token attr-value">is the weather today?</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 英语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive高级进阶</title>
      <link href="/2021/08/23/Hive%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
      <url>/2021/08/23/Hive%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive高级优化"><a href="#Hive高级优化" class="headerlink" title="Hive高级优化"></a>Hive高级优化</h1><h2 id="第-1-章-Explain-查看执行计划（重点）"><a href="#第-1-章-Explain-查看执行计划（重点）" class="headerlink" title="第 1 章 Explain 查看执行计划（重点）"></a>第 1 章 Explain 查看执行计划（重点）</h2><h3 id="1-1-创建测试用表"><a href="#1-1-创建测试用表" class="headerlink" title="1.1  创建测试用表"></a>1.1  创建测试用表</h3><h4 id="1）建大表、小表和-JOIN-后表的语句"><a href="#1）建大表、小表和-JOIN-后表的语句" class="headerlink" title="1）建大表、小表和 JOIN 后表的语句"></a>1）建大表、小表和 JOIN 后表的语句</h4><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">// 创建大表</span><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 创建小表</span><span class="token keyword">create</span> <span class="token keyword">table</span> smalltable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 创建 JOIN 后表</span><span class="token keyword">create</span> <span class="token keyword">table</span> jointable             <span class="token punctuation">(</span>                          id <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> t  <span class="token keyword">bigint</span>                        <span class="token punctuation">,</span> uid string                        <span class="token punctuation">,</span> keyword string                        <span class="token punctuation">,</span> url_rank  <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_num <span class="token keyword">int</span>                        <span class="token punctuation">,</span> click_url string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><h4 id="2）分别向大表和小表中导入数据"><a href="#2）分别向大表和小表中导入数据" class="headerlink" title="2）分别向大表和小表中导入数据"></a>2）分别向大表和小表中导入数据</h4><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">load</span> <span class="token attr-value">data local inpath '/opt/flink/hive/data/bigtable' into table bigtable;</span><span class="token attr-name">load</span> <span class="token attr-value">data local inpath '/opt/flink/hive/data/smalltable' into table smalltable;</span></code></pre><h3 id="1-2-基本语法"><a href="#1-2-基本语法" class="headerlink" title="1.2 基本语法"></a>1.2 基本语法</h3><pre class=" language-sql"><code class="language-sql"><span class="token keyword">EXPLAIN</span> <span class="token punctuation">[</span><span class="token keyword">EXTENDED</span> <span class="token operator">|</span> DEPENDENCY <span class="token operator">|</span> <span class="token keyword">AUTHORIZATION</span><span class="token punctuation">]</span> query<span class="token operator">-</span>sql</code></pre><h3 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h3><h4 id="1）查看下面这条语句的执行计划"><a href="#1）查看下面这条语句的执行计划" class="headerlink" title="1）查看下面这条语句的执行计划"></a>1）查看下面这条语句的执行计划</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> bigtable<span class="token punctuation">;</span><span class="token keyword">explain</span> <span class="token keyword">select</span> click_url<span class="token punctuation">,</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> ct <span class="token keyword">from</span> bigtable <span class="token keyword">group</span> <span class="token keyword">by</span> click_url<span class="token punctuation">;</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">hive</span> <span class="token attr-value">(wmy)> explain select * from bigtable;</span>OKExplain<span class="token attr-name">STAGE</span> <span class="token attr-value">DEPENDENCIES: 阶段的依赖关系</span><span class="token attr-name">  Stage-0</span> <span class="token attr-value">is a root stage</span><span class="token attr-name">STAGE</span> <span class="token attr-value">PLANS:</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-0 </span><span class="token attr-name">    Fetch</span> <span class="token attr-value">Operator 抓取操作</span><span class="token attr-name">      limit</span><span class="token punctuation">:</span> <span class="token attr-value">-1 没有做任何的限制</span><span class="token attr-name">      Processor</span> <span class="token attr-value">Tree: 操作树</span><span class="token attr-name">        TableScan</span> <span class="token attr-value">扫描的表</span><span class="token attr-name">          alias</span><span class="token punctuation">:</span> <span class="token attr-value">bigtable</span><span class="token attr-name">          Select</span> <span class="token attr-value">Operator 操作列表</span><span class="token attr-name">            expressions</span><span class="token punctuation">:</span> <span class="token attr-value">id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)</span><span class="token attr-name">            outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1, _col2, _col3, _col4, _col5, _col6 输出的列名</span>            ListSink<span class="token attr-name">Time</span> <span class="token attr-value">taken: 0.167 seconds, Fetched: 15 row(s)</span></code></pre><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">hive</span> <span class="token attr-value">(wmy)> explain select click_url, count(*) ct from bigtable group by click_url;</span>OKExplain<span class="token attr-name">STAGE</span> <span class="token attr-value">DEPENDENCIES:</span><span class="token attr-name">  Stage-1</span> <span class="token attr-value">is a root stage 根阶段</span><span class="token attr-name">  Stage-0</span> <span class="token attr-value">depends on stages: Stage-1 依赖于第一阶段</span><span class="token attr-name">STAGE</span> <span class="token attr-value">PLANS:</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-1 MR任务</span>    Spark<span class="token attr-name">      Edges</span><span class="token punctuation">:</span><span class="token attr-name">        Reducer</span> <span class="token attr-value">2 &lt;- Map 1 (GROUP, 11)</span><span class="token attr-name">      DagName</span><span class="token punctuation">:</span> <span class="token attr-value">root_20210824125146_d0b9a768-686b-48c3-8e62-cfdb35023df2:2</span><span class="token attr-name">      Vertices</span><span class="token punctuation">:</span><span class="token attr-name">        Map</span> <span class="token attr-value">1 </span><span class="token attr-name">            Map</span> <span class="token attr-value">Operator Tree:</span>                TableScan<span class="token attr-name">                  alias</span><span class="token punctuation">:</span> <span class="token attr-value">bigtable</span><span class="token attr-name">                  Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                  Select</span> <span class="token attr-value">Operator</span><span class="token attr-name">                    expressions</span><span class="token punctuation">:</span> <span class="token attr-value">click_url (type: string)</span><span class="token attr-name">                    outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">click_url</span><span class="token attr-name">                    Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE 这个是内部直接给定的1</span><span class="token attr-name">                    Group</span> <span class="token attr-value">By Operator</span><span class="token attr-name">                      aggregations</span><span class="token punctuation">:</span> <span class="token attr-value">count()</span><span class="token attr-name">                      keys</span><span class="token punctuation">:</span> <span class="token attr-value">click_url (type: string)</span><span class="token attr-name">                      mode</span><span class="token punctuation">:</span> <span class="token attr-value">hash</span><span class="token attr-name">                      outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1</span><span class="token attr-name">                      Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                      Reduce</span> <span class="token attr-value">Output Operator</span><span class="token attr-name">                        key</span> <span class="token attr-value">expressions: _col0 (type: string)</span><span class="token attr-name">                        sort</span> <span class="token attr-value">order: + 正序排序</span><span class="token attr-name">                        Map-reduce</span> <span class="token attr-value">partition columns: _col0 (type: string)</span><span class="token attr-name">                        Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                        value</span> <span class="token attr-value">expressions: _col1 (type: bigint)</span><span class="token attr-name">            Execution</span> <span class="token attr-value">mode: vectorized</span><span class="token attr-name">        Reducer</span> <span class="token attr-value">2 </span><span class="token attr-name">            Execution</span> <span class="token attr-value">mode: vectorized</span><span class="token attr-name">            Reduce</span> <span class="token attr-value">Operator Tree:</span><span class="token attr-name">              Group</span> <span class="token attr-value">By Operator</span><span class="token attr-name">                aggregations</span><span class="token punctuation">:</span> <span class="token attr-value">count(VALUE._col0) 做一个累加的结果</span><span class="token attr-name">                keys</span><span class="token punctuation">:</span> <span class="token attr-value">KEY._col0 (type: string)</span><span class="token attr-name">                mode</span><span class="token punctuation">:</span> <span class="token attr-value">mergepartial</span><span class="token attr-name">                outputColumnNames</span><span class="token punctuation">:</span> <span class="token attr-value">_col0, _col1</span><span class="token attr-name">                Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                File</span> <span class="token attr-value">Output Operator</span><span class="token attr-name">                  compressed</span><span class="token punctuation">:</span> <span class="token attr-value">false</span><span class="token attr-name">                  Statistics</span><span class="token punctuation">:</span> <span class="token attr-value">Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE</span><span class="token attr-name">                  table</span><span class="token punctuation">:</span><span class="token attr-name">                      input</span> <span class="token attr-value">format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><span class="token attr-name">                      output</span> <span class="token attr-value">format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><span class="token attr-name">                      serde</span><span class="token punctuation">:</span> <span class="token attr-value">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><span class="token attr-name">  Stage</span><span class="token punctuation">:</span> <span class="token attr-value">Stage-0</span><span class="token attr-name">    Fetch</span> <span class="token attr-value">Operator</span><span class="token attr-name">      limit</span><span class="token punctuation">:</span> <span class="token attr-value">-1 将全部的结果给打印</span><span class="token attr-name">      Processor</span> <span class="token attr-value">Tree:</span>        ListSink<span class="token attr-name">Time</span> <span class="token attr-value">taken: 0.223 seconds, Fetched: 56 row(s)</span></code></pre><p>从简单的SQL了解起，写hive sql ，如果不用hive sql来进行实现，写MR程序如何实现，执行计划就是将hive sql翻译成MR的程序</p><p>是如何翻译成MR任务的</p><h4 id="2）查看详细执行计划"><a href="#2）查看详细执行计划" class="headerlink" title="2）查看详细执行计划"></a>2）查看详细执行计划</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">extended</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> bigtable<span class="token punctuation">;</span><span class="token keyword">explain</span> <span class="token keyword">extended</span> <span class="token keyword">select</span> click_url<span class="token punctuation">,</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> ct <span class="token keyword">from</span> bigtable <span class="token keyword">group</span> <span class="token keyword">by</span> click_url<span class="token punctuation">;</span></code></pre><p>一般的我们是不会去这样使用，多出的部分的数据我们并不是很关心的</p><h2 id="第-2-章-Hive-建表优化"><a href="#第-2-章-Hive-建表优化" class="headerlink" title="第 2 章 Hive 建表优化"></a>第 2 章 Hive 建表优化</h2><h3 id="2-1-分区表"><a href="#2-1-分区表" class="headerlink" title="2.1 分区表"></a>2.1 分区表</h3><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所 有的数据文件。</p><p>Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。 </p><p>在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率 会提高很多，所以我们需要把常常用在 WHERE 语句中的字段指定为表的分区字段。</p><h4 id="2-1-1-分区表基本操作"><a href="#2-1-1-分区表基本操作" class="headerlink" title="2.1.1 分区表基本操作"></a>2.1.1 分区表基本操作</h4><h5 id="1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）"><a href="#1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）" class="headerlink" title="1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）"></a>1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）</h5><pre class=" language-properties"><code class="language-properties">dept_20200401.logdept_20200402.logdept_20200403.log</code></pre><h5 id="2）创建分区表语法"><a href="#2）创建分区表语法" class="headerlink" title="2）创建分区表语法"></a>2）创建分区表语法</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition             <span class="token punctuation">(</span>                          deptno <span class="token keyword">int</span>                        <span class="token punctuation">,</span> dname string                        <span class="token punctuation">,</span> loc string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          day string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p><h5 id="3）加载数据到分区表中"><a href="#3）加载数据到分区表中" class="headerlink" title="3）加载数据到分区表中"></a>3）加载数据到分区表中</h5><h6 id="（1）数据准备"><a href="#（1）数据准备" class="headerlink" title="（1）数据准备"></a>（1）数据准备</h6><p>dept_20200401.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">10</span> <span class="token attr-value">ACCOUNTING 1700</span><span class="token attr-name">20</span> <span class="token attr-value">RESEARCH 1800</span></code></pre><p>dept_20200402.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">30</span> <span class="token attr-value">SALES 1900</span><span class="token attr-name">40</span> <span class="token attr-value">OPERATIONS 1700</span></code></pre><p>dept_20200403.log</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">50</span> <span class="token attr-value">TEST 2000</span><span class="token attr-name">60</span> <span class="token attr-value">DEV 1900</span></code></pre><h6 id="（2）加载数据"><a href="#（2）加载数据" class="headerlink" title="（2）加载数据"></a>（2）加载数据</h6><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200401.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200402.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200402'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200403.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200403'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>注意：分区表加载数据时，必须指定分区</p><h5 id="4）查询分区表中数据"><a href="#4）查询分区表中数据" class="headerlink" title="4）查询分区表中数据"></a>4）查询分区表中数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 单分区查询</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 多分区查询</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token keyword">union</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200402'</span><span class="token keyword">union</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token keyword">from</span>       dept_partition<span class="token keyword">where</span>       day<span class="token operator">=</span><span class="token string">'20200403'</span><span class="token punctuation">;</span></code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824131047329.png" alt="分区中的数据"></p><h5 id="5）增加分区"><a href="#5）增加分区" class="headerlink" title="5）增加分区"></a>5）增加分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 增加单个分区</span><span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200404'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 增加多个分区</span> <span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200405'</span><span class="token punctuation">)</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200406'</span><span class="token punctuation">)</span> <span class="token punctuation">;</span></code></pre><h5 id="6）删除分区"><a href="#6）删除分区" class="headerlink" title="6）删除分区"></a>6）删除分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 删除单个分区</span><span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200406'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 删除多个分区</span> <span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition <span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200404'</span><span class="token punctuation">)</span>           <span class="token punctuation">,</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>day                     <span class="token operator">=</span><span class="token string">'20200405'</span><span class="token punctuation">)</span> <span class="token punctuation">;</span></code></pre><h5 id="7）查看分区表有多少分区"><a href="#7）查看分区表有多少分区" class="headerlink" title="7）查看分区表有多少分区"></a>7）查看分区表有多少分区</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">show</span> partitions dept_partition<span class="token punctuation">;</span></code></pre><h5 id="8）查看分区表结构"><a href="#8）查看分区表结构" class="headerlink" title="8）查看分区表结构"></a>8）查看分区表结构</h5><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">desc</span> formatted dept_partition<span class="token punctuation">;</span></code></pre><p>思考: 如果一天的日志数据量也很大，如何再将数据拆分?</p><h4 id="2-1-2-二级分区"><a href="#2-1-2-二级分区" class="headerlink" title="2.1.2 二级分区"></a>2.1.2 二级分区</h4><h5 id="1）创建二级分区表"><a href="#1）创建二级分区表" class="headerlink" title="1）创建二级分区表"></a>1）创建二级分区表</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition2             <span class="token punctuation">(</span>                          deptno <span class="token keyword">int</span>                        <span class="token punctuation">,</span> dname string                        <span class="token punctuation">,</span> loc string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          day string                        <span class="token punctuation">,</span> hour string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><h5 id="2）正常的加载数据"><a href="#2）正常的加载数据" class="headerlink" title="2）正常的加载数据"></a>2）正常的加载数据</h5><h5 id="（1）加载数据到二级分区表中"><a href="#（1）加载数据到二级分区表中" class="headerlink" title="（1）加载数据到二级分区表中"></a>（1）加载数据到二级分区表中</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/dept_20200401.log'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition2 <span class="token keyword">partition</span><span class="token punctuation">(</span>day<span class="token operator">=</span><span class="token string">'20200401'</span><span class="token punctuation">,</span> hour<span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h5 id="（2）查询分区数据"><a href="#（2）查询分区数据" class="headerlink" title="（2）查询分区数据"></a>（2）查询分区数据</h5><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span>        dept_partition2 <span class="token keyword">where</span>        day     <span class="token operator">=</span><span class="token string">'20200401'</span>        <span class="token operator">and</span> hour<span class="token operator">=</span><span class="token string">'12'</span> <span class="token punctuation">;</span></code></pre><h4 id="2-1-3-动态分区"><a href="#2-1-3-动态分区" class="headerlink" title="2.1.3 动态分区"></a>2.1.3 动态分区</h4><p>关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据 插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过， 使用 Hive 的动态分区，需要进行相应的配置。</p><h5 id="1）开启动态分区参数设置"><a href="#1）开启动态分区参数设置" class="headerlink" title="1）开启动态分区参数设置"></a>1）开启动态分区参数设置</h5><p>（1）开启动态分区功能（默认 true，开启）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为 静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>mode<span class="token operator">=</span>nonstrict<span class="token punctuation">;</span></code></pre><p>（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">;</span></code></pre><p>（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。</p><p>该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token punctuation">.</span>pernode<span class="token operator">=</span><span class="token number">100</span></code></pre><p>（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>created<span class="token punctuation">.</span>files<span class="token operator">=</span><span class="token number">100000</span></code></pre><p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>error<span class="token punctuation">.</span><span class="token keyword">on</span><span class="token punctuation">.</span>empty<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">false</span></code></pre><h5 id="2）案例实操"><a href="#2）案例实操" class="headerlink" title="2）案例实操"></a>2）案例实操</h5><p>需求：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分 区中。</p><p>（1）创建目标分区表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition_dy             <span class="token punctuation">(</span>                          id <span class="token keyword">int</span>                        <span class="token punctuation">,</span> name string             <span class="token punctuation">)</span>             partitioned <span class="token keyword">by</span>             <span class="token punctuation">(</span>                          loc string             <span class="token punctuation">)</span>             <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>（2）设置动态分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>mode <span class="token operator">=</span> nonstrict<span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition_dy <span class="token keyword">partition</span>       <span class="token punctuation">(</span>loc       <span class="token punctuation">)</span><span class="token keyword">select</span>       deptno     <span class="token punctuation">,</span> dname     <span class="token punctuation">,</span> loc<span class="token keyword">from</span>       dept<span class="token punctuation">;</span></code></pre><p>（3）查看目标分区表的分区情况</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">show</span> partitions dept_partition<span class="token punctuation">;</span></code></pre><p>记住，使用分区表，分区表的里面的字段和查询出来的字段的类型是必须要一致的，否则的话会出现问题的</p><h3 id="2-2-分桶表"><a href="#2-2-分桶表" class="headerlink" title="2.2 分桶表"></a>2.2 分桶表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理 的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围 划分。 </p><p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。分区针对的是数据的存储 路径，分桶针对的是数据文件。</p><h4 id="2-2-1-创建分桶表"><a href="#2-2-1-创建分桶表" class="headerlink" title="2.2.1 创建分桶表"></a>2.2.1 创建分桶表</h4><p>（1）数据准备</p><pre class=" language-properties"><code class="language-properties">1001,ss11002,ss21003,ss31004,ss41005,ss51006,ss61007,ss71008,ss81009,ss91010,ss101011,ss111012,ss121013,ss131014,ss141015,ss151016,ss16</code></pre><p>（2）创建分桶表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">4</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">','</span><span class="token punctuation">;</span></code></pre><p>（3）查看表结构</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">desc</span> formatted stu_buck<span class="token punctuation">;</span></code></pre><p>（4）导入数据到分桶表中，load 的方式</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/student.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">;</span></code></pre><p>（5）查看创建的分桶表中是否分成 4 个桶</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210824141808132.png" alt="image-20210824141808132"></p><p>（6）查询分桶的数据</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> stu_buck<span class="token punctuation">;</span></code></pre><p>（7）分桶规则：</p><p>根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方 式决定该条记录存放在哪个桶当中</p><p>2）分桶表操作需要注意的事项</p><p>（1）reduce 的个数设置为-1，让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个 数设置为大于等于分桶表的桶数 </p><p>（2）从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题 </p><p>（3）不要使用本地模式</p><p>3）insert 方式将数据导入分桶表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student_insert<span class="token punctuation">;</span></code></pre><h4 id="2-2-2-抽样查询"><a href="#2-2-2-抽样查询" class="headerlink" title="2.2.2 抽样查询"></a>2.2.2 抽样查询</h4><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结 果。</p><p>Hive 可以通过对表进行抽样来满足这个需求。 </p><p>语法: TABLESAMPLE(BUCKET x OUT OF y) 查询表 stu_buck 中的数据。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> stu_buck tablesample<span class="token punctuation">(</span>bucket <span class="token number">1</span> <span class="token keyword">out</span> <span class="token keyword">of</span> <span class="token number">4</span> <span class="token keyword">on</span> id<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>注意：x 的值必须小于等于 y 的值，否则</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">FAILED</span><span class="token punctuation">:</span> <span class="token attr-value">SemanticException [Error 10061]: Numerator should not be bigger</span><span class="token attr-name">than</span> <span class="token attr-value">denominator in sample clause for table stu_buck</span></code></pre><h3 id="2-3-合适的文件格式"><a href="#2-3-合适的文件格式" class="headerlink" title="2.3 合适的文件格式"></a>2.3 合适的文件格式</h3><p>Hive 支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h4 id="2-3-1-列式存储和行式存储"><a href="#2-3-1-列式存储和行式存储" class="headerlink" title="2.3.1 列式存储和行式存储"></a>2.3.1 列式存储和行式存储</h4><p>1）行存储的特点</p><p>​    查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列 的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度 更快。</p><p>​    TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；</p><p>2）列存储的特点</p><p>​    因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的 数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算 法。</p><p>​    ORC 和 PARQUET 是基于列式存储的。</p><h4 id="2-3-2-TextFile格式"><a href="#2-3-2-TextFile格式" class="headerlink" title="2.3.2 TextFile格式"></a>2.3.2 TextFile格式</h4><p>​    默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合 Gzip、Bzip2 使用， 但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。</p><h4 id="2-3-3-Orc格式"><a href="#2-3-3-Orc格式" class="headerlink" title="2.3.3 Orc格式"></a>2.3.3 Orc格式</h4><p>​    Orc (Optimized Row Columnar)是 Hive 0.11 版里引入的新的存储格式。</p><h4 id="2-3-4-Parquet格式"><a href="#2-3-4-Parquet格式" class="headerlink" title="2.3.4 Parquet格式"></a>2.3.4 Parquet格式</h4><p>​    Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的 数据和元数据，因此 Parquet 格式文件是自解析的。</p><h3 id="2-4-合适的压缩格式"><a href="#2-4-合适的压缩格式" class="headerlink" title="2.4 合适的压缩格式"></a>2.4 合适的压缩格式</h3><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825131543316.png" alt="压缩数据格式"></p><p>为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器，如下表所示。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825131735589.png" alt="编码和解码器"></p><p>压缩性能的比较</p><p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">Snappy压缩算法</a></p><p>在64位模式的酷睿i7处理器的单核上，Snappy压缩速度约为250mb /秒，解压缩速度约为500mb /秒。  </p><h2 id="第-3-章-HQL-语法优化"><a href="#第-3-章-HQL-语法优化" class="headerlink" title="第 3 章 HQL 语法优化"></a>第 3 章 HQL 语法优化</h2><h3 id="3-1-列裁剪与分区裁剪"><a href="#3-1-列裁剪与分区裁剪" class="headerlink" title="3.1 列裁剪与分区裁剪"></a>3.1 列裁剪与分区裁剪</h3><p>​    列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者 数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。 Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做 可以节省读取开销：中间表存储开销和数据整合开销。</p><h3 id="3-2-Group-By"><a href="#3-2-Group-By" class="headerlink" title="3.2 Group By"></a>3.2 Group By</h3><p>​    默认情况下，Map 阶段同一 Key 数据分发给一个 Reduce，当一个 key 数据过大时就倾 斜了。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825132110865.png" alt="数据倾斜"></p><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行 部分聚合，最后在 Reduce 端得出最终结果。</p><h4 id="3-2-1-开启-Map-端聚合参数设置"><a href="#3-2-1-开启-Map-端聚合参数设置" class="headerlink" title="3.2.1 开启 Map 端聚合参数设置"></a>3.2.1 开启 Map 端聚合参数设置</h4><p>（1）是否在 Map 端进行聚合，默认为 True</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.map.aggr = true;</span></code></pre><p>（2）在 Map 端进行聚合操作的条目数目</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.groupby.mapaggr.checkinterval = 100000;</span></code></pre><p>（3）有数据倾斜的时候进行负载均衡（默认是 false）</p><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">set</span> <span class="token attr-value">hive.groupby.skewindata = true;</span></code></pre><p>​    当选项设定为 true，生成的查询计划会有两个 MR Job。</p><p>​    第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合 操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；</p><p>​    第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然 能解决数据倾斜，但是不能让运行速度的更快）。</p><h3 id="3-3-Vectorization"><a href="#3-3-Vectorization" class="headerlink" title="3.3 Vectorization"></a>3.3 Vectorization</h3><p>​    vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization 技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210825133949490.png" alt="矢量查询"></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><h3 id="3-4-多重模式"><a href="#3-4-多重模式" class="headerlink" title="3.4 多重模式"></a>3.4 多重模式</h3><p>​    如果你碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不 同的逻辑。有可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">from</span> student<span class="token keyword">insert</span> <span class="token keyword">int</span> t_ptn <span class="token keyword">partition</span><span class="token punctuation">(</span>city<span class="token operator">=</span>A<span class="token punctuation">)</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span>name<span class="token punctuation">,</span>sex<span class="token punctuation">,</span> age <span class="token keyword">where</span> city<span class="token operator">=</span> A<span class="token keyword">insert</span> <span class="token keyword">int</span> t_ptn <span class="token keyword">partition</span><span class="token punctuation">(</span>city<span class="token operator">=</span>B<span class="token punctuation">)</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span>name<span class="token punctuation">,</span>sex<span class="token punctuation">,</span> age <span class="token keyword">where</span> city<span class="token operator">=</span> B</code></pre><p>​    如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高， 多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后， 要多次利用。</p><h3 id="3-5-in-exists-语句"><a href="#3-5-in-exists-语句" class="headerlink" title="3.5 in/exists 语句"></a>3.5 in/exists 语句</h3><p>​    在 Hive 的早期版本中，in/exists 语法是不被支持的，但是从 hive-0.8x 以后就开始支持 这个语法。但是不推荐使用这个语法。虽然经过测验，Hive-2.3.6 也支持 in/exists 操作，但 还是推荐使用 Hive 的一个高效替代方案：left semi join</p><p>​    比如说：– in / exists 实现    </p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">in</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token number">b</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>可以使用 join 来改写：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>应该转换成： – left semi join 实现</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">left</span> semi <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-6-CBO-优化"><a href="#3-6-CBO-优化" class="headerlink" title="3.6 CBO 优化"></a>3.6 CBO 优化</h3><p>join 的时候表的顺序的关系：前面的表都会被加载到内存中。后面的表进行磁盘扫描</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">c</span><span class="token punctuation">.</span><span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">join</span> <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">join</span> <span class="token number">c</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">c</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>​    Hive 自 0.14.0 开始，加入了一项 “Cost based Optimizer” 来对 HQL 执行计划进行优化， 这个功能通过 “hive.cbo.enable” 来开启。    在 Hive 1.1.0 之后，这个 feature 是默认开启的， 它可以 自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。 CBO，成本优化器，代价最小的执行计划就是最好的执行计划。传统的数据库，成本优 化器做出最优化的执行计划是依据统计信息来计算的。 Hive 的成本优化器也一样，Hive 在提供最终执行前，优化每个查询的执行逻辑和物理 执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生 潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。</p><p>​    要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>cbo<span class="token punctuation">.</span><span class="token keyword">enable</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">compute</span><span class="token punctuation">.</span>query<span class="token punctuation">.</span><span class="token keyword">using</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">column</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><h3 id="3-7-谓词下推"><a href="#3-7-谓词下推" class="headerlink" title="3.7 谓词下推"></a>3.7 谓词下推</h3><p>​    将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻 辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。 案例实操：</p><p>1）打开谓词下推优化属性</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>ppd <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 谓词下推，默认是 true</span></code></pre><p>2）查看先关联两张表，再用 where 条件过滤的执行计划</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> o<span class="token punctuation">.</span>id <span class="token keyword">from</span> bigtable <span class="token number">b</span> <span class="token keyword">join</span> bigtable o <span class="token keyword">on</span> o<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> o<span class="token punctuation">.</span>id <span class="token operator">&lt;=</span> <span class="token number">10</span><span class="token punctuation">;</span></code></pre><p>3）查看子查询后，再关联表的执行计划</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">from</span> bigtable <span class="token number">b</span> <span class="token keyword">join</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> bigtable <span class="token keyword">where</span> id <span class="token operator">&lt;=</span> <span class="token number">10</span><span class="token punctuation">)</span> o <span class="token keyword">on</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">=</span> o<span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-8-MapJoin"><a href="#3-8-MapJoin" class="headerlink" title="3.8 MapJoin"></a>3.8 MapJoin</h3><p>​    MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map 端进行 Join，避免 Reducer 处理。</p><p>1）开启 MapJoin 参数设置</p><p>（1）设置自动选择 MapJoin</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 默认为 true</span></code></pre><p>（2）大表小表的阈值设置（默认 25M 以下认为是小表）：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>smalltable<span class="token punctuation">.</span>filesize<span class="token operator">=</span><span class="token number">25000000</span><span class="token punctuation">;</span></code></pre><p>2）MapJoin 工作机制</p><p>​    MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进 程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</p><p>3）案例实操</p><p>（1）开启 MapJoin 功能</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span> <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 默认为 true</span></code></pre><p>（2）执行小表 JOIN 大表语句</p><p>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin 失效</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">explain</span> <span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> smalltable s<span class="token keyword">left</span> <span class="token keyword">join</span> bigtable <span class="token number">b</span><span class="token keyword">on</span> s<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>（3）执行大表 JOIN 小表语句</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">Explain</span> <span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable <span class="token number">b</span><span class="token keyword">left</span> <span class="token keyword">join</span> smalltable s<span class="token keyword">on</span> s<span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><h3 id="3-9-大表、大表-SMB-Join（重点）"><a href="#3-9-大表、大表-SMB-Join（重点）" class="headerlink" title="3.9 大表、大表 SMB Join（重点）"></a>3.9 大表、大表 SMB Join（重点）</h3><p>SMB Join ：Sort Merge Bucket Join</p><p>1）创建第二张大表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable2<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable2<span class="token punctuation">;</span></code></pre><p>2）测试大表直接 JOIN</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable <span class="token number">a</span><span class="token keyword">join</span> bigtable2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>3）创建分通表 1</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable_buck1<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>sorted <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">6</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable_buck1<span class="token punctuation">;</span></code></pre><p>4）创建分通表 2，分桶数和第一张表的分桶数为倍数关系</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> bigtable_buck2<span class="token punctuation">(</span> id <span class="token keyword">bigint</span><span class="token punctuation">,</span> t <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span><span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>sorted <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token keyword">into</span> <span class="token number">6</span> buckets<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/flink/hive/data/bigtable'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> bigtable_buck2<span class="token punctuation">;</span></code></pre><p>5）设置参数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin<span class="token punctuation">.</span>sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>BucketizedHiveInputFormat<span class="token punctuation">;</span></code></pre><p>6）测试 Time taken: 34.685 seconds</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> jointable<span class="token keyword">select</span> <span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>t<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>uid<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>keyword<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>url_rank<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_num<span class="token punctuation">,</span> <span class="token number">b</span><span class="token punctuation">.</span>click_url<span class="token keyword">from</span> bigtable_buck1 s<span class="token keyword">join</span> bigtable_buck2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">=</span> s<span class="token punctuation">.</span>id<span class="token punctuation">;</span></code></pre><p>3.10 笛卡尔积</p><p>​    Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用 1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode=strict，nonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p><h2 id="第-4-章-数据倾斜（重点）"><a href="#第-4-章-数据倾斜（重点）" class="headerlink" title="第 4 章 数据倾斜（重点）"></a>第 4 章 数据倾斜（重点）</h2><p>​    绝大部分任务都很快完成，只有一个或者少数几个任务执行的很慢甚至最终执行失败， 这样的现象为数据倾斜现象。 </p><p>​    一定要和数据过量导致的现象区分开，数据过量的表现为所有任务都执行的很慢，这个 时候只有提高执行资源才可以优化 HQL 的执行效率。 </p><p>​    综合来看，导致数据倾斜的原因在于按照 Key 分组以后，少量的任务负责绝大部分数据 的计算，也就是说产生数据倾斜的 HQL 中一定存在分组操作，那么从 HQL 的角度，我们可 以将数据倾斜分为单表携带了 GroupBy 字段的查询和两表（或者多表）Join 的查询。</p><h3 id="4-1-单表数据倾斜优化"><a href="#4-1-单表数据倾斜优化" class="headerlink" title="4.1 单表数据倾斜优化"></a>4.1 单表数据倾斜优化</h3><h4 id="4-1-1-使用参数"><a href="#4-1-1-使用参数" class="headerlink" title="4.1.1 使用参数"></a>4.1.1 使用参数</h4><p>​    当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据 倾斜问题。</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">-- 是否在 Map 端进行聚合，默认为 True</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">-- 在 Map 端进行聚合操作的条目数目</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>mapaggr<span class="token punctuation">.</span>checkinterval <span class="token operator">=</span> <span class="token number">100000</span><span class="token punctuation">;</span></code></pre><p>有数据倾斜的时候进行负载均衡（默认是 false）</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>skewindata <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>当选项设定为 true，生成的查询计划会有两个 MR Job。</p><h4 id="4-1-2-增加-Reduce-数量（多个-Key-同时导致数据倾斜）"><a href="#4-1-2-增加-Reduce-数量（多个-Key-同时导致数据倾斜）" class="headerlink" title="4.1.2 增加 Reduce 数量（多个 Key 同时导致数据倾斜）"></a>4.1.2 增加 Reduce 数量（多个 Key 同时导致数据倾斜）</h4><p>1）调整 reduce 个数方法一</p><p>（1）每个 Reduce 处理的数据量默认是 256MB</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer <span class="token operator">=</span> <span class="token number">256000000</span></code></pre><p>（2）每个任务最大的 reduce 数，默认为 1009</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>max <span class="token operator">=</span> <span class="token number">1009</span></code></pre><p>（3）计算 reducer 数的公式</p><pre class=" language-sql"><code class="language-sql">N<span class="token operator">=</span><span class="token function">min</span><span class="token punctuation">(</span>参数 <span class="token number">2</span>，总输入数据量<span class="token operator">/</span>参数 <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>参数 <span class="token number">2</span> 指的是上面的 <span class="token number">1009</span>，参数 <span class="token number">1</span> 值得是 256M<span class="token punctuation">)</span></code></pre><p>2）调整 reduce 个数方法二</p><p>在 hadoop 的 mapred-default.xml 文件中修改</p><p>设置每个 job 的 Reduce 个数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span></code></pre><h3 id="4-2-Join-数据倾斜优化"><a href="#4-2-Join-数据倾斜优化" class="headerlink" title="4.2 Join 数据倾斜优化"></a>4.2 Join 数据倾斜优化</h3><h4 id="4-2-1-使用参数"><a href="#4-2-1-使用参数" class="headerlink" title="4.2.1 使用参数"></a>4.2.1 使用参数</h4><p>​    在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># join 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span><span class="token keyword">key</span><span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true"># 如果是 join 过程出现倾斜应该设置为 true</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>skewjoin<span class="token operator">=</span><span class="token boolean">false</span><span class="token punctuation">;</span></code></pre><p>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的 倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过 hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>map<span class="token punctuation">.</span>tasks<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">;</span></code></pre><h4 id="4-2-2-MapJoin"><a href="#4-2-2-MapJoin" class="headerlink" title="4.2.2 MapJoin"></a>4.2.2 MapJoin</h4><p>详情见 3.8 节</p><h2 id="第-5-章-Hive-Job-优化"><a href="#第-5-章-Hive-Job-优化" class="headerlink" title="第 5 章 Hive Job 优化"></a>第 5 章 Hive Job 优化</h2><h3 id="5-1-Hive-Map-优化"><a href="#5-1-Hive-Map-优化" class="headerlink" title="5.1 Hive Map 优化"></a>5.1 Hive Map 优化</h3><h4 id="5-1-1-复杂文件增加-Map-数"><a href="#5-1-1-复杂文件增加-Map-数" class="headerlink" title="5.1.1 复杂文件增加 Map 数"></a>5.1.1 复杂文件增加 Map 数</h4><p>​    当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据<br>$$<br>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M<br>$$<br>调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p><p>1）执行查询</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span></code></pre><p>2）设置最大切片值为 100 个字节</p><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>input<span class="token punctuation">.</span>fileinputformat<span class="token punctuation">.</span>split<span class="token punctuation">.</span>maxsize<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">;</span> <span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span></code></pre><h4 id="5-1-2-小文件进行合并"><a href="#5-1-2-小文件进行合并" class="headerlink" title="5.1.2 小文件进行合并"></a>5.1.2 小文件进行合并</h4><p>1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span></code></pre><p>2）在 Map-Reduce 的任务结束时合并小文件的设置：</p><p>在 map-only 任务结束时合并小文件，默认 true</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>在 map-reduce 任务结束时合并小文件，默认 false</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapredfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></code></pre><p>合并文件的大小，默认 256M</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>task <span class="token operator">=</span> <span class="token number">268435456</span><span class="token punctuation">;</span></code></pre><p>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>smallfiles<span class="token punctuation">.</span>avgsize <span class="token operator">=</span> <span class="token number">16777216</span><span class="token punctuation">;</span></code></pre><h4 id="5-1-3-Map-端聚合"><a href="#5-1-3-Map-端聚合" class="headerlink" title="5.1.3 Map 端聚合"></a>5.1.3 Map 端聚合</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- 相当于 map 端执行 combiner</span></code></pre><h4 id="5-1-4-推测执行"><a href="#5-1-4-推测执行" class="headerlink" title="5.1.4 推测执行"></a>5.1.4 推测执行</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapred<span class="token punctuation">.</span>map<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">-- #默认是 true</span></code></pre><h3 id="5-2-Hive-Reduce-优化"><a href="#5-2-Hive-Reduce-优化" class="headerlink" title="5.2 Hive Reduce 优化"></a>5.2 Hive Reduce 优化</h3><h4 id="5-2-1-合理设置-Reduce-数"><a href="#5-2-1-合理设置-Reduce-数" class="headerlink" title="5.2.1 合理设置 Reduce 数"></a>5.2.1 合理设置 Reduce 数</h4><p>1）调整 reduce 个数方法一</p><p>（1）每个 Reduce 处理的数据量默认是 256MB</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer <span class="token operator">=</span> <span class="token number">256000000</span></code></pre><p>（2）每个任务最大的 reduce 数，默认为 1009</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>max <span class="token operator">=</span> <span class="token number">1009</span></code></pre><p>（3）计算 reducer 数的公式<br>$$<br>N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)<br>$$<br>2）调整 reduce 个数方法二</p><p>​    在 hadoop 的 mapred-default.xml 文件中修改 设置每个 job 的 Reduce 个数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span></code></pre><p>3）reduce 个数并不是越多越好</p><p>（1）过多的启动和初始化 reduce 也会消耗时间和资源； </p><p>（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那 么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； 在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数； 使单个 reduce 任务处理数据量大小要合适；</p><h4 id="5-2-2-推测执行"><a href="#5-2-2-推测执行" class="headerlink" title="5.2.2  推测执行"></a>5.2.2  推测执行</h4><pre class=" language-sql"><code class="language-sql">mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution （hadoop 里面的）hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution（hive 里面相同的参数，效果和hadoop 里面的一样两个随便哪个都行）</code></pre><h3 id="5-3-Hive-任务整体优化"><a href="#5-3-Hive-任务整体优化" class="headerlink" title="5.3 Hive 任务整体优化"></a>5.3 Hive 任务整体优化</h3><h4 id="5-3-1-Fetch-抓取"><a href="#5-3-1-Fetch-抓取" class="headerlink" title="5.3.1 Fetch 抓取"></a>5.3.1 Fetch 抓取</h4><p>​    Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。</p><p>​    例如：SELECT * FROM emp;在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出 查询结果到控制台。 </p><p>​    在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive 默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走</p><p>mapreduce。</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.fetch.task.conversion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>more<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>     期望一个[没有，最少，更多]。      一些选择查询可以转换为单个FETCH任务最小化      延迟。      当前查询应该是单源的，没有任何子查询      不应该有任何聚合或区别(这会导致RS)，      侧面视图和连接。      0.  无:关闭hive.fetch.task.conversion      1.  最小:SELECT STAR，对分区列进行过滤，仅限      2.  more: SELECT, FILTER, LIMIT only(支持TABLESAMPLE和  虚拟列)   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>1）案例实操：</p><p>（1）把 hive.fetch.task.conversion 设置成 none，然后执行查询语句，都会执行 mapreduce 程序。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>none<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp <span class="token keyword">limit</span> <span class="token number">3</span><span class="token punctuation">;</span></code></pre><p>（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不 会执行 mapreduce 程序。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>more<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp<span class="token punctuation">;</span><span class="token keyword">select</span> ename <span class="token keyword">from</span> emp <span class="token keyword">limit</span> <span class="token number">3</span><span class="token punctuation">;</span></code></pre><h4 id="5-3-2-本地模式"><a href="#5-3-2-本地模式" class="headerlink" title="5.3.2 本地模式"></a>5.3.2 本地模式</h4><p>​    大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。</p><p>​    不过， 有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能 会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机 器上处理所有的任务。</p><p>​    对于小数据集，执行时间可以明显被缩短。 用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动 启动这个优化。</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=true; //开启本地 mr//设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;//设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4set hive.exec.mode.local.auto.input.files.max=10;</code></pre><p>1）案例实操：</p><p>（1）开启本地模式，并执行查询语句</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=true;select * from emp cluster by deptno;</code></pre><p>（2）关闭本地模式，并执行查询语句</p><pre class=" language-xml"><code class="language-xml">set hive.exec.mode.local.auto=false;select * from emp cluster by deptno;</code></pre><h4 id="5-3-3-并行执行"><a href="#5-3-3-并行执行" class="headerlink" title="5.3.3 并行执行"></a>5.3.3 并行执行</h4><p>​    Hive 会将一个查询转化成一个或者多个阶段。</p><p>​    这样的阶段可以是 MapReduce 阶段、抽 样阶段、合并阶段、limit 阶段。</p><p>​    或者 Hive 执行过程中可能需要的其他阶段。默认情况下， Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能 并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行 时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。</p><p>通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中， 需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//打开任务并行执行，默认为 false</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>thread<span class="token punctuation">.</span>number<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//同一个 sql 允许最大并行度，默认为 8</span></code></pre><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来</p><p>(建议在数据量大,sql 很长的时候使用,数据量小,sql 比较的小开启有可能还不如之前快)。</p><h4 id="5-3-4-严格模式"><a href="#5-3-4-严格模式" class="headerlink" title="5.3.4 严格模式"></a>5.3.4 严格模式</h4><p>Hive 可以通过设置防止一些危险操作：</p><p>1）分区表不使用分区过滤</p><p>​    将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含 有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分 区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有 进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</p><p>2）使用 order by 没有 limit 过滤</p><p>​    将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要 求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间(开启了 limit 可以在数据进入到 reduce 之前就减少一部分数据)。</p><p>3）笛卡尔积</p><p>​    将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数 据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语 句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸 的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情 况。</p><h4 id="5-3-5-JVM-重用"><a href="#5-3-5-JVM-重用" class="headerlink" title="5.3.5 JVM 重用"></a>5.3.5 JVM 重用</h4><p>​    小文件过多的时候使用。</p><h2 id="第-6-章-Hive-On-Spark"><a href="#第-6-章-Hive-On-Spark" class="headerlink" title="第 6 章 Hive On Spark"></a>第 6 章 Hive On Spark</h2><h3 id="6-1-Executor-参数"><a href="#6-1-Executor-参数" class="headerlink" title="6.1 Executor 参数"></a>6.1 Executor 参数</h3><p>​    以单台服务器 128G 内存，32 线程为例。</p><h4 id="6-1-1-spark-executor-cores"><a href="#6-1-1-spark-executor-cores" class="headerlink" title="6.1.1 spark.executor.cores"></a>6.1.1 spark.executor.cores</h4><p>​    该参数表示每个 Executor 可利用的 CPU 核心数。其值不宜设定过大，因为 Hive 的底层 以 HDFS 存储，而 HDFS 有时对高并发写入处理不太好，容易造成 竞争条件。根据经验 实践，设定在 3~6 之间比较合理。</p><p>​    假设我们使用的服务器单节点有 32 个 CPU 核心可供使用。考虑到系统基础服务和 HDFS 等组件的余量，一般会将 YARN NodeManager 的 yarn.nodemanager.resource.cpu-vcores 参数 设为 28，也就是 YARN 能够利用其中的 28 核，此时将 spark.executor.cores 设为 4 最合适， 最多可以正好分配给 7 个 Executor 而不造成浪费。又假设 yarn.nodemanager.resource.cpuvcores 为 26，那么将 spark.executor.cores 设为 5 最合适，只会剩余 1 个核。</p><p>​    由于一个 Executor 需要一个 YARN Container 来运行，所以还需保证 spark.executor.cores 的值不能大于单个 Container 能申请到的最大核心数，即 yarn.scheduler.maximum-allocationvcores 的值。</p><h4 id="6-1-2-spark-executor-memory-spark-yarn-executor-memoryOverhead"><a href="#6-1-2-spark-executor-memory-spark-yarn-executor-memoryOverhead" class="headerlink" title="6.1.2 spark.executor.memory/spark.yarn.executor.memoryOverhead"></a>6.1.2 spark.executor.memory/spark.yarn.executor.memoryOverhead</h4><p>​    这两个参数分别表示每个 Executor 可利用的堆内内存量和堆外内存量。堆内内存越大Executor 就能缓存更多的数据，在做诸如 map join 之类的操作时就会更快，但同时也会使得 GC 变得更麻烦。spark.yarn.executor.memoryOverhead 的默认值是 executorMemory * 0.10， 最小值为 384M(每个 Executor)</p><p>​    Hive 官方提供了一个计算 Executor 总内存量的经验公式，如下：</p><p>​    yarn.nodemanager.resource.memory-mb*(spark.executor.cores/ yarn.nodemanager.resource.cpu-vcores)</p><p>​    其实就是按核心数的比例分配。在计算出来的总内存量中，80%~85%划分给堆内内存， 剩余的划分给堆外内存。</p><p>​    假设集群中单节点有 128G 物理内存，yarn.nodemanager.resource.memory-mb（即单个 NodeManager 能够利用的主机内存量）设为 100G，那么每个 Executor 大概就是 100*(4/28)= 约 14G。</p><p>​    再 按 8:2 比 例 划 分 的 话 ， 最 终 spark.executor.memory 设 为 约 11.2G ， spark.yarn.executor.memoryOverhead 设为约 2.8G。</p><p>​    通过这些配置，每个主机一次可以运行多达 7 个 executor。每个 executor 最多可以运行 4 个 task(每个核一个)。因此，每个 task 平均有 3.5 GB(14 / 4)内存。在 executor 中运行的所 有 task 共享相同的堆空间。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memory<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">.</span>2g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">.</span>8g<span class="token punctuation">;</span></code></pre><p>同理，这两个内存参数相加的总量也不能超过单个 Container 最多能申请到的内存量， 即 yarn.scheduler.maximum-allocation-mb 配置的值。</p><h4 id="6-1-3-spark-executor-instances"><a href="#6-1-3-spark-executor-instances" class="headerlink" title="6.1.3 spark.executor.instances"></a>6.1.3 spark.executor.instances</h4><p>​    该参数表示执行查询时一共启动多少个 Executor 实例，这取决于每个节点的资源分配 情况以及集群的节点数。若我们一共有 10 台 32C/128G 的节点，并按照上述配置（即每个节 点承载 7 个 Executor），那么理论上讲我们可以将 spark.executor.instances 设为 70，以使集群 资源最大化利用。但是实际上一般都会适当设小一些（推荐是理论值的一半左右，比如 40）， 因为 Driver 也要占用资源，并且一个 YARN 集群往往还要承载除了 Hive on Spark 之外的其他 业务。</p><h4 id="6-1-4-spark-dynamicAllocation-enabled"><a href="#6-1-4-spark-dynamicAllocation-enabled" class="headerlink" title="6.1.4 spark.dynamicAllocation.enabled"></a>6.1.4 spark.dynamicAllocation.enabled</h4><p>​    上面所说的固定分配 Executor 数量的方式可能不太灵活，尤其是在 Hive 集群面向很多,用户提供分析服务的情况下。所以更推荐将 spark.dynamicAllocation.enabled 参数设为 true， 以启用 Executor 动态分配。</p><h4 id="6-1-5-参数配置样例参考"><a href="#6-1-5-参数配置样例参考" class="headerlink" title="6.1.5 参数配置样例参考"></a>6.1.5 参数配置样例参考</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>execution<span class="token punctuation">.</span><span class="token keyword">engine</span><span class="token operator">=</span>spark<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memory<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">.</span>2g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">.</span>8g<span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>cores<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>instances<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>dynamicAllocation<span class="token punctuation">.</span>enabled<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token keyword">set</span> spark<span class="token punctuation">.</span>serializer<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>serializer<span class="token punctuation">.</span>KryoSerializer<span class="token punctuation">;</span></code></pre><h3 id="6-2-Driver-参数"><a href="#6-2-Driver-参数" class="headerlink" title="6.2 Driver 参数"></a>6.2 Driver 参数</h3><h4 id="6-2-1-spark-driver-cores"><a href="#6-2-1-spark-driver-cores" class="headerlink" title="6.2.1 spark.driver.cores"></a>6.2.1 spark.driver.cores</h4><p>​    该参数表示每个 Driver 可利用的 CPU 核心数。绝大多数情况下设为 1 都够用。</p><h4 id="6-2-2-spark-driver-memory-spark-driver-memoryOverhead"><a href="#6-2-2-spark-driver-memory-spark-driver-memoryOverhead" class="headerlink" title="6.2.2 spark.driver.memory/spark.driver.memoryOverhead"></a>6.2.2 spark.driver.memory/spark.driver.memoryOverhead</h4><p>​    这两个参数分别表示每个 Driver 可利用的堆内内存量和堆外内存量。根据资源富余程 度和作业的大小，一般是将总量控制在 512MB~4GB 之间，并且沿用 Executor 内存的“二八分 配方式”。例如，spark.driver.memory 可以设为约 819MB，spark.driver.memoryOverhead 设为 约 205MB，加起来正好 1G。</p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink提交流程以及Flinkdebug流程分享</title>
      <link href="/2021/08/23/Flink%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8AFlinkdebug%E6%B5%81%E7%A8%8B%E5%88%86%E4%BA%AB/"/>
      <url>/2021/08/23/Flink%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8AFlinkdebug%E6%B5%81%E7%A8%8B%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink提交流程以及Flinkdebug流程分享"><a href="#Flink提交流程以及Flinkdebug流程分享" class="headerlink" title="Flink提交流程以及Flinkdebug流程分享"></a>Flink提交流程以及Flinkdebug流程分享</h1><h2 id="Flink三种提交流程"><a href="#Flink三种提交流程" class="headerlink" title="Flink三种提交流程"></a>Flink三种提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130021872.png" alt="Flink提交流程"></p><h2 id="为什么引入Application模式"><a href="#为什么引入Application模式" class="headerlink" title="为什么引入Application模式"></a>为什么引入Application模式</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130250657.png" alt="三种模式"></p><p>目前我们公司使用的是PerJob模式，必须得使用接口机来进行上传，会给服务器造成很大的压力，大量的网络IO开销，我们任务是非常多的，Flink1.11之后，引入了Application模式，全部都上传到master上，可以把jar指定在hdfs上，减少上传和部署作业的时间</p><h2 id="Flink提交流程概览"><a href="#Flink提交流程概览" class="headerlink" title="Flink提交流程概览"></a>Flink提交流程概览</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823130937087.png" alt="Flink提交流程"></p><h2 id="Flink-Stream-sql客户端提交流程"><a href="#Flink-Stream-sql客户端提交流程" class="headerlink" title="Flink Stream sql客户端提交流程"></a>Flink Stream sql客户端提交流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823131345554.png" alt="Flink SQL提交流程"></p><p>Flink Stream SQL的源码不是很多，有时间的话，可以去看看</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823132035655.png" alt="Flink SQL主类"></p><p>Flink SQL中的executor是非常重要的，Flink client提交SQL的一个流程</p><h2 id="Flink-Session-Runtime"><a href="#Flink-Session-Runtime" class="headerlink" title="Flink Session Runtime"></a>Flink Session Runtime</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823132427416.png" alt="Flink Session Runtime"></p><p>这些都是可以好好的把Flink 1.12的源码给好好阅读一下，并做好自己的笔记是非常的重要的</p><p>提交和调度，7大点调度任务，作业调度，对象不明确</p><p>Flink 提交源码可以总结成一个流程图，可以根据自己写的类图，启动集群，提交任务，任务调度都可以进行一个完整的程序</p><h2 id="Flink-Debug-流程"><a href="#Flink-Debug-流程" class="headerlink" title="Flink Debug 流程"></a>Flink Debug 流程</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210823133735059.png" alt="Flink Debug流程"></p><p>多去尝试，多去打断点就好了，可以根据yarn session上面的ip 来进行查看</p>]]></content>
      
      
      <categories>
          
          <category> Flink流式引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink流式引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink电商实时数仓DWM层业务实现</title>
      <link href="/2021/08/23/Flink%E7%94%B5%E5%95%86%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93DWM%E5%B1%82%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0/"/>
      <url>/2021/08/23/Flink%E7%94%B5%E5%95%86%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93DWM%E5%B1%82%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink电商实时数仓DWM层业务实现"><a href="#Flink电商实时数仓DWM层业务实现" class="headerlink" title="Flink电商实时数仓DWM层业务实现"></a>Flink电商实时数仓DWM层业务实现</h1><p>DWS是每天的聚合结果，按天做分区，DWT是一个累计的结果聚合</p><p>DWD是不考虑需求，和需求是没有关系的，DWM,DWS是需要考虑需求，日志数据的分流，业务数据的分流</p><p>实时计算指标计算：</p><p>​    1、实时的运维和资源的要求都是很高的</p><p>​    2、在公司中可能是按照主题来进行分析和计算</p><p>​    3、在离线的数仓当中是没有关键词分析的，这里的话，加入了搜索关键字主题分析</p><h2 id="第1章-DWS层与DWM层的设计"><a href="#第1章-DWS层与DWM层的设计" class="headerlink" title="第1章 DWS层与DWM层的设计"></a>第1章 DWS层与DWM层的设计</h2><table><thead><tr><th align="center"><strong>统计主题</strong></th><th align="center"><strong>需求指标</strong></th><th align="center"><strong>输出方式</strong></th><th align="center"><strong>计算来源</strong></th><th align="center"><strong>来源层级</strong></th></tr></thead><tbody><tr><td align="center">访客</td><td align="center">pv</td><td align="center">可视化大屏</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">uv</td><td align="center">可视化大屏</td><td align="center">需要用page_log过滤去重</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">跳出率</td><td align="center">可视化大屏</td><td align="center">需要通过page_log行为判断</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">进入页面数</td><td align="center">可视化大屏</td><td align="center">需要识别开始访问标识</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">连续访问时长</td><td align="center">可视化大屏</td><td align="center">page_log直接可求</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">商品</td><td align="center">点击</td><td align="center">多维分析</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">收藏</td><td align="center">多维分析</td><td align="center">收藏表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">加入购物车</td><td align="center">多维分析</td><td align="center">购物车表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">下单</td><td align="center">可视化大屏</td><td align="center">订单宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">支付</td><td align="center">多维分析</td><td align="center">支付宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">退款</td><td align="center">多维分析</td><td align="center">退款表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">评论</td><td align="center">多维分析</td><td align="center">评论表</td><td align="center">dwd</td><td align="center"></td></tr><tr><td align="center">地区</td><td align="center">pv</td><td align="center">多维分析</td><td align="center">page_log直接可求</td><td align="center">dwd</td></tr><tr><td align="center">uv</td><td align="center">多维分析</td><td align="center">需要用page_log过滤去重</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">下单</td><td align="center">可视化大屏</td><td align="center">订单宽表</td><td align="center">dwm</td><td align="center"></td></tr><tr><td align="center">关键词</td><td align="center">搜索关键词</td><td align="center">可视化大屏</td><td align="center">页面访问日志 直接可求</td><td align="center">dwd</td></tr><tr><td align="center">点击商品关键词</td><td align="center">可视化大屏</td><td align="center">商品主题下单再次聚合</td><td align="center">dws</td><td align="center"></td></tr><tr><td align="center">下单商品关键词</td><td align="center">可视化大屏</td><td align="center">商品主题下单再次聚合</td><td align="center">dws</td><td align="center"></td></tr></tbody></table><ul><li><input disabled="" type="checkbox"> 访问UV计算<ul><li><input disabled="" type="checkbox"> 按用户去重，在天的基础上，安装各种维度信息来进行统计UV，DWM作为一个中间表，复用性，中心思想</li></ul></li><li><input disabled="" type="checkbox"> 跳出明细计算</li><li><input disabled="" type="checkbox"> 订单宽表</li><li><input disabled="" type="checkbox"> 支付宽表</li></ul><h2 id="第2章-DWM层-访客UV的计算"><a href="#第2章-DWM层-访客UV的计算" class="headerlink" title="第2章 DWM层-访客UV的计算"></a>第2章 DWM层-访客UV的计算</h2><h3 id="2-1-需求描述"><a href="#2-1-需求描述" class="headerlink" title="2.1 需求描述"></a>2.1 需求描述</h3><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">UV</span> <span class="token attr-value">全称是Unique Visitor，即访客数，对于实时计算中，也可以称为DAU(Daily Active User)，即每日的活跃用户数，因为实时计算中心的UV通常是指当日的访客数</span>那么如何从用户行为日志识别当日的访客：    1、识别出该用户打开的第一个页面，表示这个访客开始进入我们的应用    2、由于访客可以在一天中多次进入应用，我们需要要在一天的范围内进行去重</code></pre><h3 id="2-2-思考点"><a href="#2-2-思考点" class="headerlink" title="2.2 思考点"></a>2.2 思考点</h3><pre class=" language-properties"><code class="language-properties">使用状态编程来对多次进入的用户进行过滤，那么问题来了，状态里面存入什么才能更好的对用户进行一个过滤勒？状态如果为null，说明这个数据是新的，如果状态中存入的是时间，那么和数据中的时间进行比较就可以判断是否是重复的数据</code></pre><h3 id="2-3-如何去清空状态"><a href="#2-3-如何去清空状态" class="headerlink" title="2.3 如何去清空状态"></a>2.3 如何去清空状态</h3><pre class=" language-properties"><code class="language-properties">我们使用TTL的方式来进行清空状态昨天的状态，状态有没有关系都是没有关系，判断的时候一定要进行双重判断，否则会进行数据丢失按天去重，最少也是要保存一天的</code></pre><h3 id="2-4-两种方式的实现"><a href="#2-4-两种方式的实现" class="headerlink" title="2.4 两种方式的实现"></a>2.4 两种方式的实现</h3><pre class=" language-properties"><code class="language-properties">有的公司加入了session会话</code></pre><h3 id="2-5-测试流程"><a href="#2-5-测试流程" class="headerlink" title="2.5 测试流程"></a>2.5 测试流程</h3><pre class=" language-properties"><code class="language-properties">我们在进行测试的时候，一定得清楚数据流是如何进行处理得，如果不能清楚这个得化，是很难能够了解整个业务得，最后可以加一个测试，因为就是那个第一条和第二条都是可以加入测试的<span class="token attr-name">总之这个Flink</span> <span class="token attr-value">UV统计是流计算当中最简单的一个，因为我们只需要判断它是否是活跃或者访问的是进入页面的首页面，然后判断去重就可以直接将数据进行一个输出的操作</span><span class="token attr-name">其中设计到的kafka</span> <span class="token attr-value">topic</span><span class="token attr-name">Window</span><span class="token punctuation">:</span>    BaseLogAPP    UniqueVisitAPPLinux：<span class="token attr-name">    java</span> <span class="token attr-value">-jar gmall2020-mock-log-2020-12-18.jar</span><span class="token attr-name">    java</span> <span class="token attr-value">-jar userBehaviorLog-0.0.1-SNAPSHOT.jar</span>    dwd_page_log    dwm_unique_visit目前系统内存占比是</code></pre><h3 id="2-6-能力提升和项目问题疑问"><a href="#2-6-能力提升和项目问题疑问" class="headerlink" title="2.6 能力提升和项目问题疑问"></a>2.6 能力提升和项目问题疑问</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/state/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/state/</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/custom_serialization/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/custom_serialization/</a></p><h4 id="2-6-1-Flink对状态清空的方法"><a href="#2-6-1-Flink对状态清空的方法" class="headerlink" title="2.6.1 Flink对状态清空的方法"></a>2.6.1 Flink对状态清空的方法</h4><ul><li>使用自动的TTL来对状态进行清空</li><li>使用定时器来对状态进行手动的清空</li></ul><h4 id="2-6-2-案例演示"><a href="#2-6-2-案例演示" class="headerlink" title="2.6.2 案例演示"></a>2.6.2 案例演示</h4><h3 id="2-7-使用到的技术点"><a href="#2-7-使用到的技术点" class="headerlink" title="2.7 使用到的技术点"></a>2.7 使用到的技术点</h3><pre class=" language-properties"><code class="language-properties">状态检查点删除检查点动态分流</code></pre><h3 id="2-8-思考"><a href="#2-8-思考" class="headerlink" title="2.8 思考"></a>2.8 思考</h3><pre class=" language-properties"><code class="language-properties">如果业务数据库不是MySQL的话，我们可以采取定时去读取的那种方式，今天把它给弄明白，然后彻底的掌握清楚</code></pre><h2 id="第3章-DWM层-跳出明细计算"><a href="#第3章-DWM层-跳出明细计算" class="headerlink" title="第3章 DWM层-跳出明细计算"></a>第3章 DWM层-跳出明细计算</h2><h3 id="3-1-需求分析与思路"><a href="#3-1-需求分析与思路" class="headerlink" title="3.1   需求分析与思路"></a>3.1   需求分析与思路</h3><h4 id="3-1-1-什么是跳出"><a href="#3-1-1-什么是跳出" class="headerlink" title="3.1.1 什么是跳出"></a>3.1.1 什么是跳出</h4><pre class=" language-properties"><code class="language-properties">跳出就是用户成功访问了网站的一个页面后就退出，不在继续访问网站的其它页面。而跳出率就是用跳出次数除以访问次数。关注跳出率，可以看出引流过来的访客是否能很快的被吸引，渠道引流过来的用户之间的质量对比，对于应用优化前后跳出率的对比也能看出优化改进的成果。</code></pre><h4 id="3-1-2-计算跳出行为的思路"><a href="#3-1-2-计算跳出行为的思路" class="headerlink" title="3.1.2 计算跳出行为的思路"></a>3.1.2 计算跳出行为的思路</h4><pre class=" language-properties"><code class="language-properties">首先要识别哪些是跳出行为，要把这些跳出的访客最后一个访问的页面识别出来。那么要抓住几个特征：<span class="token attr-name"></span> <span class="token attr-value">   该页面是用户近期访问的第一个页面</span>这个可以通过该页面是否有上一个页面（last_page_id）来判断，如果这个表示为空，就说明这是这个访客这次访问的第一个页面。<span class="token attr-name"></span> <span class="token attr-value">   首次访问之后很长一段时间（自己设定），用户没继续再有其他页面的访问。</span>这第一个特征的识别很简单，保留last_page_id为空的就可以了。但是第二个访问的判断，其实有点麻烦，首先这不是用一条数据就能得出结论的，需要组合判断，要用一条存在的数据和不存在的数据进行组合判断。而且要通过一个不存在的数据求得一条存在的数据。更麻烦的他并不是永远不存在，而是在一定时间范围内不存在。那么如何识别有一定失效的组合行为呢？最简单的办法就是Flink自带的CEP技术。这个CEP非常适合通过多条数据组合来识别某个事件。用户跳出事件，本质上就是一个条件事件加一个超时事件的组合。</code></pre><h3 id="3-2-代码实现"><a href="#3-2-代码实现" class="headerlink" title="3.2    代码实现"></a>3.2    代码实现</h3><h4 id="3-2-1-从kafka的dwd-page-log主题中读取页面日志"><a href="#3-2-1-从kafka的dwd-page-log主题中读取页面日志" class="headerlink" title="3.2.1 从kafka的dwd_page_log主题中读取页面日志"></a>3.2.1 从kafka的dwd_page_log主题中读取页面日志</h4><h4 id="3-2-2-通过Flink的CEP完成跳出判断"><a href="#3-2-2-通过Flink的CEP完成跳出判断" class="headerlink" title="3.2.2 通过Flink的CEP完成跳出判断"></a>3.2.2 通过Flink的CEP完成跳出判断</h4><h5 id="1-确认添加了CEP的依赖包"><a href="#1-确认添加了CEP的依赖包" class="headerlink" title="1)  确认添加了CEP的依赖包"></a>1)  确认添加了CEP的依赖包</h5><h5 id="2-设定时间语义为事件时间并指定数据中的ts字段为事件时间"><a href="#2-设定时间语义为事件时间并指定数据中的ts字段为事件时间" class="headerlink" title="2)  设定时间语义为事件时间并指定数据中的ts字段为事件时间"></a>2)  设定时间语义为事件时间并指定数据中的ts字段为事件时间</h5><h5 id="3-根据日志数据的mid进行分组"><a href="#3-根据日志数据的mid进行分组" class="headerlink" title="3)  根据日志数据的mid进行分组"></a>3)  根据日志数据的mid进行分组</h5><h5 id="4-配置CEP表达式"><a href="#4-配置CEP表达式" class="headerlink" title="4)  配置CEP表达式"></a>4)  配置CEP表达式</h5><h5 id="5-根据表达式筛选流"><a href="#5-根据表达式筛选流" class="headerlink" title="5)  根据表达式筛选流"></a>5)  根据表达式筛选流</h5><h5 id="6-提取命中的数据"><a href="#6-提取命中的数据" class="headerlink" title="6)  提取命中的数据"></a>6)  提取命中的数据</h5><h4 id="3-2-3-将跳出数据写回到kafka的DWM层"><a href="#3-2-3-将跳出数据写回到kafka的DWM层" class="headerlink" title="3.2.3 将跳出数据写回到kafka的DWM层"></a>3.2.3 将跳出数据写回到kafka的DWM层</h4><h4 id="3-2-4-测试"><a href="#3-2-4-测试" class="headerlink" title="3.2.4 测试"></a>3.2.4 测试</h4><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_11","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263563000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_11","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263564000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_12","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263564000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_13","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263570000}</span><span class="token attr-name">{"common"</span><span class="token punctuation">:</span><span class="token attr-value">{"ar":"440000","ba":"vivo","ch":"web","is_new":"1","md":"vivo iqoo3","mid":"mid_13","os":"Android 11.0","uid":"23","vc":"v2.1.134"},"page":{"during_time":8488,"page_id":"search"},"ts":1608263574000}</span></code></pre><p>超时事件处理不是很难，主要是业务是比较难得，我们通过处理放到了kafka层</p><h2 id="第4章-DWM层-订单宽表"><a href="#第4章-DWM层-订单宽表" class="headerlink" title="第4章 DWM层-订单宽表"></a>第4章 DWM层-订单宽表</h2><p>重点：旁路缓存，异步IO</p><p>GMV做分时统计</p><p>这里有一个疑问就是，table_process表里面的数据和kafka的字段的信息是不一样的，出现了问题了，这里需要排查一下</p><p>订单宽表是整个数仓当中最难的整个了</p><p>Phoenix构建表的时候也是非常关键的</p><p>列，值，对象，如何去结合在一起，将属性和值设置给对象 commonBeanutils</p><h3 id="4-1-Dimutil-和-PhoenixUtils的编写"><a href="#4-1-Dimutil-和-PhoenixUtils的编写" class="headerlink" title="4.1 Dimutil 和 PhoenixUtils的编写"></a>4.1 Dimutil 和 PhoenixUtils的编写</h3><p>我们通过工具类直接在hbase中读取维度数据，但是我现在想要通过Redis来做一次缓存，所以中间的话，我 加入了Redis来进行一个缓存的操作的</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210826093647190.png" alt="旁路缓存数据库"></p><pre class=" language-properties"><code class="language-properties">先查询Redis数据，Redis有什么数据,JSONString使用什么类型？以及RedisKey设计成什么样子？RedisKey?<span class="token attr-name">Hash</span> <span class="token attr-value">String Set List Zset</span><span class="token attr-name">Hash</span><span class="token punctuation">:</span><span class="token attr-name">    RedisKey</span><span class="token punctuation">:</span> <span class="token attr-value">tableName</span><span class="token attr-name">    HashKey</span><span class="token punctuation">:</span> <span class="token attr-value">value</span><span class="token attr-name">根据Hash</span> <span class="token attr-value">Key来进行获取数据</span><span class="token attr-name">String</span><span class="token punctuation">:</span> <span class="token attr-value">tableName: value</span><span class="token attr-name">List</span> <span class="token attr-value">| Set ---> 不太合适，不能单条数据进行查询，一查的话就是一个集合</span>hash可以可以它的hashKey来进行获取数据</code></pre><p>Redis使用来存储热点数据，对单个用户来进行设置超时时间，最后我们使用String 来进行存储</p><p>进行维度关联的时候是一条数据吗</p><p>如果说MySQL中的数据变化。hbase中的数据也进行变化，那么Redis中的数据会发生改变吗</p><p>问题就是，Redis中的数据只要不过时，那么就还是会原来的数据，加一个方法就是如果是更新数据的话，那么就把Redis中的数据删掉，然后在重新的进行插入就好了</p><p>在DimSinkFunction的时候进行开发，做项目的时候都是一步步的进行完善的操作</p><p>—raw 这是显示中文的</p><p>注意就是在查询的时候，注意要转入成大写的，否则的话会出现表名不一致的情况</p><h4 id="4-1-1-旁路缓存-扩展RedisBitMap"><a href="#4-1-1-旁路缓存-扩展RedisBitMap" class="headerlink" title="4.1.1  旁路缓存 扩展RedisBitMap"></a>4.1.1  旁路缓存 扩展RedisBitMap</h4><pre class=" language-properties"><code class="language-properties">布隆过滤器，三种不同的Hash算法，减少Hash碰撞，足够长Redis直接操作bit位</code></pre><p>把Redis给好好的学习，对面试只有好处，没有坏处</p><h4 id="4-1-2-Redis三大问题"><a href="#4-1-2-Redis三大问题" class="headerlink" title="4.1.2 Redis三大问题"></a>4.1.2 Redis三大问题</h4><pre class=" language-properties"><code class="language-properties">读写分离：热点数据    缓存穿透    缓存击穿    缓存雪崩面试的重点和概念</code></pre><h4 id="4-1-3-缓存穿透"><a href="#4-1-3-缓存穿透" class="headerlink" title="4.1.3  缓存穿透"></a>4.1.3  缓存穿透</h4><pre class=" language-properties"><code class="language-properties">查询的Key不存在，大量的访问数据库</code></pre><h4 id="4-1-4-缓存击穿"><a href="#4-1-4-缓存击穿" class="headerlink" title="4.1.4 缓存击穿"></a>4.1.4 缓存击穿</h4><pre class=" language-properties"><code class="language-properties">热点Key突然失效，大量的访问到数据库</code></pre><h4 id="4-1-5-缓存雪崩"><a href="#4-1-5-缓存雪崩" class="headerlink" title="4.1.5 缓存雪崩"></a>4.1.5 缓存雪崩</h4><pre class=" language-properties"><code class="language-properties">很多Key同时失效，大量的访问数据库</code></pre><h3 id="4-2-异步查询"><a href="#4-2-异步查询" class="headerlink" title="4.2 异步查询"></a>4.2 异步查询</h3><p>有时间下去去查看一下，利用抽象类的方式来进行一个动态的传参的过程</p><p>不要盲目的去学习设计模式，用到了在去学习</p><p>BIRTHDAY，这个数据类型写成varchar，Flink CDC在封装数据的时候和我们想要的不一样</p><h3 id="4-3-出生年月转换成年龄，这个可以进行查询一下"><a href="#4-3-出生年月转换成年龄，这个可以进行查询一下" class="headerlink" title="4.3 出生年月转换成年龄，这个可以进行查询一下"></a>4.3 出生年月转换成年龄，这个可以进行查询一下</h3><p>simpledateFormat相差8个小时，用到了时间，DateTime，读到的并不是字符串</p><p>计算的时候注意看是否超精度了</p><p>Flink CDC读取DateTime数据的时候是有问题的</p><p>FlinkCDC读取时间类型的数据，会自动进行一个解析，那个毫秒数是否会差八小时，maxwell中</p><p>这个就是Flink CDC解析时间的问题</p><h2 id="第5章-DWM层支付宽表（练习）"><a href="#第5章-DWM层支付宽表（练习）" class="headerlink" title="第5章 DWM层支付宽表（练习）"></a>第5章 DWM层支付宽表（练习）</h2><h2 id="第6章-总结"><a href="#第6章-总结" class="headerlink" title="第6章 总结"></a>第6章 总结</h2>]]></content>
      
      
      <categories>
          
          <category> 企业级数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 企业级数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL面试必刷</title>
      <link href="/2021/08/22/MySQL%E9%9D%A2%E8%AF%95%E5%BF%85%E5%88%B7/"/>
      <url>/2021/08/22/MySQL%E9%9D%A2%E8%AF%95%E5%BF%85%E5%88%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="数据库-MySQL"><a href="#数据库-MySQL" class="headerlink" title="数据库-MySQL"></a>数据库-MySQL</h1><p><a href="https://www.mysql.com/" target="_blank" rel="noopener">https://www.mysql.com/</a></p><p><a href="https://www.cs.usfca.edu/~galles/visualization/Algorithms.html" target="_blank" rel="noopener">https://www.cs.usfca.edu/~galles/visualization/Algorithms.html</a></p><h2 id="1、索引的底层数据结构是B还是B-树"><a href="#1、索引的底层数据结构是B还是B-树" class="headerlink" title="1、索引的底层数据结构是B还是B+树"></a>1、索引的底层数据结构是B还是B+树</h2><ul><li>为什么要设计索引</li><li>如果是你，会如何设计索引</li><li>设计索引的时候使用什么数据结构</li><li>MySQL是如何实现的</li></ul><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">OLTP：</span> <span class="token attr-value">业务系统的支持，及时性非常高;</span>OLAP：对历史数据的分析MySQL中使用的是B+树索引，为什么要使用B+树，面试中可能会聊到树的概念官网中有B树，底层是使用的B+树？他们之间有什么区别？MySQL中除了B,B+有其它数据结构？<span class="token attr-name">Hash</span> <span class="token attr-name">存储引擎</span> <span class="token attr-value">---> 不同的数据文件在物理磁盘上的不同的组织形式</span>MySQL的存储引擎</code></pre><p>MySQL中的存储引擎</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822175643586.png" alt="MySQL的存储引擎"></p><p>默认是使用InnoDB，Memory使用的是Hash索引</p><pre class=" language-properties"><code class="language-properties">InnoDB支持Hash索引，但是是自适应Hash，为什么要使用自适应的Hash</code></pre><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822175834821.png" alt="索引的数据结构"></p><p>存储引擎到底是个啥，可以去存储的目录下面去查看这些文件的存储格式，不同的存储引擎的格式是不一样的</p><h2 id="2、MySQL的索引系统"><a href="#2、MySQL的索引系统" class="headerlink" title="2、MySQL的索引系统"></a>2、MySQL的索引系统</h2><pre class=" language-properties"><code class="language-properties">操作系统的支持，数据都是会放到磁盘上，但是对数据进行实际读取的时候，加载内存，然后才能读取到数据，磁盘和内存的读取的速度是不一样的硬件的延迟对应图，内存的数据很快的，需要把磁盘的数据加载到内存中，如果内存是无限大，可以全量的加载，内存的数据是不安全的，因此还是会进行持久化的操作预加载的过程，取一行记录，操作系统的一方面就是磁盘预读，局部性的原理程序有聚集趋向，时间局部性，磁盘预读，读的是某一个页的数据，页的大小和操作系统大小是有关系，一般都是4K，一般都是页的整数倍Hash表和二叉树hash，二叉树，B树，B+树，这个非常重要的，面试面的很多，得举例<span class="token attr-name">Hash</span> <span class="token attr-value">:</span><span class="token attr-name">    1、存在hash碰撞，必须要设置一个非常完美的hash算法，必须有一个</span> <span class="token attr-value">扰动函数</span>    2、hash适合等值查询，不适合范围查询红黑树、B树、B+树红黑树的数据格式：    HashMap里面的东西得结合起来</code></pre><h2 id="3、前缀索引实例说明"><a href="#3、前缀索引实例说明" class="headerlink" title="3、前缀索引实例说明"></a>3、前缀索引实例说明</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">degree</span> <span class="token attr-value">度</span>磁盘块的放取，16K，是磁盘预读，</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客维护问题</title>
      <link href="/2021/08/22/%E5%8D%9A%E5%AE%A2%E7%BB%B4%E6%8A%A4%E9%97%AE%E9%A2%98/"/>
      <url>/2021/08/22/%E5%8D%9A%E5%AE%A2%E7%BB%B4%E6%8A%A4%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="博客维护"><a href="#博客维护" class="headerlink" title="博客维护"></a>博客维护</h1><h2 id="1、解决博客上传问题"><a href="#1、解决博客上传问题" class="headerlink" title="1、解决博客上传问题"></a>1、解决博客上传问题</h2><p>有时间会上传失败的话，多尝试几次就好了，可能是网速不太好</p><h2 id="2、上传图床的问题"><a href="#2、上传图床的问题" class="headerlink" title="2、上传图床的问题"></a>2、上传图床的问题</h2><p>我选择的是gitee做为自己的图床，来进行保存图片，图片就不需要本地来进行保存了，这样很方便的就行文件的复制以及和移动</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822163427850.png" alt="图床的准备"></p><h2 id="3、解决问题超时的问题"><a href="#3、解决问题超时的问题" class="headerlink" title="3、解决问题超时的问题"></a>3、解决问题超时的问题</h2><p>网上的资料好多都很乱，只需要更改一下下面这个就好了，特别是要注意的自己的配置</p><h1 id="这个是上传到github超时的时候给配置的"><a href="#这个是上传到github超时的时候给配置的" class="headerlink" title="这个是上传到github超时的时候给配置的"></a>这个是上传到github超时的时候给配置的</h1><p>151.101.185.194 github.global-ssl.fastly.net</p>]]></content>
      
      
      <categories>
          
          <category> 博客维护 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客维护 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis单节点部署</title>
      <link href="/2021/08/19/Redis%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/19/Redis%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Redis单节点部署"><a href="#大数据环境搭建-Redis单节点部署" class="headerlink" title="大数据环境搭建-Redis单节点部署"></a>大数据环境搭建-Redis单节点部署</h1><h2 id="安装编译器"><a href="#安装编译器" class="headerlink" title="安装编译器"></a>安装编译器</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">yum</span> <span class="token attr-value">install centos-release-scl scl-utils-build</span><span class="token attr-name">yum</span> <span class="token attr-value">install -y devtoolset-8-toolchain</span><span class="token attr-name">scl</span> <span class="token attr-value">enable devtoolset-8 bash</span></code></pre><h2 id="安装Redis"><a href="#安装Redis" class="headerlink" title="安装Redis"></a>安装Redis</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">tar</span> <span class="token attr-value">-zxvf redis-6.2.1.tar.gz -C /opt/module/</span><span class="token attr-name">cd</span> <span class="token attr-value">redis-6.2.1</span>make如果出错了的话，就执行这个，出错的原因可能就是编译器的安装没有准备好<span class="token attr-name">make</span> <span class="token attr-value">distclean  ---> 如果出错了，先执行make，然后在执行make install</span><span class="token attr-name">make</span> <span class="token attr-value">install</span>安装目录：/usr/local/bin查看默认安装目录：<span class="token attr-name">redis-benchmark</span><span class="token punctuation">:</span><span class="token attr-value">性能测试工具，可以在自己本子运行，看看自己本子性能如何</span>redis-check-aof：修复有问题的AOF文件，rdb和aof后面讲redis-check-dump：修复有问题的dump.rdb文件redis-sentinel：Redis集群使用redis-server：Redis服务器启动命令redis-cli：客户端，操作入口<span class="token attr-name">mv</span> <span class="token attr-value">redis.conf redis</span><span class="token attr-name">daemonize</span> <span class="token attr-value">no改成yes</span></code></pre><h2 id="配置文件的更改"><a href="#配置文件的更改" class="headerlink" title="配置文件的更改"></a>配置文件的更改</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">daemonize</span> <span class="token attr-value">yes ---> 后台启动</span><span class="token attr-name">bind</span> <span class="token attr-value">0.0.0.0  ---> 表任何IP地址都能连接上，以前只有127.0.0.1表示主机连接</span><span class="token attr-name">c</span> <span class="token attr-value">no ---> 将也是改成no，将安全连接关闭，不然也连接不上</span><span class="token attr-name">dir</span> <span class="token attr-value">/opt/module/redis/db</span><span class="token attr-name">dbfilename</span> <span class="token attr-value">wmy.rdb</span><span class="token attr-name">pidfile</span> <span class="token attr-value">/opt/module/redis/redis_6379.pid</span>查看是否关闭防火墙<span class="token attr-name">systemctl</span> <span class="token attr-value">status firewalld</span><span class="token attr-name">systemctl</span> <span class="token attr-value">stop firewalld.service</span><span class="token attr-name">redis-server</span> <span class="token attr-value">/opt/module/redis/redis.conf</span><span class="token attr-name">ps</span> <span class="token attr-value">-ef | grep redis</span><span class="token attr-name">redis-cli</span> <span class="token attr-value">-h flink01</span><span class="token attr-name">redis-cli</span> <span class="token attr-value">shutdown</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBASE2.0.5分布式部署</title>
      <link href="/2021/08/18/HBASE2-0-5%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/HBASE2-0-5%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-HBASE2-0-5分布式部署"><a href="#大数据环境搭建-HBASE2-0-5分布式部署" class="headerlink" title="大数据环境搭建-HBASE2.0.5分布式部署"></a>大数据环境搭建-HBASE2.0.5分布式部署</h1><h2 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h2><pre class=" language-xml"><code class="language-xml">export HBASE_MANAGES_ZK=false</code></pre><h2 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.rootdir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820/hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.cluster.distributed<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01,flink02,flink03<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.master.maxclockskew<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>180000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Time difference of regionserver from master<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="kylin"><a href="#kylin" class="headerlink" title="kylin"></a>kylin</h2><p>这个安装很简单，只需要解压完成就好了</p><h2 id="phoenix安装"><a href="#phoenix安装" class="headerlink" title="phoenix安装"></a>phoenix安装</h2><p>安装成功了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark3.0.0-hadoop3.2分布式部署</title>
      <link href="/2021/08/18/Spark3-0-0-hadoop3-2%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Spark3-0-0-hadoop3-2%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Spark3-0-0-hadoop3-2分布式部署"><a href="#大数据环境搭建-Spark3-0-0-hadoop3-2分布式部署" class="headerlink" title="大数据环境搭建-Spark3.0.0-hadoop3.2分布式部署"></a>大数据环境搭建-Spark3.0.0-hadoop3.2分布式部署</h1><h2 id="spark-env-sh"><a href="#spark-env-sh" class="headerlink" title="spark-env.sh"></a>spark-env.sh</h2><pre class=" language-shell"><code class="language-shell">SPARK_MASTER_HOST=flink01SPARK_MASTER_PORT=7077export JAVA_HOME=/opt/module/jdk1.8.0_144</code></pre><h2 id="spark-defaults-conf"><a href="#spark-defaults-conf" class="headerlink" title="spark-defaults.conf"></a>spark-defaults.conf</h2><pre class=" language-shell"><code class="language-shell">spark.master    yarnspark.eventLog.enabled    truespark.eventLog.dir    hdfs://flink01:9820/spark-historyspark.executor.memory    1gspark.driver.memory    1gspark.yarn.jars hdfs://flink01:9820/spark-jars/*</code></pre><h2 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h2><pre class=" language-xml"><code class="language-xml">flink01flink02flink03</code></pre><p>在这里，我只是把Spark当作了Hive的计算引擎了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveOnSpark分布式搭建</title>
      <link href="/2021/08/18/hiveOnSpark%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/hiveOnSpark%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建HiveOnSpark"><a href="#大数据环境搭建HiveOnSpark" class="headerlink" title="大数据环境搭建HiveOnSpark"></a>大数据环境搭建HiveOnSpark</h1><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><pre class=" language-shell"><code class="language-shell">mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bakmv mysql-connector-java-5.1.27-bin.jar hive-3.1.2/lib/</code></pre><h2 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162352155.png" alt="image-20210822162352155"></p><p>其实很奇怪的事情就是，我们hive把spark给当作引擎，我们可以不用启动，但是还是必须要有的，不过这个也不奇怪，毕竟在工作当中，一个平台不可能没有Spark的</p><h2 id="capacity-scheduler-xml"><a href="#capacity-scheduler-xml" class="headerlink" title="capacity-scheduler.xml"></a>capacity-scheduler.xml</h2><p>记住，这个要和hadoop配置文件中的都必须是类似的</p><pre class=" language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--     Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License. See accompanying LICENSE file.--></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Maximum number of applications that can be pending and running.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>0.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Maximum percent of resources in the cluster which can be used to run   application masters i.e. controls number of concurrent running  applications.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.resource-calculator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ResourceCalculator implementation to be used to compare   Resources in the scheduler.  The default i.e. DefaultResourceCalculator only uses Memory while  DominantResourceCalculator uses dominant-resource to compare   multi-dimensional resources such as Memory, CPU etc.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.queues<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>default,hive<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The queues at the this level (root is the root queue).<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>50<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Default queue target capacity.yuan 50 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.user-limit-factor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Default queue user limit a percentage from 0.0 to 1.0.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>100<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The maximum capacity of the default queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.state<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>RUNNING<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The state of the default queue. State can be one of RUNNING or STOPPED.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_submit_applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can submit jobs to the default queue.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_administer_queue<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can administer jobs on the default queue.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.acl_application_max_priority<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  The ACL of who can submit applications with configured priority.  For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.maximum-application-lifetime <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>    Maximum lifetime of an application which is submitted to a queue    in seconds. Any value less than or equal to zero will be considered as    disabled.    This will be a hard time limit for all applications in this    queue. If positive value is configured then any application submitted    to this queue will be killed after exceeds the configured lifetime.    User can also specify lifetime per application basis in    application submission context. But user lifetime will be    overridden if it exceeds queue maximum lifetime. It is point-in-time    configuration.    Note : Configuring too low value will result in killing application    sooner. This feature is applicable only for leaf queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.default-application-lifetime <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>    Default lifetime of an application which is submitted to a queue    in seconds. Any value less than or equal to zero will be considered as    disabled.    If the user has not submitted application with lifetime value then this    value will be taken. It is point-in-time configuration.    Note : Default lifetime can't exceed maximum lifetime. This feature is    applicable only for leaf queue. <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.node-locality-delay<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>40<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Number of missed scheduling opportunities after which the CapacityScheduler   attempts to schedule rack-local containers.  When setting this parameter, the size of the cluster should be taken into account.  We use 40 as the default value, which is approximately the number of nodes in one rack.  Note, if this value is -1, the locality constraint in the container request  will be ignored, which disables the delay scheduling.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.rack-locality-additional-delay<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Number of additional missed scheduling opportunities over the node-locality-delay  ones, after which the CapacityScheduler attempts to schedule off-switch containers,  instead of rack-local ones.  Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will  attempt rack-local assignments after 40 missed opportunities, and off-switch assignments  after 40+20=60 missed opportunities.  When setting this parameter, the size of the cluster should be taken into account.  We use -1 as the default value, which disables this feature. In this case, the number  of missed opportunities for assigning off-switch containers is calculated based on  the number of containers and unique locations specified in the resource request,  as well as the size of the cluster.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.queue-mappings<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  A list of mappings that will be used to assign jobs to queues  The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*  Typically this list will be used to map users to queues,  for example, u:%user:%user maps all users to queues with the same name  as the user.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.queue-mappings-override.enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  If a queue mapping is present, will it override the value specified  by the user? This can be used by administrators to place jobs in queues  that are different than the one specified by the user.  The default is false.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Controls the number of OFF_SWITCH assignments allowed  during a node's heartbeat. Increasing this value can improve  scheduling rate for OFF_SWITCH containers. Lower values reduce  "clumping" of applications on particular nodes. The default is 1.  Legal values are 1-MAX_INT. This config is refreshable.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.application.fail-fast<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  Whether RM should fail during recovery if previous applications'  queue is no longer valid.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>0.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  集群中用于运行应用程序ApplicationMaster的资源比例上限，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的ApplicationMaster资源比例上限可通过参数yarn.scheduler.capacity.maximum-am-resource-percent设置，而单个队列可通过参数yarn.scheduler.capacity. queue-path .maximum-am-resource-percent设置适合自己的值。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>50<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列的容量为50%<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  一个用户最多能够获取该队列资源容量的比例，取值0-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>80<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列的最大容量（自己队列资源不够，可以使用其他队列资源上限）<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.state<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>RUNNING<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  开启hive队列运行，不设置队列不能使用<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  访问控制，控制谁可以将任务提交到该队列,*表示任何人<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  指定哪个用户可以提交配置任务优先级<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>  hive队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>            org.apache.hadoop.io.compress.GzipCodec,            org.apache.hadoop.io.compress.DefaultCodec,            org.apache.hadoop.io.compress.BZip2Codec,            org.apache.hadoop.io.compress.SnappyCodec,            com.hadoop.compression.lzo.LzoCodec,            com.hadoop.compression.lzo.LzopCodec        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink03:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 测试环境指定HDFS副本的数量1 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hive-site-xml-1"><a href="#hive-site-xml-1" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://flink01:3306/wmyHive?useSSL=false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>000000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.warehouse.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/user/hive/warehouse<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.schema.verification<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.server2.thrift.port<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.server2.thrift.bind.host<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.event.db.notification.api.auth<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.header<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.current.db<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>spark.yarn.jars<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:9820/spark-jars/*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>      <span class="token comment" spellcheck="true">&lt;!--Hive执行引擎--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.execution.engine<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--Hive和Spark连接超时时间--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.spark.client.connect.timeout<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1000000ms<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.uris<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>thrift://flink01:9083<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 历史服务器端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 历史服务器web端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="spark-defaults-conf"><a href="#spark-defaults-conf" class="headerlink" title="spark-defaults.conf"></a>spark-defaults.conf</h2><pre class=" language-xml"><code class="language-xml">spark.master    yarnspark.eventLog.enabled    truespark.eventLog.dir    hdfs://flink01:9820/spark-historyspark.executor.memory    1gspark.driver.memory    1gspark.yarn.jars hdfs://flink01:9820/spark-jars/*</code></pre><h2 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对虚拟内存的限制检查 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 开启日志聚集功能 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志聚集服务器地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log.server.url<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>http://flink01:19888/jobhistory/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志保留时间为7天 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation.retain-seconds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>604800<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>如果想要看Spark是怎么安装的也可以写一篇博客来进行记录</p><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162412732.png" alt="image-20210822162412732"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop3.1.3分布式环境搭建</title>
      <link href="/2021/08/18/hadoop3-1-3%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/hadoop3-1-3%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-hadoop3-1-3"><a href="#大数据环境搭建-hadoop3-1-3" class="headerlink" title="大数据环境搭建-hadoop3.1.3"></a>大数据环境搭建-hadoop3.1.3</h1><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th></th><th>flink01</th><th>flink02</th><th>flink03</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode | DataNode</td><td>DataNode</td><td>SecondaryNameNode | DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager | NodeManager</td><td>NodeManager</td></tr></tbody></table><h2 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://flink01:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>            org.apache.hadoop.io.compress.GzipCodec,            org.apache.hadoop.io.compress.DefaultCodec,            org.apache.hadoop.io.compress.BZip2Codec,            org.apache.hadoop.io.compress.SnappyCodec,            com.hadoop.compression.lzo.LzoCodec,            com.hadoop.compression.lzo.LzopCodec        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink03:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 测试环境指定HDFS副本的数量1 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink02<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对虚拟内存的限制检查 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 开启日志聚集功能 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志聚集服务器地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log.server.url<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>http://flink01:19888/jobhistory/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 设置日志保留时间为7天 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation.retain-seconds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>604800<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h2><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 历史服务器端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 历史服务器web端地址 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>flink01:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h2 id="hadoop-env-sh-yarn-env-sh-mapred-env-sh"><a href="#hadoop-env-sh-yarn-env-sh-mapred-env-sh" class="headerlink" title="hadoop-env.sh,yarn-env.sh,mapred-env.sh"></a>hadoop-env.sh,yarn-env.sh,mapred-env.sh</h2><pre class=" language-xml"><code class="language-xml">export JAVA_HOME=/opt/flink/platform/jdk1.8.0_144</code></pre><h2 id="将start-dfs-sh，stop-dfs-sh两个文件顶部添加以下参数"><a href="#将start-dfs-sh，stop-dfs-sh两个文件顶部添加以下参数" class="headerlink" title="将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数"></a>将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数</h2><pre class=" language-shell"><code class="language-shell">HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root</code></pre><h2 id="还有，start-yarn-sh，stop-yarn-sh顶部也需添加以下"><a href="#还有，start-yarn-sh，stop-yarn-sh顶部也需添加以下" class="headerlink" title="还有，start-yarn.sh，stop-yarn.sh顶部也需添加以下"></a>还有，start-yarn.sh，stop-yarn.sh顶部也需添加以下</h2><pre class=" language-shell"><code class="language-shell">YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>平台节点的免密登录</title>
      <link href="/2021/08/18/%E5%B9%B3%E5%8F%B0%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/"/>
      <url>/2021/08/18/%E5%B9%B3%E5%8F%B0%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-免密节点设置"><a href="#大数据环境搭建-免密节点设置" class="headerlink" title="大数据环境搭建-免密节点设置"></a>大数据环境搭建-免密节点设置</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>免密设置的初心就是大数据节点在平台之间的传输的过程当中不会进行一个密码的输入，简化开发，其中设置和配置是非常简单的，但是需要使用一个技巧就是必须的发送所有命令到所有的会话，这样copy的次数的就会比较少</p><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">192.168.21.151</span> <span class="token attr-value">flink01</span><span class="token attr-name">192.168.21.152</span> <span class="token attr-value">flink02</span><span class="token attr-name">192.168.21.153</span> <span class="token attr-value">flink03</span><span class="token attr-name">ssh-keygen</span> <span class="token attr-value">-t rsa </span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink01</span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink02</span><span class="token attr-name">ssh-copy-id</span> <span class="token attr-value">flink03</span></code></pre><p>这样就可以搭建成功了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JDK1.8分布式环境搭建</title>
      <link href="/2021/08/18/JDK1-8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/08/18/JDK1-8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-JDK环境搭建"><a href="#大数据环境搭建-JDK环境搭建" class="headerlink" title="大数据环境搭建-JDK环境搭建"></a>大数据环境搭建-JDK环境搭建</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>大数据平台环境搭建基本的原理都是一个类似的操作</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>jdk-8u144-linux-x64.tar.gz</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><p>这个的话只需要进行一个解压操作就好了</p><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>java</p><p>javac</p><p>java -version</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据组件Shell脚本</title>
      <link href="/2021/08/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6Shell%E8%84%9A%E6%9C%AC/"/>
      <url>/2021/08/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6Shell%E8%84%9A%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<h2 id="大数据平台组件的命令"><a href="#大数据平台组件的命令" class="headerlink" title="大数据平台组件的命令"></a>大数据平台组件的命令</h2><h2 id="查看进程脚本"><a href="#查看进程脚本" class="headerlink" title="查看进程脚本"></a>查看进程脚本</h2><pre class=" language-shell"><code class="language-shell">#!/bin/bashfor i in flink01 flink02 flink02do   echo "====================== $i JPS ======================="   ssh $i /opt/module/jdk1.8.0_144/bin/jpsdone</code></pre><h2 id="传输文件"><a href="#传输文件" class="headerlink" title="传输文件"></a>传输文件</h2><p>yum -y install rsync，记住这个话必须得先安装这个，要不然得就会报错。。。</p><pre class=" language-shell"><code class="language-shell">#!/bin/bash#1. 判断参数个数if [ $# -lt 1 ]then  echo Not Enough Arguement!  exit;fi#2. 遍历集群所有机器for host in flink01 flink02 flink03do  echo ====================  $host  ====================  #3. 遍历所有目录，挨个发送  for file in $@  do    #4 判断文件是否存在    if [ -e $file ]    then      #5. 获取父目录      pdir=$(cd -P $(dirname $file); pwd)      #6. 获取当前文件的名称      fname=$(basename $file)      ssh $host "mkdir -p $pdir"      rsync -av $pdir/$fname $host:$pdir    else      echo $file does not exists!    fi  donedone</code></pre><h2 id="查看kafka-topic个数"><a href="#查看kafka-topic个数" class="headerlink" title="查看kafka topic个数"></a>查看kafka topic个数</h2><pre class=" language-shell"><code class="language-shell">/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper flink01:2181/kafka --list</code></pre><h2 id="启动kafka"><a href="#启动kafka" class="headerlink" title="启动kafka"></a>启动kafka</h2><pre class=" language-properties"><code class="language-properties"><span class="token comment" spellcheck="true">#! /bin/bash</span><span class="token attr-name">case</span> <span class="token attr-value">$1 in</span>"start"){<span class="token attr-name">    for</span> <span class="token attr-value">i in flink01 flink02 flink03</span>    do<span class="token attr-name">        echo</span> <span class="token attr-value">"------------------- kafka $i -------------------"</span><span class="token attr-name">        ssh</span> <span class="token attr-value">$i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties"</span>    done};;"stop"){<span class="token attr-name">        for</span> <span class="token attr-value">i in flink01 flink02 flink03</span>        do<span class="token attr-name">                echo</span> <span class="token attr-value">"------------------- kafka $i -------------------"</span><span class="token attr-name">                ssh</span> <span class="token attr-value">$i "/opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh"</span>        done};;esac</code></pre><p>注意，这里有一个问题就是我们启动Kafka的时候使用这个是启动不起来的，但是可以使用单台进行一个启动操作</p><h2 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h2><pre class=" language-properties"><code class="language-properties"><span class="token attr-name">/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh</span> <span class="token attr-value">--zookeeper flink01:2181,flink02:2181,flink03:2181/kafka  --create --replication-factor 1 --partitions 1 --topic $1</span></code></pre><h2 id="启动Flink"><a href="#启动Flink" class="headerlink" title="启动Flink"></a>启动Flink</h2><pre class=" language-shell"><code class="language-shell">#! /bin/bashcase $1 in"start"){    echo "------------------- 启动flink -------------------"    /opt/module/flink-1.13.2/bin/start-cluster.sh};;"stop"){    echo "------------------- 关闭flink -------------------"    /opt/module/flink-1.13.2/bin/stop-cluster.sh};;esac</code></pre><h2 id="启动hadoop"><a href="#启动hadoop" class="headerlink" title="启动hadoop"></a>启动hadoop</h2><pre class=" language-shell"><code class="language-shell">#!/bin/bashif [ $# -lt 1 ]then    echo "No Args Input..."    exit ;ficase $1 in"start")        echo " =================== 启动 hadoop集群 ==================="        echo " --------------- 启动 hdfs ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"        echo " --------------- 启动 yarn ---------------"        ssh flink02 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"        echo " --------------- 启动 historyserver ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver";;"stop")        echo " =================== 关闭 hadoop集群 ==================="        echo " --------------- 关闭 historyserver ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"        echo " --------------- 关闭 yarn ---------------"        ssh flink02 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"        echo " --------------- 关闭 hdfs ---------------"        ssh flink01 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh";;*)    echo "Input Args Error...";;esac</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.13.2环境分布式部署</title>
      <link href="/2021/08/18/Flink1-13-2%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Flink1-13-2%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>Flink是不依赖于任何组件的，但是它可以去集成周边的组件化，来看一下如何去搭建一个Flink-1.13.2的环境勒，这里的话，我是搭建的Flink Standalone的方式来进行一个测试操作</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>flink1.13.2</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>使用启动命令来进行一个测试</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka-2-11-2.4.1环境分布式部署</title>
      <link href="/2021/08/18/Kafka-2-11-2-4-1%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Kafka-2-11-2-4-1%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-kafka-2-11-2-4-1"><a href="#大数据环境搭建-kafka-2-11-2-4-1" class="headerlink" title="大数据环境搭建-kafka-2.11-2.4.1"></a>大数据环境搭建-kafka-2.11-2.4.1</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>这个和以前有点区别，它是一依赖于zookeeper的，所以必须在zookeeper安装之后才能进行一个安装的处理</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>kafka_2.11-2.4.1.tgz</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><p>这个安装步骤很简单，但是要注意的就是必须和myid的id号一致，否则的话就会启动失败的操作</p><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162430215.png" alt="image-20210822162430215"></p><p>这个测试的方便一些，搭建的过程当中一定要注意细节性的问题，如果不注意的话就会花大量的时间查找错误的，这些错误是不值得去犯错的</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper3.5.7环境分布式部署</title>
      <link href="/2021/08/18/Zookeeper3-5-7%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/Zookeeper3-5-7%E7%8E%AF%E5%A2%83%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-Zookeeper"><a href="#大数据环境搭建-Zookeeper" class="headerlink" title="大数据环境搭建-Zookeeper"></a>大数据环境搭建-Zookeeper</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>这个和之前都是一样的，首先得准备需要安装的节点的机器</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p>zookeeper-3.5.7</p><h2 id="安装所需要的命令和步骤"><a href="#安装所需要的命令和步骤" class="headerlink" title="安装所需要的命令和步骤"></a>安装所需要的命令和步骤</h2><pre class=" language-properties"><code class="language-properties">注意：必须在bin/zkEnv.sh<span class="token comment" spellcheck="true"># 如果不配置这个的话启动的时候就会报错误</span><span class="token attr-name">export</span> <span class="token attr-value">JAVA_HOME=/opt/flink/platform/jdk1.8.0_144</span><span class="token attr-name">dataDir</span><span class="token punctuation">=</span><span class="token attr-value">/opt/module/zookeeper-3.5.7/zkData</span><span class="token attr-name">server.1</span><span class="token punctuation">=</span><span class="token attr-value">flink01:2888:3888</span><span class="token attr-name">server.2</span><span class="token punctuation">=</span><span class="token attr-value">flink02:2888:3888</span><span class="token attr-name">server.3</span><span class="token punctuation">=</span><span class="token attr-value">flink03:2888:3888</span></code></pre><h2 id="测试安装是否成功"><a href="#测试安装是否成功" class="headerlink" title="测试安装是否成功"></a>测试安装是否成功</h2><p>记住，这里的话就会有一个小坑，每一次都是myid不一致导致错误的，这里和kafka哪里也要注意，否则会花大量的时间进行一个排查错误的情况</p><pre class=" language-properties"><code class="language-properties"><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink01 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2795</span> <span class="token attr-value">Jps</span><span class="token attr-name">2525</span> <span class="token attr-value">QuorumPeerMain</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink02 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2038</span> <span class="token attr-value">QuorumPeerMain</span><span class="token attr-name">2086</span> <span class="token attr-value">Jps</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span> flink02 JPS <span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token punctuation">=</span><span class="token attr-name">2038</span> <span class="token attr-value">QuorumPeerMain</span><span class="token attr-name">2104</span> <span class="token attr-value">Jps</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL5.7-Linux环境下部署</title>
      <link href="/2021/08/18/MySQL5-7-Linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/08/18/MySQL5-7-Linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据环境搭建-MySQL环境部署"><a href="#大数据环境搭建-MySQL环境部署" class="headerlink" title="大数据环境搭建-MySQL环境部署"></a>大数据环境搭建-MySQL环境部署</h1><h2 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h2><p>需要一台虚拟机，能够连网，防火墙关闭，各方面正常的虚拟机就可以开始MySQL的环境搭建了</p><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><p><img src="https://gitee.com/wmybigdata/blog-drawing-bed/raw/master/img/image-20210822162500471.png" alt="image-20210822162500471"></p><h2 id="安装所需要的命令和步骤如何"><a href="#安装所需要的命令和步骤如何" class="headerlink" title="安装所需要的命令和步骤如何"></a>安装所需要的命令和步骤如何</h2><pre class=" language-properties"><code class="language-properties">1、查看是都有安装过MySQL或者是默认的mariadb<span class="token attr-name">rpm</span> <span class="token attr-value">-qa | grep mysql</span><span class="token attr-name">rpm</span> <span class="token attr-value">-qa | grep mariadb</span>2、删除自带的mariadba<span class="token attr-name">rpm</span> <span class="token attr-value">-e --nodeps mariadb-libs-5.5.56-2.el7.x86_64</span>3、解压MySQL的配置<span class="token attr-name">tar</span> <span class="token attr-value">-xvf mysql-5.7.19-1.el7.x86_64.rpm-bundle.tar</span>4、安装MySQL的rpm<span class="token attr-name">rpm</span> <span class="token attr-value">-ivh --nodeps mysql-community-server-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh --nodeps mysql-community-client-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm</span><span class="token attr-name">rpm</span> <span class="token attr-value">-ivh mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm</span>5、查看是否启动以及启动MySQL<span class="token attr-name">systemctl</span> <span class="token attr-value">status mysqld</span><span class="token attr-name">systemctl</span> <span class="token attr-value">start mysqld</span>6、查看生成的密码<span class="token attr-name">cat</span> <span class="token attr-value">/var/log/mysqld.log | grep password</span>7、第一次登录MySQL<span class="token attr-name">mysql</span> <span class="token attr-value">-uroot -p</span>注意，如果出现了密码里面有一些特殊符号的，可以回车之后在进行输入密码就可以了8、对MySQL进行配置和设置密码以及对远程连接进行一个设置的操作<span class="token attr-name">set</span> <span class="token attr-value">global validate_password_policy=0;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_mixed_case_count=0; </span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_number_count=3;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_special_char_count=0;</span><span class="token attr-name">set</span> <span class="token attr-value">global validate_password_length=3;</span><span class="token attr-name">SET</span> <span class="token attr-value">PASSWORD FOR 'root'@'localhost' = PASSWORD('000000');</span><span class="token attr-name">update</span> <span class="token attr-value">mysql.user set host = '%' where user = 'root';</span><span class="token attr-name">flush</span> <span class="token attr-value">privileges;</span>quit<span class="token attr-name">mysql</span> <span class="token attr-value">-uroot -p000000;</span></code></pre><h2 id="使用Navicat来进行链接MySQL"><a href="#使用Navicat来进行链接MySQL" class="headerlink" title="使用Navicat来进行链接MySQL"></a>使用Navicat来进行链接MySQL</h2><h2 id="开启binlog日志"><a href="#开启binlog日志" class="headerlink" title="开启binlog日志"></a>开启binlog日志</h2><pre class=" language-properties"><code class="language-properties"><span class="token comment" spellcheck="true"># binlog start</span><span class="token attr-name">server-id</span><span class="token punctuation">=</span><span class="token attr-value">1</span><span class="token attr-name">log-bin</span><span class="token punctuation">=</span><span class="token attr-value">mysql-bin</span><span class="token attr-name">binlog_format</span><span class="token punctuation">=</span><span class="token attr-value">row</span><span class="token attr-name">binlog-do-db</span><span class="token punctuation">=</span><span class="token attr-value">flink</span></code></pre><p>systemctl restart mysqld</p>]]></content>
      
      
      <categories>
          
          <category> 大数据环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
